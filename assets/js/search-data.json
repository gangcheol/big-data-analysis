{
  
    
        "post0": {
            "title": "중간고사 대비",
            "content": "1&#51452;&#52264; : &#45800;&#49692;&#49440;&#54805;&#54924;&#44480; . import numpy as np import matplotlib.pyplot as plt import tensorflow as tf . &#50728;&#46020;&#50640; &#46384;&#47480; &#50500;&#47700;&#47532;&#52852;&#45432; &#54032;&#47588;&#47049; . $$아이스 아메리카노 = beta_1 times 온도 + varepsilon$$ . - 온도 $ bf{x}$를 아래와 같이 생성 . x = tf.constant([20.1,22.2,22.7,23.3,24.4,25.1,26.2,27.3,28.4,30.4]) x . &lt;tf.Tensor: shape=(10,), dtype=float32, numpy= array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4], dtype=float32)&gt; . - 가정 : 만약 아메리카노 판매량 $ bf{y}$가 아래와 같다고 하자 . $$ bf {y} approx 10.2 + 2.2 bf{x}$$ . tf.random.set_seed(202150256) epsilon = tf.random.normal([10]) y = 10.2 + 2.2*x + epsilon y . &lt;tf.Tensor: shape=(10,), dtype=float32, numpy= array([53.728127, 57.886845, 59.39006 , 63.192116, 63.904133, 63.547157, 68.00105 , 70.056755, 72.11336 , 78.33952 ], dtype=float32)&gt; . - 일상적인 데이터 형식에서 다음과 같이 자료를 모은 셈이다 . tf.transpose(tf.concat([[x],[y]],0)) . &lt;tf.Tensor: shape=(10, 2), dtype=float32, numpy= array([[20.1 , 53.728127], [22.2 , 57.886845], [22.7 , 59.39006 ], [23.3 , 63.192116], [24.4 , 63.904133], [25.1 , 63.547157], [26.2 , 68.00105 ], [27.3 , 70.056755], [28.4 , 72.11336 ], [30.4 , 78.33952 ]], dtype=float32)&gt; . &#46020;&#49885;&#54868; . plt.figure(figsize=(8,6)) plt.plot(x,y,&quot;.&quot;,label=&quot;y&quot;) plt.plot(x,10.2+2.2*x,&quot;--&quot;,label =&quot;hat y&quot;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7ff99b361c50&gt; . &#47785;&#54364; . $$loss = sum (y - hat {y})^2 $$ . loss를 최소화 하는 것이 우리의 목표이다. | . 일일이 $ beta_i$의 값을 집어넣으면 loss를 최소화하는 $ beta_i$를 구할 수 있지만 사실상 무한번 저 과정을 반복하는 것은 불가능하다. | . 따라서 위를 각각의 $ beta_i$로 미분하면 다음과 같이 구할 수 있다. | . $$ hat { beta_1} = frac {S_{xy}}{S_{xx}} = frac { sum (x- bar x)(y- bar y)}{ sum {(x- bar x)^2}}$$ . $$ hat { beta_0} = bar y - hat { beta_1} bar x $$ . import numpy as np . Sxx = sum((x- np.mean(x))**2) Sxy = sum((x-np.mean(x))*(y-np.mean(y))) . hat_b1 = Sxy/Sxx hat_b0 = np.mean(y) - hat_b1*np.mean(x) . hat_b1,hat_b0 . (&lt;tf.Tensor: shape=(), dtype=float32, numpy=2.311769&gt;, &lt;tf.Tensor: shape=(), dtype=float32, numpy=7.198574&gt;) . - 위의 그래프와 비교해보자 . $y$ : 실제 값 | $ hat {y_1}$ : 우리가 일전에 가정한 것(세상의 법칡) | $ hat {y_2}$ : 미분으로 구한 베타 추정치 | . $$ hat {y_1} = 10.2 + 2.2 x$$ . $$ hat {y_2} = hat { beta_{0,2}} + hat { beta_{1,2}} times x $$ . plt.figure(figsize = (10,6)) plt.plot(x, y,&quot;.&quot;, label = &quot;y&quot;) plt.plot(x, 10.2+2.2*x,&quot;--&quot;, label = &quot;hat y1&quot;) plt.plot(x, hat_b1*x + hat_b0, label = &quot;hat y2&quot;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7ff99cd223d0&gt; . 여튼 미분을 통해서 $ beta_i$를 추정한다는 것은 수학적 관점에서 꽤나 훌륭한 방법인 것 같다. | . 그러나 만약 $x_i,i = 1,2, dots 10009$ 이런식으로 무수히 많은 변수가 존재할 경우 확장이 어렵고 beta_i 를 추정하는데 어마어마한 시간이 걸릴 것임 | . 이를 행렬구조로 바꾸어서 생각해보자 | . $$ bf {y} = bf {x} boldsymbol{ beta} + boldsymbol{ varepsilon}$$ . $ bf {y} to (n times 1)$ | . $ bf {x} to (n times p), quad boldsymbol { beta} to ( bf p times 1), quad therefore quad bf {x} boldsymbol{ beta} to (n times 1)$ | . $ boldsymbol { varepsilon} to ( bf n times 1)$ | . $$loss = sum (y- hat y)^2 = ( bf y - bf X boldsymbol { beta} )^{ top}( bf y - bf X boldsymbol { beta} ) $$ . 위를 풀면 . $$loss = bf y^{ top}y - bf y^{ top} bf X boldsymbol{ beta} - boldsymbol { beta}^{ top} bf {X}^{ top}y + boldsymbol { beta}^{ top} bf{X}^{ top} bf X boldsymbol { beta}$$ . 위를 미분하여 $loss$를 최소화하는 $ hat { boldsymbol { beta}}$를 구하면 . $$ hat { boldsymbol { beta}} = ( bf {X^{ top} X})^{-1} bf X^{ top} bf y$$ . - 이제 직접 코드를 짜서 $loss$를 최소화하는 $ hat { boldsymbol{ beta}}$를 구해보자 . &#47588;&#53944;&#47533;&#49828;&#47484; &#51060;&#50857;&#54620; &#48288;&#53440; &#52628;&#51221;&#52824; &#44396;&#54616;&#44592; . from tensorflow.python.ops.numpy_ops import np_config np_config.enable_numpy_behavior() ## numpy 처럼 사용하기 위해! . x = tf.constant([20.1,22.2,22.7,23.3,24.4,25.1,26.2,27.3,28.4,30.4]) epsilon = tf.random.normal([10]) y = 10.2 + 2.2*x + epsilon n = len(x) . X = tf.concat([[[1.0]*n],[x]],0).T . X . &lt;tf.Tensor: shape=(10, 2), dtype=float32, numpy= array([[ 1. , 20.1], [ 1. , 22.2], [ 1. , 22.7], [ 1. , 23.3], [ 1. , 24.4], [ 1. , 25.1], [ 1. , 26.2], [ 1. , 27.3], [ 1. , 28.4], [ 1. , 30.4]], dtype=float32)&gt; . print(tf.linalg.inv(X.T @ X) @ X.T @ y) . tf.Tensor([9.045627 2.2322702], shape=(2,), dtype=float32) . 값이 좀 다르긴 한데 이는 텐서플로우가 계산을 대충하는 것임 | . 실제로 정확한 계산을 위해 tensorflow 안에 내장된 nummpy를 이용하자 | . tensorflow&#51032; &#51221;&#54869;&#54620; &#44228;&#49328;&#51012; &#50948;&#54644;! . import tensorflow.experimental.numpy as tnp . x = tnp.array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4]) y = 10.2 + 2.2 *x +epsilon . Sxx = sum((x-np.mean(x))**2) Sxy = sum((x-np.mean(x))* (y-np.mean(y))) beta1 = Sxy/Sxx beta0 = np.mean(y) - beta1*np.mean(x) . [beta0,beta1] . [&lt;tf.Tensor: shape=(), dtype=float64, numpy=9.045680157770313&gt;, &lt;tf.Tensor: shape=(), dtype=float64, numpy=2.2322740618055756&gt;] . X = tnp.concatenate([[[1.0]*n],[x]]).T tf.linalg.inv(X.T @ X) @ X.T @ y . &lt;tf.Tensor: shape=(2,), dtype=float64, numpy=array([9.04568016, 2.23227406])&gt; . . Extra . $$L = loss = ( bf y - bf X boldsymbol{ beta})^{ top}( bf y - bf X boldsymbol{ beta})$$ . 위를 실제로 미분하여 수식을 써보장 | . $L = bf y^{ top}y - bf y^{ top} bf X boldsymbol{ beta} - boldsymbol { beta}^{ top} bf X^{ top}y + boldsymbol { beta}^{ top} bf X^{ top}X boldsymbol { beta}$ . 기억하자 . 벡터든, 메트릭스든 $ frac { partial}{ partial {x}}x^{ top}$는 $ bf I$ 가 성립한다. . &#48289;&#53552; &#48120;&#48516; . 1. $ , , bf y^{ top}y = sum y^{2}$ . $ bf {y} to (n times 1), quad bf {y}^{ top} to (1 times n)$ . $ bf y^{ top}y = sum y^{2} to (1 times 1)$ 이므로 . $$ therefore quad frac { partial sum y^2}{ partial boldsymbol { beta}} = 0$$ . 2. $ bf y^{ top} X boldsymbol { beta}$ . $ bf y^{ top} X boldsymbol { beta} to (1 times n) , (n times p) , (p times 1) = (1 times 1) $ 이는 . $( bf y^{ top} X boldsymbol { beta})^{ top} = boldsymbol{ beta}^{ top} bf X^ { top} bf y $ 를 의미한다 . 따라서 . $$ frac { partial }{ partial boldsymbol beta} left ( boldsymbol { beta}^{ top} bf X^{ top} bf y right )= bf X^{ top} bf y$$ . 3. $ boldsymbol { beta}^{ top} bf {X}^{ top} bf {X} boldsymbol { beta}$ . 알아야할 것 2 . $ frac { partial}{ partial y} y^{ top}y neq = bf I y$ 가 성립한다. . because !! . $y^{ top}y= y_1^{2} + y_2^{2} dots y_n^{2}$ 이고 . $ partial , bf y = [ , partial y_1, dots partial y_n ,]$ 이므로 $(n times 1) times (1 times 1) to (n times 1)$이 성립한다. . 따라서 위 같은 경우 $ frac { partial}{ partial y} y^{ top}y= 2 bf y$ 이다. . 이를 아래와 같이 표현할 수 있다. . $f(y) = y^{ top}y, quad g(y) = y^{ top}y$ . $ frac { partial }{ partial y} left { , (f(y)+ g(y) , right } = frac { partial}{ partial y} f(y) + frac { partial}{ partial y} g(y)$가 성립! . 어렵게 썼지만 2번 미분한다고 생각하자!! . 다시 돌아와서 아래식을 2번 미분하면? . $ boldsymbol { beta}^{ top} bf X^{ top} bf X boldsymbol { beta} = bf X^{ top} bf X boldsymbol { beta} + bf X^{ top} bf X boldsymbol { beta} = 2 bf X^{ top} bf X boldsymbol { beta} $ . $ frac { partial}{ partial boldsymbol { beta}} L = 0 -2 bf X ^{ top} bf y + 2 bf X^{ top} bf X boldsymbol { beta} =0$ . $ bf X^{ top} bf y = bf X ^{ top}X boldsymbol { beta}$ 이므로 . $$ hat { boldsymbol { beta}} = left ( bf X ^{ top}X right )^{-1} bf X^{ top} bf y$$ . . 2&#51452;&#52264; : Tensorflow . import tensorflow as tf import numpy as np . - tensoflow의 CPU 연결법 . tf.config.experimental.list_physical_devices(&quot;GPU&quot;) . [PhysicalDevice(name=&#39;/physical_device:GPU:0&#39;, device_type=&#39;GPU&#39;)] . tf.constant . - 리스트 . lst = list(range(6)) lst . [0, 1, 2, 3, 4, 5] . - 중첩 리스트 (2 x 2) . lst = [[1,2],[3,4]] lst . [[1, 2], [3, 4]] . - 중첩 리스트 (4 x 1) . lst = [[1],[2],[3],[4]] lst . [[1], [2], [3], [4]] . - 중첩 리스트 (1 x 4) . lst = [list([1,2,3,4])] lst . [[1, 2, 3, 4]] . tensorflow&#51032; &#48320;&#49688; &#49440;&#50616; . &#49828;&#52860;&#46972; . tf.constant(3.14) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=3.14&gt; . tf.constant(3.14) + tf.constant(3.14) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=6.28&gt; . &#48289;&#53552; . vector = tf.constant([1,2,3]) vector . &lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 2, 3], dtype=int32)&gt; . &#47588;&#53944;&#47533;&#49828; . matrix = tf.constant([[1,0],[0,1]]) matrix . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 0], [0, 1]], dtype=int32)&gt; . 3&#52264;&#50896; &#51060;&#49345;&#51032; &#48176;&#50676; . np.array( [[[0,1,1],[1,2,-1]], [[0,1,2],[1,2,-1]]]) . array([[[ 0, 1, 1], [ 1, 2, -1]], [[ 0, 1, 2], [ 1, 2, -1]]]) . tf.constant([[[0,1,1],[1,2,-1]], [[0,1,2],[1,2,-1]]]) . &lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy= array([[[ 0, 1, 1], [ 1, 2, -1]], [[ 0, 1, 2], [ 1, 2, -1]]], dtype=int32)&gt; . tf.constant([[[0,1,1],[1,2,-1]], [[0,1,2],[1,2,-1]],[[0,1,2],[1,2,-1]]]) . &lt;tf.Tensor: shape=(3, 2, 3), dtype=int32, numpy= array([[[ 0, 1, 1], [ 1, 2, -1]], [[ 0, 1, 2], [ 1, 2, -1]], [[ 0, 1, 2], [ 1, 2, -1]]], dtype=int32)&gt; . type(tf.constant([[[0,1,1],[1,2,-1]], [[0,1,2],[1,2,-1]]])) . tensorflow.python.framework.ops.EagerTensor . EagerTensor가 나오는 것을 기억하자 . &#51064;&#45937;&#49905; . matrix = tf.constant([[[0,1,1],[1,2,-1]], [[0,1,2],[1,2,-1]],[[0,1,2],[1,2,-1]]]) matrix . &lt;tf.Tensor: shape=(3, 2, 3), dtype=int32, numpy= array([[[ 0, 1, 1], [ 1, 2, -1]], [[ 0, 1, 2], [ 1, 2, -1]], [[ 0, 1, 2], [ 1, 2, -1]]], dtype=int32)&gt; . matrix[0][1][2], matrix[0][1][1] . (&lt;tf.Tensor: shape=(), dtype=int32, numpy=-1&gt;, &lt;tf.Tensor: shape=(), dtype=int32, numpy=2&gt;) . tf.constant&#51032; &#45800;&#51216; . - 모든 원소의 데이터 타입이 동일해야하고, 데이터 타입의 묵시적 변환이 불가능하다. . - 원소 수정이 불가능함 . a = tf.constant([1,2]) a . &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)&gt; . a[0] . &lt;tf.Tensor: shape=(), dtype=int32, numpy=1&gt; . a[0] =11 . TypeError Traceback (most recent call last) &lt;ipython-input-39-cf0e5bd1fc84&gt; in &lt;module&gt;() -&gt; 1 a[0] =11 TypeError: &#39;tensorflow.python.framework.ops.EagerTensor&#39; object does not support item assignment . tf.constant -&gt; &#45336;&#54028;&#51060; . np.array([tf.constant(1)]) . array([1], dtype=int32) . a = tf.constant(3.14) type(a) . tensorflow.python.framework.ops.EagerTensor . a.numpy() . tensorflow.python.framework.ops.EagerTensor . &#50672;&#49328; . &#45908;&#54616;&#44592; . a = tf.constant([1,2]) b = tf.constant([3,4]) a+b . &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([4, 6], dtype=int32)&gt; . a+b . &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([4, 6], dtype=int32)&gt; . tf.add(a,b) . &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([4, 6], dtype=int32)&gt; . &#44273;&#54616;&#44592; . - 아래값을 살펴보면 우리가 일반적으로 원하는 행렬 연산이 수행되지 않는다 . a = tf.constant([[1,2],[3,4]]) b = tf.constant([[5,6],[7,8]]) . a*b . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[ 5, 12], [21, 32]], dtype=int32)&gt; . - 아래와 같은 연산자를 이용하자 . a@b . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[19, 22], [43, 50]], dtype=int32)&gt; . tf.matmul(a,b) . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[19, 22], [43, 50]], dtype=int32)&gt; . &#50669;&#54665;&#47148; . - 초기 매트릭스를 int로 설정할 경우 역행렬 연산이 수행되지 않는다. . tf.linalg.inv(a) . InvalidArgumentError Traceback (most recent call last) &lt;ipython-input-52-fd9ade66c025&gt; in &lt;module&gt;() -&gt; 1 tf.linalg.inv(a) /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_linalg_ops.py in matrix_inverse(input, adjoint, name) 1504 return _result 1505 except _core._NotOkStatusException as e: -&gt; 1506 _ops.raise_from_not_ok_status(e, name) 1507 except _core._FallbackException: 1508 pass /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name) 7184 def raise_from_not_ok_status(e, name): 7185 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7186 raise core._status_to_exception(e) from None # pylint: disable=protected-access 7187 7188 InvalidArgumentError: Value for attr &#39;T&#39; of int32 is not in the list of allowed values: double, float, half, complex64, complex128 ; NodeDef: {{node MatrixInverse}}; Op&lt;name=MatrixInverse; signature=input:T -&gt; output:T; attr=adjoint:bool,default=false; attr=T:type,allowed=[DT_DOUBLE, DT_FLOAT, DT_HALF, DT_COMPLEX64, DT_COMPLEX128]&gt; [Op:MatrixInverse] . a = tf.constant([[1,2],[3,4]], dtype=float) tf.linalg.inv(a) . &lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy= array([[-2. , 1. ], [ 1.5, -0.5]], dtype=float32)&gt; . - 위와 같이 dtype을 실수로 설정해야 역행렬 연산이 가능하다 . &#54665;&#47148;&#49885; (determinant) &#44228;&#49328; . tf.linalg.det(a) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=-2.0&gt; . Trace(&#45824;&#44033;&#54633;) &#44228;&#49328; . tf.linalg.trace(a) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=5.0&gt; . &#54805;&#53468;&#48320;&#54872; . 2 x 2 . a = tf.constant(range(1,5)) a . &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt; . tf.reshape(a,(2,2)) . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 2], [3, 4]], dtype=int32)&gt; . temp = tf.reshape(a,(2,2,1)) temp . &lt;tf.Tensor: shape=(2, 2, 1), dtype=int32, numpy= array([[[1], [2]], [[3], [4]]], dtype=int32)&gt; . &#45796;&#52264;&#50896;&#51032; &#51201;&#50857; . a = tf.constant(range(1,13)) a . &lt;tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], dtype=int32)&gt; . tf.reshape(a,(2,2,3)) . &lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy= array([[[ 1, 2, 3], [ 4, 5, 6]], [[ 7, 8, 9], [10, 11, 12]]], dtype=int32)&gt; . tf.reshape(a,(2,2,-1)) . &lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy= array([[[ 1, 2, 3], [ 4, 5, 6]], [[ 7, 8, 9], [10, 11, 12]]], dtype=int32)&gt; . tf.reshape(a,(4,3)) . &lt;tf.Tensor: shape=(4, 3), dtype=int32, numpy= array([[ 1, 2, 3], [ 4, 5, 6], [ 7, 8, 9], [10, 11, 12]], dtype=int32)&gt; . tf.reshape(a,(4,-1)) ## 이거는 난 죽어도 안쓸 것 같으니 알아만 두자 . &lt;tf.Tensor: shape=(4, 3), dtype=int32, numpy= array([[ 1, 2, 3], [ 4, 5, 6], [ 7, 8, 9], [10, 11, 12]], dtype=int32)&gt; . b = tf.reshape(a,(2,2,-1)) b . &lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy= array([[[ 1, 2, 3], [ 4, 5, 6]], [[ 7, 8, 9], [10, 11, 12]]], dtype=int32)&gt; . tf.reshape(b,-1) . &lt;tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], dtype=int32)&gt; . c = tf.reshape(a,(4,-1)) ## 이거는 난 죽어도 안쓸 것 같으니 알아만 두자 c . &lt;tf.Tensor: shape=(4, 3), dtype=int32, numpy= array([[ 1, 2, 3], [ 4, 5, 6], [ 7, 8, 9], [10, 11, 12]], dtype=int32)&gt; . tf.reshape(c,-1) . &lt;tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], dtype=int32)&gt; . &#49440;&#50616;&#44256;&#44553; . 리스트나 넘파이로 만들고 output 을 tensor로 변경하는 것도 좋은 방법이다. | . l = list(range(1,5)) l . [1, 2, 3, 4] . tf.constant(l) . &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt; . tf.constant(np.diag(l)) . &lt;tf.Tensor: shape=(4, 4), dtype=int64, numpy= array([[1, 0, 0, 0], [0, 2, 0, 0], [0, 0, 3, 0], [0, 0, 0, 4]])&gt; . tf.zeros, tf.ones . - 같은 방법이지만 dtype이 달라짐을 주의하자 . tf.zeros([3,3]) . &lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy= array([[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]], dtype=float32)&gt; . tf.reshape(tf.constant([0]*9),[3,3]) . &lt;tf.Tensor: shape=(3, 3), dtype=int32, numpy= array([[0, 0, 0], [0, 0, 0], [0, 0, 0]], dtype=int32)&gt; . tf.ones([3,3]) . &lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy= array([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.]], dtype=float32)&gt; . tf.reshape(tf.constant([1]*9),[3,3]) . &lt;tf.Tensor: shape=(3, 3), dtype=int32, numpy= array([[1, 1, 1], [1, 1, 1], [1, 1, 1]], dtype=int32)&gt; . tf.linspace(0,1,10) . &lt;tf.Tensor: shape=(10,), dtype=float64, numpy= array([0. , 0.11111111, 0.22222222, 0.33333333, 0.44444444, 0.55555556, 0.66666667, 0.77777778, 0.88888889, 1. ])&gt; . tf.concat &#49929;&#51473;&#50836; . &#51200;&#52264;&#50896; &#54665;&#47148; . step.1 (2 x 1) 행렬 생성 . a = tf.constant([[1],[2]]) b = tf.constant([[3],[4]]) a,b . (&lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy= array([[1], [2]], dtype=int32)&gt;, &lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy= array([[3], [4]], dtype=int32)&gt;) . step.2 (1 x 2) + (1 x 2) $ to$ (2 x 2) or (1 x 4) . tf.concat([a,b],axis=1) . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 3], [2, 4]], dtype=int32)&gt; . tf.concat([a,b],axis=0) . &lt;tf.Tensor: shape=(4, 1), dtype=int32, numpy= array([[1], [2], [3], [4]], dtype=int32)&gt; . a = tf.constant([[1,2]]) b = tf.constant([[3,4]]) a,b . (&lt;tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[1, 2]], dtype=int32)&gt;, &lt;tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[3, 4]], dtype=int32)&gt;) . tf.concat([a,b],axis=0) . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 2], [3, 4]], dtype=int32)&gt; . tf.concat([a,b],axis=1) . &lt;tf.Tensor: shape=(1, 4), dtype=int32, numpy=array([[1, 2, 3, 4]], dtype=int32)&gt; . &#44256;&#52264;&#50896; &#54665;&#47148; (2 x 2 x 3) . - 내 생각 1 : 다차원의 경우 기존 n x p 의 케이크를 썬다고 생각하자 . - 내 생각 2 : tf.concat에서 다차원의 경우 axis 값이 늘어날 수록 차원은 저차원으로 간다고 생각하자 . a = tf.reshape(tf.constant(range(12)),(2,2,3)) b = -a . - 즉, 아래 a는 4 x 3행렬을 2 x 2 x 3으로 쪼갠 것이다. . tf.reshape(tf.constant(range(12)),(4,3)) . &lt;tf.Tensor: shape=(4, 3), dtype=int32, numpy= array([[ 0, 1, 2], [ 3, 4, 5], [ 6, 7, 8], [ 9, 10, 11]], dtype=int32)&gt; . a . &lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy= array([[[ 0, 1, 2], [ 3, 4, 5]], [[ 6, 7, 8], [ 9, 10, 11]]], dtype=int32)&gt; . a.shape . TensorShape([2, 2, 3]) . temp = tf.concat([a,b],axis=0) . temp.shape . TensorShape([4, 2, 3]) . temp . &lt;tf.Tensor: shape=(4, 2, 3), dtype=int32, numpy= array([[[ 0, 1, 2], [ 3, 4, 5]], [[ 6, 7, 8], [ 9, 10, 11]], [[ 0, -1, -2], [ -3, -4, -5]], [[ -6, -7, -8], [ -9, -10, -11]]], dtype=int32)&gt; . temp = tf.concat([a,b],axis=1) . temp.shape . TensorShape([2, 4, 3]) . temp . &lt;tf.Tensor: shape=(2, 4, 3), dtype=int32, numpy= array([[[ 0, 1, 2], [ 3, 4, 5], [ 0, -1, -2], [ -3, -4, -5]], [[ 6, 7, 8], [ 9, 10, 11], [ -6, -7, -8], [ -9, -10, -11]]], dtype=int32)&gt; . temp = tf.concat([a,b],axis=2) . temp.shape . TensorShape([2, 2, 6]) . temp . &lt;tf.Tensor: shape=(2, 2, 6), dtype=int32, numpy= array([[[ 0, 1, 2, 0, -1, -2], [ 3, 4, 5, -3, -4, -5]], [[ 6, 7, 8, -6, -7, -8], [ 9, 10, 11, -9, -10, -11]]], dtype=int32)&gt; . &#51452;&#51032;&#54624; &#44144; . - 차원이 (n,) 이런식으로 되어있을 경우 . a = tf.constant (range(4)) b = -a . a.shape . TensorShape([4]) . tf.concat([a,b],axis=0) . &lt;tf.Tensor: shape=(8,), dtype=int32, numpy=array([ 0, 1, 2, 3, 0, -1, -2, -3], dtype=int32)&gt; . tf.concat([a,b],axis=1) . InvalidArgumentError Traceback (most recent call last) &lt;ipython-input-171-6f52ef50c654&gt; in &lt;module&gt;() -&gt; 1 tf.concat([a,b],axis=1) /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py in error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name) 7184 def raise_from_not_ok_status(e, name): 7185 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7186 raise core._status_to_exception(e) from None # pylint: disable=protected-access 7187 7188 InvalidArgumentError: ConcatOp : Expected concatenating dimensions in the range [-1, 1), but got 1 [Op:ConcatV2] name: concat . - 위 같은 에러가 안뜨게 하려면 . a = tf.constant([list(range(4))]) b=-a . a.shape . TensorShape([1, 4]) . tf.concat([a,b],axis=0) . &lt;tf.Tensor: shape=(2, 4), dtype=int32, numpy= array([[ 0, 1, 2, 3], [ 0, -1, -2, -3]], dtype=int32)&gt; . tf.concat([a,b],axis=1) . &lt;tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[ 0, 1, 2, 3, 0, -1, -2, -3]], dtype=int32)&gt; . tf.stack . &#51200;&#52264;&#50896;&#51032; &#44221;&#50864; . a = tf.constant(range(4)) b = -a . a.shape . TensorShape([4]) . tf.stack([a,b],axis=0) . &lt;tf.Tensor: shape=(2, 4), dtype=int32, numpy= array([[ 0, 1, 2, 3], [ 0, -1, -2, -3]], dtype=int32)&gt; . tf.stack([a,b],axis=1) . &lt;tf.Tensor: shape=(4, 2), dtype=int32, numpy= array([[ 0, 0], [ 1, -1], [ 2, -2], [ 3, -3]], dtype=int32)&gt; . &#44256;&#52264;&#50896;&#51032; &#44221;&#50864; . a = tf.reshape(tf.constant(range(12)),(2,2,3)) b = -a a.shape . TensorShape([2, 2, 3]) . a . &lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy= array([[[ 0, 1, 2], [ 3, 4, 5]], [[ 6, 7, 8], [ 9, 10, 11]]], dtype=int32)&gt; . temp = tf.stack([a,b],axis=0) temp.shape . TensorShape([2, 2, 2, 3]) . temp . &lt;tf.Tensor: shape=(2, 2, 2, 3), dtype=int32, numpy= array([[[[ 0, 1, 2], [ 3, 4, 5]], [[ 6, 7, 8], [ 9, 10, 11]]], [[[ 0, -1, -2], [ -3, -4, -5]], [[ -6, -7, -8], [ -9, -10, -11]]]], dtype=int32)&gt; . temp = tf.stack([a,b],axis=1) temp.shape . TensorShape([2, 2, 2, 3]) . temp . &lt;tf.Tensor: shape=(2, 2, 2, 3), dtype=int32, numpy= array([[[[ 0, 1, 2], [ 3, 4, 5]], [[ 0, -1, -2], [ -3, -4, -5]]], [[[ 6, 7, 8], [ 9, 10, 11]], [[ -6, -7, -8], [ -9, -10, -11]]]], dtype=int32)&gt; . temp = tf.stack([a,b],axis=2) temp.shape . TensorShape([2, 2, 2, 3]) . temp . &lt;tf.Tensor: shape=(2, 2, 2, 3), dtype=int32, numpy= array([[[[ 0, 1, 2], [ 0, -1, -2]], [[ 3, 4, 5], [ -3, -4, -5]]], [[[ 6, 7, 8], [ -6, -7, -8]], [[ 9, 10, 11], [ -9, -10, -11]]]], dtype=int32)&gt; . Summary . tf.concat : 케이크를 차원 수만큼 썰고 다시 합하는데 설정한 접시에 수는 변하지 않는다. (즉, 두 그룹의 각각 접시(차원) 수를 합한 차원은 변하지 않는다) | . tf.stack : 케이크를 차원 수만큼 썰고 다시 합하는데 이때는 접시가 하나 추가된다. | . tf.concat 과 tf.stack의 axis 는 axis가 늘어날 수록 세부 요소로 간다는 것!! | . tnp . tf는 넘파이에 비해 만드는 것이 상당히 힘듬 | . 울지말자 나같은 생각을 하는 어떤 천재가 이미 만들어 놨으니 케케 | . import tensorflow.experimental.numpy as tnp tnp.experimental_enable_numpy_behavior() . 이제 자료형이 달라도 연산이 가능하다 | . tnp.array([1,2,3]) + tnp.array([1.0,2.0,3.0]) . &lt;tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 4., 6.])&gt; . tf.constant([1,2,3])+tf.constant([1.0,2.0,3.0]) . &lt;tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 4., 6.])&gt; . tnp.array([1,2,3]) *tnp.array([1.0,2.0,3.0]) . &lt;tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 4., 9.])&gt; . a= tnp.diag([1,2,3]) a . &lt;tf.Tensor: shape=(3, 3), dtype=int64, numpy= array([[1, 0, 0], [0, 2, 0], [0, 0, 3]])&gt; . &#49900;&#51648;&#50612; . tnp.array(1) + tnp.array([1.0,2.0,3.0]) . &lt;tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 3., 4.])&gt; . a= tnp.diag([1,2,3]) a . &lt;tf.Tensor: shape=(3, 3), dtype=int64, numpy= array([[1, 0, 0], [0, 2, 0], [0, 0, 3]])&gt; . a.min(), a.max() . (&lt;tf.Tensor: shape=(), dtype=int64, numpy=0&gt;, &lt;tf.Tensor: shape=(), dtype=int64, numpy=3&gt;) . a.reshape(9,1) . &lt;tf.Tensor: shape=(9, 1), dtype=int64, numpy= array([[1], [0], [0], [0], [2], [0], [0], [0], [3]])&gt; . &#49440;&#50616; &#44256;&#44553; . np.random.randn(5) . array([-1.78435059, 1.29045981, 0.08127629, -0.79964532, 0.72590563]) . tnp.random.randn(5) . &lt;tf.Tensor: shape=(5,), dtype=float64, numpy=array([ 0.96818851, 1.46349868, -1.97239766, 0.93021469, -0.41628377])&gt; . type(np.random.randn(5)) . numpy.ndarray . type(tnp.random.randn(5)) . tensorflow.python.framework.ops.EagerTensor . &#44536;&#47084;&#45208;.... . a = tnp.array(range(4)) a . &lt;tf.Tensor: shape=(4,), dtype=int64, numpy=array([0, 1, 2, 3])&gt; . a[0]=11 . TypeError Traceback (most recent call last) &lt;ipython-input-230-b909b2ec59d1&gt; in &lt;module&gt;() -&gt; 1 a[0]=11 TypeError: &#39;tensorflow.python.framework.ops.EagerTensor&#39; object does not support item assignment . 여전히 기존 변수의 원소 값을 새로운 값으로 할당하는 것은 불가능하다. | . Summary of tnp . numpy 처럼 사용이 가능 | . 심지어 묵시적 형변환도 가능 | . 그러나 여전히 기존 변수의 새로운 값을 할당하는 것은 불가능하다 | . . 3&#51452;&#52264; . import tensorflow as tf import numpy as np import tensorflow.experimental.numpy as tnp tnp.experimental_enable_numpy_behavior() . tf.config.experimental.list_physical_devices(&quot;GPU&quot;) . [PhysicalDevice(name=&#39;/physical_device:GPU:0&#39;, device_type=&#39;GPU&#39;)] . &#51648;&#45212;&#44053;&#51032; &#48372;&#52649; . a = tf.constant(range(1,5)) a . &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt; . a.mean(), tf.reduce_mean(a) . (&lt;tf.Tensor: shape=(), dtype=float64, numpy=2.5&gt;, &lt;tf.Tensor: shape=(), dtype=int32, numpy=2&gt;) . stack . a = tf.reshape(tf.constant(range(12)),(2,2,3)) b=-a a.shape . TensorShape([2, 2, 3]) . tf.stack([a,b],axis=0) . &lt;tf.Tensor: shape=(2, 2, 2, 3), dtype=int32, numpy= array([[[[ 0, 1, 2], [ 3, 4, 5]], [[ 6, 7, 8], [ 9, 10, 11]]], [[[ 0, -1, -2], [ -3, -4, -5]], [[ -6, -7, -8], [ -9, -10, -11]]]], dtype=int32)&gt; . tf.stack([a,b],axis=1) . &lt;tf.Tensor: shape=(2, 2, 2, 3), dtype=int32, numpy= array([[[[ 0, 1, 2], [ 3, 4, 5]], [[ 0, -1, -2], [ -3, -4, -5]]], [[[ 6, 7, 8], [ 9, 10, 11]], [[ -6, -7, -8], [ -9, -10, -11]]]], dtype=int32)&gt; . tf.stack([a,b],axis=2) . &lt;tf.Tensor: shape=(2, 2, 2, 3), dtype=int32, numpy= array([[[[ 0, 1, 2], [ 0, -1, -2]], [[ 3, 4, 5], [ -3, -4, -5]]], [[[ 6, 7, 8], [ -6, -7, -8]], [[ 9, 10, 11], [ -9, -10, -11]]]], dtype=int32)&gt; . tf.stack([a,b],axis=3) . &lt;tf.Tensor: shape=(2, 2, 3, 2), dtype=int32, numpy= array([[[[ 0, 0], [ 1, -1], [ 2, -2]], [[ 3, -3], [ 4, -4], [ 5, -5]]], [[[ 6, -6], [ 7, -7], [ 8, -8]], [[ 9, -9], [ 10, -10], [ 11, -11]]]], dtype=int32)&gt; . tf.stack([a,b],axis=-1) . &lt;tf.Tensor: shape=(2, 2, 3, 2), dtype=int32, numpy= array([[[[ 0, 0], [ 1, -1], [ 2, -2]], [[ 3, -3], [ 4, -4], [ 5, -5]]], [[[ 6, -6], [ 7, -7], [ 8, -8]], [[ 9, -9], [ 10, -10], [ 11, -11]]]], dtype=int32)&gt; . concat . (2,2,3),(2,2,3),(2,2,3) $ to$ (?,?,?) . a = tf.reshape(tf.constant(range(12)),(2,2,3)) b = -a c= 2*a . tf.concat([a,b,c],axis=0) . &lt;tf.Tensor: shape=(6, 2, 3), dtype=int32, numpy= array([[[ 0, 1, 2], [ 3, 4, 5]], [[ 6, 7, 8], [ 9, 10, 11]], [[ 0, -1, -2], [ -3, -4, -5]], [[ -6, -7, -8], [ -9, -10, -11]], [[ 0, 2, 4], [ 6, 8, 10]], [[ 12, 14, 16], [ 18, 20, 22]]], dtype=int32)&gt; . tf.concat([a,b,c],axis=1) . &lt;tf.Tensor: shape=(2, 6, 3), dtype=int32, numpy= array([[[ 0, 1, 2], [ 3, 4, 5], [ 0, -1, -2], [ -3, -4, -5], [ 0, 2, 4], [ 6, 8, 10]], [[ 6, 7, 8], [ 9, 10, 11], [ -6, -7, -8], [ -9, -10, -11], [ 12, 14, 16], [ 18, 20, 22]]], dtype=int32)&gt; . tf.concat([a,b,c],axis=2) . &lt;tf.Tensor: shape=(2, 2, 9), dtype=int32, numpy= array([[[ 0, 1, 2, 0, -1, -2, 0, 2, 4], [ 3, 4, 5, -3, -4, -5, 6, 8, 10]], [[ 6, 7, 8, -6, -7, -8, 12, 14, 16], [ 9, 10, 11, -9, -10, -11, 18, 20, 22]]], dtype=int32)&gt; . a=tf.reshape(tf.constant(range(2*3*4)),(2,3,4)) b=tf.reshape(-tf.constant(range(4*3*4)),(4,3,4)) . tf.concat([a,b],axis=0) . &lt;tf.Tensor: shape=(6, 3, 4), dtype=int32, numpy= array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[ 12, 13, 14, 15], [ 16, 17, 18, 19], [ 20, 21, 22, 23]], [[ 0, -1, -2, -3], [ -4, -5, -6, -7], [ -8, -9, -10, -11]], [[-12, -13, -14, -15], [-16, -17, -18, -19], [-20, -21, -22, -23]], [[-24, -25, -26, -27], [-28, -29, -30, -31], [-32, -33, -34, -35]], [[-36, -37, -38, -39], [-40, -41, -42, -43], [-44, -45, -46, -47]]], dtype=int32)&gt; . - 아래와 같은 경우 axis=0의 차원이 같지 않아 에러가 뜨는 것임 . tf.concat([a,b],axis=1) . InvalidArgumentError Traceback (most recent call last) &lt;ipython-input-270-6f52ef50c654&gt; in &lt;module&gt;() -&gt; 1 tf.concat([a,b],axis=1) /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py in error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name) 7184 def raise_from_not_ok_status(e, name): 7185 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7186 raise core._status_to_exception(e) from None # pylint: disable=protected-access 7187 7188 InvalidArgumentError: ConcatOp : Dimension 0 in both shapes must be equal: shape[0] = [2,3,4] vs. shape[1] = [4,3,4] [Op:ConcatV2] name: concat . tf.concat([a,b],axis=2) . InvalidArgumentError Traceback (most recent call last) &lt;ipython-input-271-faa2763321d0&gt; in &lt;module&gt;() -&gt; 1 tf.concat([a,b],axis=2) /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py in error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name) 7184 def raise_from_not_ok_status(e, name): 7185 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7186 raise core._status_to_exception(e) from None # pylint: disable=protected-access 7187 7188 InvalidArgumentError: ConcatOp : Dimension 0 in both shapes must be equal: shape[0] = [2,3,4] vs. shape[1] = [4,3,4] [Op:ConcatV2] name: concat . (2,2) @ (2,) &#50672;&#49328; . numpy . a = np.diag([1,1]) b = np.array([77,-88]) . a.shape . (2, 2) . b.shape . (2,) . a @ b . array([ 77, -88]) . numpy의 경우 무리없이 잘 되는 것 같다. | . tensorflow . I = tf.constant(np.diag([1,1]),dtype=&quot;float&quot;) x = tf.constant([77.0, -88.0]) . I.shape . TensorShape([2, 2]) . x.shape . TensorShape([2]) . I @ x . &lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([ 77., -88.], dtype=float32)&gt; . x @ I . &lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([ 77., -88.], dtype=float32)&gt; . 원래 안되는 데 위에서 tnp 설정을 해놔서 그럼 | . I @ tf.reshape(x,(2,1)) . &lt;tf.Tensor: shape=(2, 1), dtype=float32, numpy= array([[ 77.], [-88.]], dtype=float32)&gt; . tf.reshape(x,(1,2)) @ I . &lt;tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 77., -88.]], dtype=float32)&gt; . 그냥 tnp 쓰는게 편하다... | . tnp.Variable() . 1. 기본 선언 . tf.Variable(range(1,5)) . &lt;tf.Variable &#39;Variable:0&#39; shape=(4,) dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt; . 2. tf.constant()선언 후 변환 . tf.Variable(tf.constant(range(1,5))) . &lt;tf.Variable &#39;Variable:0&#39; shape=(4,) dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt; . 3. numpy 선언 후 변환 . tf.Variable(np.array([1,2,3,4])) . &lt;tf.Variable &#39;Variable:0&#39; shape=(4,) dtype=int64, numpy=array([1, 2, 3, 4])&gt; . a= tf.Variable([1,2,3,4]) id(a) . 140086641080720 . a . &lt;tf.Variable &#39;Variable:0&#39; shape=(4,) dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt; . a.assign_add([-1,-2,-3,-4]) . &lt;tf.Variable &#39;UnreadVariable&#39; shape=(4,) dtype=int32, numpy=array([0, 0, 0, 0], dtype=int32)&gt; . a . &lt;tf.Variable &#39;Variable:0&#39; shape=(4,) dtype=int32, numpy=array([0, 0, 0, 0], dtype=int32)&gt; . id(a) . 140086641080720 . Summary . tf.constant 와 tf.Variable의 큰 차이는 잘 모르겠음 | . 그냥 tnp가 최고임 | . &#48120;&#48516; . $$y = 3x ^2$$ . 위 식의 $x=2$에서 접선의 기울기를 구해보자 . &#44256;&#46321;&#49688;&#54617; . $$(x=2) to frac { partial}{ partial x} 3x^2 = 6 x = 12$$ . &#52980;&#54504;&#53552;&#47484; &#51060;&#50857;&#54620; &#54400;&#51060; . x1=2 y1=3*x1**2 . x2 = 2+0.000001 y2 = 3*x2**2 . (y2-y1)/(x2-x1) . 12.000003000266702 . 어찌어찌 근사값이 나온다. . &#49324;&#50857;&#51088; &#54632;&#49688;&#47484; &#51221;&#51032; . def f(x) : return (3*x**2) . def d(f,x) : return (f(x+0.0000001)- f(x))/(0.0000001) . d(f,2) . 12.000000282341716 . lambda &#49324;&#50857; . d(lambda x : 3*x**2,2) . 12.000000282341716 . &#44208;&#54633;&#48516;&#54252; . $$f(x,y) = x^2 + 3y $$ . def f(x,y) : return (x**2+3*y) . f(2,3) . 13 . d(f,(2,3)) . TypeError Traceback (most recent call last) &lt;ipython-input-324-b22cba150a6a&gt; in &lt;module&gt;() -&gt; 1 d(f,(2,3)) &lt;ipython-input-317-19118b8b2c89&gt; in d(f, x) 1 def d(f,x) : -&gt; 2 return (f(x+0.0000001)- f(x))/(0.0000001) TypeError: can only concatenate tuple (not &#34;float&#34;) to tuple . 사전에 y값에 대한 설정은 안하고 도함수를 정의했으므로 당연히 에러가 뜬다 | . tf.GradientTape() . $$y = 3x^2$$ . x = tf.Variable(2.0) a = tf.Variable(3.0) . mytape = tf.GradientTape() mytape.__enter__() ## 기록시작 y = a*x**2 ## y 정의 mytape.__exit__(None,None,None) ## 기록끝 . y를 x=2로 미분 | . mytape.gradient(y,x) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=12.0&gt; . . $$a= frac 32 x$$ . $$y = ax^2 = frac 32 x^3$$ . $$ frac { partial y}{ partial x} = frac 92 x^{2}$$ . x = tf.Variable(2.0) mytape = tf.GradientTape() mytape.__enter__() a = (3/2)*x y = a*x**2 mytape.__exit__(None,None,None) . mytape.gradient(y,x) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=18.0&gt; . - 근데 뭐가 좀 이상하다? . 우리는 $x=2$로 설정했기 때문에 $a=3$으로 인식해서 . $$a= 3$$ . $$y = ax^2 = 3 x^2$$ . $$ frac { partial y}{ partial x} = 6x =12$$ . 이렇게 되야지 않나?? . 이걸 풀어서 쓰면 . x = tf.Variable(2.0) a = (3/2)*x mytape = tf.GradientTape() mytape.__enter__() y = a*x**2 mytape.__exit__(None,None,None) . mytape.gradient(y,x) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=12.0&gt; . summary . mytape을 컴퓨터에게 넘겨줄 연습장이라고 생각하자. | . 우리는 a를 연습장 밖에서 선언했기 때문에 a를 변수가 아닌 상수 취급한다 | . 즉,우리는 컴퓨터와 대화하는 입장에서 적절한 위치에 적절한 변수 선언을 해주어 한다. | . with . python . with expression as myname: ## with문 시작: myname.__enter__() blabla ~ yadiyadi !! ## with문 끝: myname.__exit__() . 1. expression의 실행결과 $ to$ 오브젝트 생성 . 2. 생성된 오브젝트는 .__enter__(), .__exit__()를 숨겨진 기능으로 포함해야한다. . 3. with문이 시작되면서 myname.__enter__()가 실행 . 4. 그 후 밑에 짜바리들 실행되고 . 5. 마지막에 myname.__exit__()가 실행되며 종료함 . $$ a = frac 32 x$$ . $$y = a x^2$$ . 위 예제로 돌아와보자 . - 미분한 값이 12로 나오는 경우 . x = tf.Variable(2.0) a = (3/2)*x with tf.GradientTape() as mytape : y=a*x**2 . mytape.gradient(y,x) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=12.0&gt; . - 미분한 값이 정상적인 18로 나오려면? . x = tf.Variable(2.0) with tf.GradientTape() as mytape : a = (3/2)*x y=a*x**2 . mytape.gradient(y,x) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=18.0&gt; . - persistent를 쓰면 값을 보존하기 때문에 mytape.gradient를 두번 실행해도 에러가 나지 않음 . x = tf.Variable(2.0) with tf.GradientTape(persistent=True) as mytape : a = (3/2)*x y=a*x**2 . mytape.gradient(y,x) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=18.0&gt; . - 관찰 1: $x$ 값을 constant로 전달했을 경우 . x=tf.constant(2.0) with tf.GradientTape(persistent=True) as mytape: a=(x/2)*3 ## a=(3/2)x y=a*x**2 ## y=ax^2 = (3/2)x^3 . print(mytape.gradient(y,x)) . None . - $x$를 상수로 선언했기 때문에 변수로 인식하지 않는다. . 2 관찰 2: $x =$ constant, watch적용 . x=tf.constant(2.0) with tf.GradientTape(persistent=True) as mytape: mytape.watch(x) a=(x/2)*3 ## a=(3/2)x y=a*x**2 ## y=ax^2 = (3/2)x^3 . print(mytape.gradient(y,x)) . tf.Tensor(18.0, shape=(), dtype=float32) . - watch는 수동감시모드로 임의의 상수로 선언한 객체를 감시하여 변수처럼 취급한다. . 3 관찰 3 : $x=$ Variable, 자동감시해제 . x=tf.constant(2.0) with tf.GradientTape(persistent=True,watch_accessed_variables=False) as mytape: a=(x/2)*3 ## a=(3/2)x y=a*x**2 ## y=ax^2 = (3/2)x^3 . 자동 감시 모드를 해제하였기 때문에 변수로 선언한 값도 상수취급함 . print(mytape.gradient(y,x)) . None . &#51221;&#47532; . tensorflow의 GradientTape을 이용해 미분값을 구할시 tf.constant와 tf.Variable의 차이가 극명히 들어난다 | . 또한 컴퓨터와 대화하는 메모장선언 시 변수의 위치를 고려해야한다! | . 4&#51452;&#52264; : &#48120;&#48516; . import matplotlib.pyplot as plt import tensorflow as tf import tensorflow.experimental.numpy as tnp tnp.experimental_enable_numpy_behavior() . &#52852;&#54168;&#50696;&#51228; (&#51473;&#44036;&#44256;&#49324; &#44592;&#52636;&#47928;&#51228;) . $$y approx 10.2 + 2.2x + ɛ$$ . x = tnp.array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4]) epsilon = tnp.random.randn(10) y = 10.2 + 2.2*x + epsilon . &#52488;&#44592; &#48288;&#53440;&#44050; &#49444;&#51221; . beta0 = tf.Variable(9.0) beta1 = tf.Variable(2.0) . loss &#51221;&#51032; . with tf.GradientTape(persistent = True) as tape : loss = sum((y-beta0-beta1*x)**2) . &#48120;&#48516;&#44050; &#44228;&#49328; . tape.gradient(loss,beta0), tape.gradient(loss,beta1) . (&lt;tf.Tensor: shape=(), dtype=float32, numpy=-126.90627&gt;, &lt;tf.Tensor: shape=(), dtype=float32, numpy=-3217.9387&gt;) . &#47588;&#53944;&#47533;&#49828; &#48260;&#51204; . - $ bf X to (10 times 2)$ . X = tnp.array([[1.0]*10,x]).reshape(2,10).T . - $ boldsymbol { beta} to (2 times 1)$ . beta_true = tnp.array([10.2,2.2]).reshape(2,1) . - $y to (10 times 1)$ . tnp.random.seed(202150256) y = X@beta_true + tnp.random.randn(10).reshape(10,1) y . &lt;tf.Tensor: shape=(10, 1), dtype=float64, numpy= array([[55.80570022], [58.06839247], [58.76324706], [61.33410375], [62.81827495], [63.45381384], [69.00421863], [70.73413289], [71.64628327], [75.23973972]])&gt; . &#51076;&#51032;&#51032; &#48288;&#53440;&#44050; &#49444;&#51221; . beta_temp = tnp.array([9.0,2.0]).reshape(2,1) . &#47588;&#53944;&#47533;&#49828; &#48260;&#51204;&#51032; &#48120;&#48516;&#44050; &#54869;&#51064; . $$loss = ( bf y - X boldsymbol{ beta})^{ top}( bf y - X boldsymbol{ beta}) =( bf y - hat {y})^{ top}( bf y - hat y) $$ . with tf.GradientTape(persistent = True) as tape : tape.watch(beta_temp) yhat = X@beta_temp loss = (y-yhat).T @ (y-yhat) . - tensorflow가 계산한 손실함수의 미분값 . tape.gradient(loss, beta_temp) . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[ -113.33581358], [-2847.34947422]])&gt; . - 이론적인 값과 동일한지 확인 . $$ frac { partial}{ partial boldsymbol beta } loss = -2 bf X^{ top}y + 2X^{ top}X boldsymbol beta$$ . tape.gradient(loss, beta_temp),-2*X.T@y + 2*X.T@X@beta_temp . (&lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[ -113.33581358], [-2847.34947422]])&gt;, &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[ -113.33581358], [-2847.34947422]])&gt;) . - 베타 추정치 계산 . $$ hat { boldsymbol beta} = left( bf X^{ top}X right )^{-1} bf X^{ top}y $$ . $$y approx 10.2 + 2.2 x$$ . tf.linalg.inv(X.T@X) @ X.T @ y . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[12.84179765], [ 2.07297053]])&gt; . &#51060;&#47200;&#51201; &#48288;&#53440; &#52628;&#51221;&#52824;&#47484; &#51060;&#50857;&#54644; loss&#51032; &#48120;&#48516;&#44050;&#51060; 0&#51064;&#51648; &#54869;&#51064; . - 이론적으로 추정한 $ beta$로 $loss$를 미분했을 때 실제 베타에 대한 최적 추정치가 구해진 것이다. . beta_optimal = tf.linalg.inv(X.T@X) @ X.T @ y . with tf.GradientTape(persistent=True) as tape : tape.watch(beta_optimal) yhat = X@beta_optimal loss = (y-yhat).T @ (y-yhat) . - loss 의 기울기 계산 . tape.gradient(loss, beta_optimal) . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[-6.56541488e-12], [-1.65769620e-10]])&gt; . Summary . 이론적으로 구한 쵲거의 베타를 이용하여 loss를 미분한 결과 0 보단 큰값을 산출하였다. | . 그러나 표본의 수가 커질 경우 최적치와 실제 베타값이 일치해질 것이다 (대수의 법칙!!) | . &#44221;&#49324;&#54616;&#44053;&#48277; . $$loss = left ( frac 12 beta -1 right )^2$$ . 위 같은 경우 당연히 $ beta = 2$일 때 최적화이다. | . 이를 컴퓨터로 구현 | . &#48169;&#48277; 1: grid search . 단순히 베타를 개많이 만들고 그 중에서 최적의 베타를 찾는다. | . beta = tnp.linspace(-10,10,1000) . loss = ((1/2)*beta -1 )**2 . 찾아본 결과 599 번째 베타에서loss 값이 최소가 된다. | . tnp.argmin(loss) . &lt;tf.Tensor: shape=(), dtype=int64, numpy=599&gt; . loss[599] . &lt;tf.Tensor: shape=(), dtype=float64, numpy=1.603204806408293e-05&gt; . &#47928;&#51228;&#51216; . 만약 설정한 범위밖에 해가 존재할 수 있음 | . 최적의 해를 찾았지만 그 이후에도 탐색이 계속 진행됨 | . &#48169;&#48277; 2: &#54869;&#47456;&#51201; &#44221;&#49324;&#54616;&#44053;&#48277; (Stochastic Grandient Descent) . step 1. 초기 베타값 설정 . ((1/2)*(-5) -1)**2 . 12.25 . step 2. beta = -5근처에서 조금씩 이동 . ((1/2)*(-4.99) -1)**2 . 12.215025 . ((1/2)*(-5.01) -1)**2 . 12.285025 . step 3. 위 결과를 보고 유리한 쪽으로 이동 . step 4. 위 과정을 반복하고 어느쪽으로 이동해도 이득이 없다면 멈춤 . 이를 수식화하면 . $$ beta_{new} = beta_{old} + 0.01 , to , , frac {∂}{ partial beta} loss &lt; 0 $$ . $$ beta_{new} = beta_{old} - 0.01 , to , , frac {∂}{ partial beta} loss &gt; 0 $$ . 알고리즘의 개선 $ to$ 동일하게 0.01 씩 이동하는 것이 맞나? | . $$loss = left ( frac 12 beta -1 right )^2$$ . 위의 경우 $ beta = 2$ 가 최적해임을 알고 있다. . $ beta = -10 $ 일경우 $loss^{ prime} = -6$ . $ beta =-4 $ 일경우 $loss^{ prime} = -3$ . 위 같은 경우 최적해를 찾을 때 동일한 각 베타 지점에서 동일한 거리만큼 이동하면 계산 시간이 낭비된다. | . 따라서 이를 $ alpha $ (learning rate) 를 이용하여 멀리 떨어질 수록 자신있게 성큼성큼 이동하자는 것이다. | . $$ beta_{new } = beta_{old} - alpha times loss^{ prime}( beta_{old}), quad ( alpha &gt;0) $$ . &#44396;&#54788; &#53076;&#46300; . iter1 : $ beta = -10$에서 출발 . beta = tf.Variable(-10.0) . with tf.GradientTape(persistent=True) as tape: loss = ((1/2)*beta-1)**2 . tape.gradient(loss,beta) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=-6.0&gt; . $ alpha$값 설정 . alpha = 0.01/6 . 계산된 $ beta$값 으로 값 변경 . beta.assign_sub(alpha*tape.gradient(loss,beta)) . &lt;tf.Variable &#39;UnreadVariable&#39; shape=() dtype=float32, numpy=-9.99&gt; . iter $ beta = -9.99$ . with tf.GradientTape(persistent=True) as tape: loss = ((1/2)*beta-1)**2 . tape.gradient(loss,beta) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=-5.995&gt; . beta.assign_sub(alpha*tape.gradient(loss,beta)) . &lt;tf.Variable &#39;UnreadVariable&#39; shape=() dtype=float32, numpy=-9.980008&gt; . 걍 for문을 이용하자 . beta = tf.Variable(-10.0) alpha = 0.01/6 . for i in range(100) : with tf.GradientTape(persistent=True) as tape: loss = ((1/2)*beta-1)**2 beta.assign_sub(tape.gradient(loss,beta)) . beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=2.0&gt; . &#54617;&#49845;&#47456;&#50640; &#46384;&#47480; &#52572;&#51201;&#54644; &#49688;&#47156; . $beta : -10 to -9 to -8$ 이런식으로 이동한다고 가정 | . beta_lst = tnp.linspace(-10.0,-8.0,3) . loss = ((beta_lst/2)-1)**2 . fig = plt.figure() ## 도화지 생성 ax = fig.add_subplot() ## 도화지안의 틀 생성 _beta = tnp.linspace(-15,19,100) ## 도로생성한다고 생각하자 _loss = ((_beta/2)-1)**2 ax.plot(_beta,_loss) pnts, = ax.plot(beta_lst[0],loss[0],&quot;ro&quot;) . 애니메이션 정의 . plt.rcParams[&quot;animation.html&quot;]=&quot;jshtml&quot; from matplotlib import animation . def animate(i): pnts.set_xdata(beta_lst[:(i+1)]) pnts.set_ydata(loss[:(i+1)]) . animation.FuncAnimation(fig,animate,frames=3) . &lt;/input&gt; Once Loop Reflect 확률적 경사하강법에 의도대로 애니메이션을 구현해보자 | . beta = tf.Variable(-10.0) _loss = ((beta.numpy()/2)-1)**2 alpha = 0.1 . beta_lst = [] loss_lst = [] beta_lst.append(beta.numpy()) loss_lst.append(_loss) . for i in range(100) : with tf.GradientTape(persistent=True) as tape : tape.watch(beta) loss = (beta/2-1)**2 beta.assign_sub(alpha*tape.gradient(loss,beta)) beta_lst.append(beta.numpy()) loss_lst.append((beta.numpy()/2-1)**2) . _beta = tnp.linspace(-15,19,100) _loss = ((_beta/2)-1)**2 . fig = plt.figure() ax = fig.add_subplot() ax.plot(_beta,_loss) pnts, = ax.plot(beta_lst[0],loss_lst[0],&quot;ro&quot;) . def animate(i): pnts.set_xdata(beta_lst[:(i+1)]) pnts.set_ydata(loss_lst[:(i+1)]) . animation.FuncAnimation(fig,animate,frames=100) . &lt;/input&gt; Once Loop Reflect 4&#51452;&#52264; &#49689;&#51228; . $y= (x-1)^2$의를 최소화하는 $x$를 확률적 경사하강법을 이용햇 찾고 수렴과정을 애니메이션으로 시각화 . x = tf.Variable(-3.0) _y = (x.numpy()-1)**2 alpha = 0.1 x_lst = [x.numpy()] y_lst = [_y] for i in range(100) : with tf.GradientTape(persistent = True) as tape: y = (x-1)**2 x.assign_sub(alpha*tape.gradient(y,x)) x_lst.append(x.numpy()) y_lst.append((x.numpy()-1)**2) ## 여기도 x.numpy 무조건 해주자 ## 안해주면 애니메이션작살난다 . _x = tnp.linspace(-4,6) _y = (_x-1)**2 . fig =plt.figure() ax= fig.add_subplot() ax.plot(_x,_y) pnts, = ax.plot(_x[0],_y[0],&quot;ro&quot;) . def animate(i): pnts.set_xdata(x_lst[:(i+1)]) pnts.set_ydata(y_lst[:(i+1)]) animation.FuncAnimation(fig,animate,frames=100) . &lt;/input&gt; Once Loop Reflect",
            "url": "https://gangcheol.github.io/big-data-analysis/2022/04/19/(%EC%A4%91%EA%B0%84%EA%B3%A0%EC%82%AC-%EB%8C%80%EB%B9%84).html",
            "relUrl": "/2022/04/19/(%EC%A4%91%EA%B0%84%EA%B3%A0%EC%82%AC-%EB%8C%80%EB%B9%84).html",
            "date": " • Apr 19, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "(6주차) 4월11일",
            "content": "&#44053;&#51032;&#50689;&#49345; . imports . import numpy as np import matplotlib.pyplot as plt import tensorflow as tf import tensorflow.experimental.numpy as tnp . tnp.experimental_enable_numpy_behavior() . import graphviz def gv(s): return graphviz.Source(&#39;digraph G{ rankdir=&quot;LR&quot;&#39;+s + &#39;; }&#39;) . $x to hat{y}$ &#44032; &#46104;&#45716; &#44284;&#51221;&#51012; &#44536;&#47548;&#51004;&#47196; &#44536;&#47532;&#44592; . - 단순회귀분석의 예시 . $ hat{y}_i = hat{ beta}_0 + hat{ beta}_1 x_i, quad i=1,2, dots,n$ | . (표현1) . gv(&#39;&#39;&#39; &quot;1&quot; -&gt; &quot;β̂₀ + xₙ*β̂₁, bias=False&quot;[label=&quot;* β̂₀&quot;] &quot;xₙ&quot; -&gt; &quot;β̂₀ + xₙ*β̂₁, bias=False&quot;[label=&quot;* β̂₁&quot;] &quot;β̂₀ + xₙ*β̂₁, bias=False&quot; -&gt; &quot;ŷₙ&quot;[label=&quot;identity&quot;] &quot;.&quot; -&gt; &quot;....................................&quot;[label=&quot;* β̂₀&quot;] &quot;..&quot; -&gt; &quot;....................................&quot;[label=&quot;* β̂₁&quot;] &quot;....................................&quot; -&gt; &quot;...&quot;[label=&quot; &quot;] &quot;1 &quot; -&gt; &quot;β̂₀ + x₂*β̂₁, bias=False&quot;[label=&quot;* β̂₀&quot;] &quot;x₂&quot; -&gt; &quot;β̂₀ + x₂*β̂₁, bias=False&quot;[label=&quot;* β̂₁&quot;] &quot;β̂₀ + x₂*β̂₁, bias=False&quot; -&gt; &quot;ŷ₂&quot;[label=&quot;identity&quot;] &quot;1 &quot; -&gt; &quot;β̂₀ + x₁*β̂₁, bias=False&quot;[label=&quot;* β̂₀&quot;] &quot;x₁&quot; -&gt; &quot;β̂₀ + x₁*β̂₁, bias=False&quot;[label=&quot;* β̂₁&quot;] &quot;β̂₀ + x₁*β̂₁, bias=False&quot; -&gt; &quot;ŷ₁&quot;[label=&quot;identity&quot;] &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G 1 1 β̂₀ + xₙ*β̂₁, &#160;&#160;&#160;bias=False β̂₀ + xₙ*β̂₁, &#160;&#160;&#160;bias=False 1&#45;&gt;β̂₀ + xₙ*β̂₁, &#160;&#160;&#160;bias=False * β̂₀ ŷₙ ŷₙ β̂₀ + xₙ*β̂₁, &#160;&#160;&#160;bias=False&#45;&gt;ŷₙ identity xₙ xₙ xₙ&#45;&gt;β̂₀ + xₙ*β̂₁, &#160;&#160;&#160;bias=False * β̂₁ . . .................................... .................................... .&#45;&gt;.................................... * β̂₀ ... ... ....................................&#45;&gt;... .. .. ..&#45;&gt;.................................... * β̂₁ 1 1 β̂₀ + x₂*β̂₁, &#160;&#160;&#160;bias=False β̂₀ + x₂*β̂₁, &#160;&#160;&#160;bias=False 1 &#45;&gt;β̂₀ + x₂*β̂₁, &#160;&#160;&#160;bias=False * β̂₀ ŷ₂ ŷ₂ β̂₀ + x₂*β̂₁, &#160;&#160;&#160;bias=False&#45;&gt;ŷ₂ identity x₂ x₂ x₂&#45;&gt;β̂₀ + x₂*β̂₁, &#160;&#160;&#160;bias=False * β̂₁ 1 &#160; 1 &#160; β̂₀ + x₁*β̂₁, &#160;&#160;&#160;bias=False β̂₀ + x₁*β̂₁, &#160;&#160;&#160;bias=False 1 &#160;&#45;&gt;β̂₀ + x₁*β̂₁, &#160;&#160;&#160;bias=False * β̂₀ ŷ₁ ŷ₁ β̂₀ + x₁*β̂₁, &#160;&#160;&#160;bias=False&#45;&gt;ŷ₁ identity x₁ x₁ x₁&#45;&gt;β̂₀ + x₁*β̂₁, &#160;&#160;&#160;bias=False * β̂₁ - 표현1의 소감? . 교수님이 고생해서 만든것 같음 | 그런데 그냥 다 똑같은 그림의 반복이라 사실 고생한 의미가 없음. | . (표현2) . - 그냥 아래와 같이 그리고 &quot;모든 $i=1,2,3, dots,n$에 대하여 $ hat{y}_i$을 아래의 그림과 같이 그린다&quot;고 하면 될것 같다. . gv(&#39;&#39;&#39; &quot;1&quot; -&gt; &quot;β̂₀ + xᵢ*β̂₁, bias=False&quot;[label=&quot;* β̂₀&quot;] &quot;xᵢ&quot; -&gt; &quot;β̂₀ + xᵢ*β̂₁, bias=False&quot;[label=&quot;* β̂₁&quot;] &quot;β̂₀ + xᵢ*β̂₁, bias=False&quot; -&gt; &quot;ŷᵢ&quot;[label=&quot;identity&quot;] &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G 1 1 β̂₀ + xᵢ*β̂₁, &#160;&#160;&#160;bias=False β̂₀ + xᵢ*β̂₁, &#160;&#160;&#160;bias=False 1&#45;&gt;β̂₀ + xᵢ*β̂₁, &#160;&#160;&#160;bias=False * β̂₀ ŷᵢ ŷᵢ β̂₀ + xᵢ*β̂₁, &#160;&#160;&#160;bias=False&#45;&gt;ŷᵢ identity xᵢ xᵢ xᵢ&#45;&gt;β̂₀ + xᵢ*β̂₁, &#160;&#160;&#160;bias=False * β̂₁ (표현3) . - 그런데 &quot;모든 $i=1,2,3, dots,n$에 대하여 $ hat{y}_i$을 아래의 그림과 같이 그린다&quot; 라는 언급자체도 반복할 필요가 없을 것 같다. (어차피 당연히 그럴테니까) 그래서 단순히 아래와 같이 그려도 무방할듯 하다. . gv(&#39;&#39;&#39; &quot;1&quot; -&gt; &quot;β̂₀ + x*β̂₁, bias=False&quot;[label=&quot;* β̂₀&quot;] &quot;x&quot; -&gt; &quot;β̂₀ + x*β̂₁, bias=False&quot;[label=&quot;* β̂₁&quot;] &quot;β̂₀ + x*β̂₁, bias=False&quot; -&gt; &quot;ŷ&quot;[label=&quot;identity&quot;] &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G 1 1 β̂₀ + x*β̂₁, &#160;&#160;&#160;bias=False β̂₀ + x*β̂₁, &#160;&#160;&#160;bias=False 1&#45;&gt;β̂₀ + x*β̂₁, &#160;&#160;&#160;bias=False * β̂₀ ŷ ŷ β̂₀ + x*β̂₁, &#160;&#160;&#160;bias=False&#45;&gt;ŷ identity x x x&#45;&gt;β̂₀ + x*β̂₁, &#160;&#160;&#160;bias=False * β̂₁ (표현4) . - 위의 모델은 아래와 같이 쓸 수 있다. ($ beta_0$를 바이어스로 표현) . gv(&#39;&#39;&#39; &quot;x&quot; -&gt; &quot;x*β̂₁, bias=True&quot;[label=&quot;*β̂₁&quot;] ; &quot;x*β̂₁, bias=True&quot; -&gt; &quot;ŷ&quot;[label=&quot;indentity&quot;] &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G x x x*β̂₁, &#160;&#160;&#160;bias=True x*β̂₁, &#160;&#160;&#160;bias=True x&#45;&gt;x*β̂₁, &#160;&#160;&#160;bias=True *β̂₁ ŷ ŷ x*β̂₁, &#160;&#160;&#160;bias=True&#45;&gt;ŷ indentity 실제로는 이 표현을 많이 사용함 | . (표현5) . - 벡터버전으로 표현하면 아래와 같다. 이 경우에는 ${ bf X}=[1,x]$에 포함된 1이 bias의 역할을 해주므로 bias = False 임. . gv(&#39;&#39;&#39; &quot;X&quot; -&gt; &quot;X@β̂, bias=False&quot;[label=&quot;@β̂&quot;] ; &quot;X@β̂, bias=False&quot; -&gt; &quot;ŷ&quot;[label=&quot;indentity&quot;] &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G X X X@β̂, &#160;&#160;&#160;bias=False X@β̂, &#160;&#160;&#160;bias=False X&#45;&gt;X@β̂, &#160;&#160;&#160;bias=False @β̂ ŷ ŷ X@β̂, &#160;&#160;&#160;bias=False&#45;&gt;ŷ indentity 저는 이걸 좋아해요 | . (표현6) . - 딥러닝에서는 $ hat{ boldsymbol{ beta}}$ 대신에 $ hat$을 라고 표현한다. . gv(&#39;&#39;&#39; &quot;X&quot; -&gt; &quot;X@Ŵ, bias=False&quot;[label=&quot;@Ŵ&quot;] ; &quot;X@Ŵ, bias=False&quot; -&gt; &quot;ŷ&quot;[label=&quot;identity&quot;] &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G X X X@Ŵ, &#160;&#160;&#160;bias=False X@Ŵ, &#160;&#160;&#160;bias=False X&#45;&gt;X@Ŵ, &#160;&#160;&#160;bias=False @Ŵ ŷ ŷ X@Ŵ, &#160;&#160;&#160;bias=False&#45;&gt;ŷ identity - 실제로는 표현4 혹은 표현5를 외우면 된다. . Layer&#51032; &#44060;&#45392; . - (표현4) 혹은 (표현5)의 그림은 레이어로 설명할 수 있다. . - 레이어는 항상 아래와 같은 규칙을 가진다. . 첫 동그라미는 레이어의 입력이다. | 첫번째 화살표는 선형변환을 의미한다. | 두번째 동그라미는 선형변환의 결과이다. (이때 bias가 false인지 true인지에 따라서 실제 수식이 조금 다름) | 두번째 화살표는 두번째 동그라미에 어떠한 함수 $f$를 취하는 과정을 의미한다. | 세번째 동그라미는 레이어의 최종출력이다. | . - 엄청 복잡한데, 결국 레이어를 만들때 위의 그림들을 의미하도록 하려면 아래의 4개의 요소만 필요하다. . 레이어의 입력차원 | 선형변환의 결과로 얻어지는 차원 | 선형변환에서 바이어스를 쓸지? 안쓸지? | 함수 $f$ | - 주목: 1,2가 결정되면 자동으로 $ hat$의 차원이 결정된다. . (예시) . 레이어의 입력차원=2, 선형변환의 결과로 얻어지는 차원=1: $ hat{ bf W}$는 (2,1) 매트릭스 | 레이어의 입력차원=20, 선형변환의 결과로 얻어지는 차원=5: $ hat{ bf W}$는 (20,5) 매트릭스 | 레이어의 입력차원=2, 선형변환의 결과로 얻어지는 차원=50: $ hat{ bf W}$는 (2,50) 매트릭스 | . - 주목2: 이중에서 절대 생략불가능 것은 &quot;2. 선형변환의 결과로 얻어지는 차원&quot; 이다. . 레이어의 입력차원: 실제 레이어에 데이터가 들어올 때 데이터의 입력차원을 컴퓨터 스스로 체크하여 $ hat{ bf W}$의 차원을 결정할 수 있음. | 바이어스를 쓸지? 안쓸지? 기본적으로 쓴다고 가정한다. | 함수 $f$: 기본적으로 항등함수를 가정하면 된다. | . Keras&#47484; &#51060;&#50857;&#54620; &#54400;&#51060; . - 기본뼈대: net생성 $ to$ add(layer) $ to$ compile(opt,loss) $ to$ fit(data,epochs) . - 데이터정리 . $${ bf y} approx 2.5 +4x$$ . tnp.random.seed(43052) N= 200 x= tnp.linspace(0,1,N) epsilon= tnp.random.randn(N)*0.5 y= 2.5+4*x +epsilon . X=tf.stack([tf.ones(N,dtype=&#39;float64&#39;),x],axis=1) . &#54400;&#51060;1: &#49828;&#52860;&#46972;&#48260;&#51204; . (0단계) 데이터정리 . y=y.reshape(N,1) x=x.reshape(N,1) x.shape,y.shape . (TensorShape([200, 1]), TensorShape([200, 1])) . (1단계) net 생성 . net = tf.keras.Sequential() . (2단계) net.add(layer) . layer = tf.keras.layers.Dense(1) ## 선형변환의 결과 차원 # 입력차원? 데이터를 넣어보고 결정, 바이어스=디폴드값을 쓰겠음 (use_bias=true), 함수도 디폴트값을 쓰겠음 (f(x)=x) net.add(layer) . (3단계) net.compile(opt,loss_fn) . net.compile(tf.keras.optimizers.SGD(0.1), tf.keras.losses.MSE) . (4단계) net.fit(x,y,epochs) . net.fit(x,y,epochs=1000,verbose=0,batch_size=N) # batch_size=N 일 경우에 경사하강법이 적용, batch_size!=N 이면 확률적 경사하강법 적용 . &lt;keras.callbacks.History at 0x7ff91540e790&gt; . (결과확인) . net.weights . [&lt;tf.Variable &#39;dense/kernel:0&#39; shape=(1, 1) dtype=float32, numpy=array([[3.9330251]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense/bias:0&#39; shape=(1,) dtype=float32, numpy=array([2.5836723], dtype=float32)&gt;] . $${ bf y} approx 2.5 +4x$$ . &#54400;&#51060;2: &#48289;&#53552;&#48260;&#51204; . (0단계) 데이터정리 . X.shape,y.shape . (TensorShape([200, 2]), TensorShape([200, 1])) . (1단계) net 생성 . net = tf.keras.Sequential() . (2단계) net.add(layer) . layer = tf.keras.layers.Dense(1,use_bias=False) net.add(layer) . (3단계) net.compile(opt,loss_fn) . net.compile(tf.keras.optimizers.SGD(0.1), tf.keras.losses.MSE) . (4단계) net.fit(x,y,epochs) . net.fit(X,y,epochs=1000,verbose=0,batch_size=N) # batch_size=N 일 경우에 경사하강법이 적용, batch_size!=N 이면 확률적 경사하강법 적용 . &lt;keras.callbacks.History at 0x7ff91550e210&gt; . (결과확인) . net.weights . [&lt;tf.Variable &#39;dense_1/kernel:0&#39; shape=(2, 1) dtype=float32, numpy= array([[2.5836728], [3.9330244]], dtype=float32)&gt;] . &#51104;&#49884;&#47928;&#48277;&#51221;&#47532; . - 잠깐 Dense layer를 만드는 코드를 정리해보자. . (1) 아래는 모두 같은 코드이다. . tf.keras.layers.Dense(1) | tf.keras.layers.Dense(units=1) | tf.keras.layers.Dense(units=1,activation=&#39;linear&#39;) // identity 가 더 맞는것 같은데.. | tf.keras.layers.Dense(units=1,activation=&#39;linear&#39;,use_bias=True) | . (2) 아래의 코드1,2는 (1)의 코드들과 살짝 다른코드이다. (코드1과 코드2는 같은코드임) . tf.keras.layers.Dense(1,input_dim=2) # 코드1 | tf.keras.layers.Dense(1,input_shape=(2,)) # 코드2 | . (3) 아래는 사용불가능한 코드이다. . tf.keras.layers.Dense(1,input_dim=(2,)) # 코드1 | tf.keras.layers.Dense(1,input_shape=2) # 코드2 | . - 왜 input_dim이 필요한가? . net1 = tf.keras.Sequential() net1.add(tf.keras.layers.Dense(1,use_bias=False)) . net2 = tf.keras.Sequential() net2.add(tf.keras.layers.Dense(1,use_bias=False,input_dim=2)) . - net1의 경우 input_dim을 명시해주지 않아 Weight를 알 수 없다 . net1.weights . ValueError Traceback (most recent call last) &lt;ipython-input-22-be60abe348fc&gt; in &lt;module&gt;() -&gt; 1 net1.weights /usr/local/lib/python3.7/dist-packages/keras/engine/training.py in weights(self) 2733 A list of variables. 2734 &#34;&#34;&#34; -&gt; 2735 return self._dedup_weights(self._undeduplicated_weights) 2736 2737 @property /usr/local/lib/python3.7/dist-packages/keras/engine/training.py in _undeduplicated_weights(self) 2738 def _undeduplicated_weights(self): 2739 &#34;&#34;&#34;Returns the undeduplicated list of all layer variables/weights.&#34;&#34;&#34; -&gt; 2740 self._assert_weights_created() 2741 weights = [] 2742 for layer in self._self_tracked_trackables: /usr/local/lib/python3.7/dist-packages/keras/engine/sequential.py in _assert_weights_created(self) 470 # When the graph has not been initialized, use the Model&#39;s implementation to 471 # to check if the weights has been created. --&gt; 472 super(functional.Functional, self)._assert_weights_created() # pylint: disable=bad-super-call 473 474 /usr/local/lib/python3.7/dist-packages/keras/engine/training.py in _assert_weights_created(self) 2931 # been invoked yet, this will cover both sequential and subclass model. 2932 # Also make sure to exclude Model class itself which has build() defined. -&gt; 2933 raise ValueError(f&#39;Weights for model {self.name} have not yet been &#39; 2934 &#39;created. &#39; 2935 &#39;Weights are created when the Model is first called on &#39; ValueError: Weights for model sequential_2 have not yet been created. Weights are created when the Model is first called on inputs or `build()` is called with an `input_shape`. . net2.weights . [&lt;tf.Variable &#39;dense_3/kernel:0&#39; shape=(2, 1) dtype=float32, numpy= array([[-1.053657 ], [ 1.3536845]], dtype=float32)&gt;] . - 또한 입력차원을 모르기 깨문에 summary값도 알 수 없다. . net1.summary() . ValueError Traceback (most recent call last) &lt;ipython-input-24-7ddacd115786&gt; in &lt;module&gt;() -&gt; 1 net1.summary() /usr/local/lib/python3.7/dist-packages/keras/engine/training.py in summary(self, line_length, positions, print_fn, expand_nested, show_trainable) 2774 if not self.built: 2775 raise ValueError( -&gt; 2776 &#39;This model has not yet been built. &#39; 2777 &#39;Build the model first by calling `build()` or by calling &#39; 2778 &#39;the model on a batch of data.&#39;) ValueError: This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data. . net2.summary() . Model: &#34;sequential_3&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_3 (Dense) (None, 1) 2 ================================================================= Total params: 2 Trainable params: 2 Non-trainable params: 0 _________________________________________________________________ . &#54400;&#51060;3: &#49828;&#52860;&#46972;&#48260;&#51204;, &#51076;&#51032;&#51032; &#52488;&#44592;&#44050;&#51012; &#49444;&#51221; . (0단계) 데이터정리 . y=y.reshape(N,1) x=x.reshape(N,1) x.shape,y.shape . (TensorShape([200, 1]), TensorShape([200, 1])) . (1단계) net생성 . net = tf.keras.Sequential() . (2단계) net.add(layer) . layer = tf.keras.layers.Dense(1,input_dim=1) . net.add(layer) . . 초기값을 설정 . net.weights . [&lt;tf.Variable &#39;dense_4/kernel:0&#39; shape=(1, 1) dtype=float32, numpy=array([[0.534932]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_4/bias:0&#39; shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)&gt;] . net.get_weights() . [array([[0.534932]], dtype=float32), array([0.], dtype=float32)] . weight, bias순으로 출력 | . net.set_weights? . layer_b.set_weights(layer_a.get_weights()) 와 같은방식으로 쓴다는 것이군? | . - 한번따라해보자. . _w = net.get_weights() _w . [array([[0.534932]], dtype=float32), array([0.], dtype=float32)] . _w[0] . array([[0.534932]], dtype=float32) . 길이가 2인 리스트이고, 각 원소는 numpy array 임 | . net.set_weights( [np.array([[10.0]],dtype=np.float32), # weight, β1_hat np.array([-5.0],dtype=np.float32)] # bias, β0_hat ) . net.weights . [&lt;tf.Variable &#39;dense_4/kernel:0&#39; shape=(1, 1) dtype=float32, numpy=array([[10.]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_4/bias:0&#39; shape=(1,) dtype=float32, numpy=array([-5.], dtype=float32)&gt;] . . (3단계) net.compile() . net.compile(tf.keras.optimizers.SGD(0.1),tf.losses.MSE) . (4단계) net.fit() . net.fit(x,y,epochs=1000,verbose=0,batch_size=N) . &lt;keras.callbacks.History at 0x7ff9152a5c50&gt; . 결과확인 . net.weights . [&lt;tf.Variable &#39;dense_4/kernel:0&#39; shape=(1, 1) dtype=float32, numpy=array([[3.933048]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_4/bias:0&#39; shape=(1,) dtype=float32, numpy=array([2.58366], dtype=float32)&gt;] . &#54400;&#51060;4: &#48289;&#53552;&#48260;&#51204;, &#51076;&#51032;&#51032; &#52488;&#44592;&#44050;&#51012; &#49444;&#51221; . (0단계) 데이터정리 . X.shape, y.shape . (TensorShape([200, 2]), TensorShape([200, 1])) . (1단계) net생성 . net = tf.keras.Sequential() . (2단계) net.add(layer) . layer = tf.keras.layers.Dense(1,use_bias=False,input_dim=2) . net.add(layer) . . 초기값을 설정하자 . net.set_weights([np.array([[ -5.0],[10.0]], dtype=np.float32)]) . net.get_weights() . [array([[-5.], [10.]], dtype=float32)] . . (3단계) net.compile() . net.compile(tf.keras.optimizers.SGD(0.1), tf.losses.MSE) . (4단계) net.fit() . net.fit(X,y,epochs=1000,verbose=0,batch_size=N) . &lt;keras.callbacks.History at 0x7ff9151dbad0&gt; . net.weights . [&lt;tf.Variable &#39;dense_5/kernel:0&#39; shape=(2, 1) dtype=float32, numpy= array([[2.58366 ], [3.933048]], dtype=float32)&gt;] . - 사실 실전에서는 초기값을 설정할 필요가 별로 없음. . &#54400;&#51060;5: &#48289;&#53552;&#48260;&#51204; &#49324;&#50857;&#51088;&#51221;&#51032; &#49552;&#49892;&#54632;&#49688; . (0단계) 데이터정리 . X.shape, y.shape . (TensorShape([200, 2]), TensorShape([200, 1])) . (1단계) net생성 . net = tf.keras.Sequential() . (2단계) net.add(layer) . layer = tf.keras.layers.Dense(1,use_bias=False) . net.add(layer) . (3단계) net.compile() . loss_fn = lambda y,yhat: (y-yhat).T @ (y-yhat) / N . net.compile(tf.keras.optimizers.SGD(0.1), loss_fn) . (4단계) net.fit() . net.fit(X,y,epochs=1000,verbose=0,batch_size=N) . &lt;keras.callbacks.History at 0x7ff915103dd0&gt; . net.weights . [&lt;tf.Variable &#39;dense_6/kernel:0&#39; shape=(2, 1) dtype=float32, numpy= array([[2.5836728], [3.9330244]], dtype=float32)&gt;] . &#54400;&#51060;6: &#48289;&#53552;&#48260;&#51204;, net.compile&#51032; &#50741;&#49496;&#51004;&#47196; &#49552;&#49892;&#54632;&#49688; &#51648;&#51221; . (0단계) 데이터정리 . X.shape, y.shape . (TensorShape([200, 2]), TensorShape([200, 1])) . (1단계) net생성 . net = tf.keras.Sequential() . (2단계) net.add(layer) . net.add(tf.keras.layers.Dense(1,use_bias=False)) . (3단계) net.compile() . net.compile(tf.keras.optimizers.SGD(0.1), loss=&#39;mse&#39;) . (4단계) net.fit() . net.fit(X,y,epochs=1000,verbose=0,batch_size=N) . &lt;keras.callbacks.History at 0x7ff91502bb50&gt; . net.weights . [&lt;tf.Variable &#39;dense_7/kernel:0&#39; shape=(2, 1) dtype=float32, numpy= array([[2.5836728], [3.9330244]], dtype=float32)&gt;] . &#54400;&#51060;7: &#48289;&#53552;&#48260;&#51204;, net.compile&#51032; &#50741;&#49496;&#51004;&#47196; &#49552;&#49892;&#54632;&#49688; &#51648;&#51221; + &#50741;&#54000;&#47560;&#51060;&#51200; &#51648;&#51221; . (0단계) 데이터정리 . X.shape, y.shape . (TensorShape([200, 2]), TensorShape([200, 1])) . (1단계) net생성 . net = tf.keras.Sequential() . (2단계) net.add(layer) . net.add(tf.keras.layers.Dense(1,use_bias=False)) . (3단계) net.compile() . net.compile(optimizer=&#39;sgd&#39;, loss=&#39;mse&#39;) #net.optimizer.lr = tf.Variable(0.1,dtype=tf.float32) #net.optimizer.lr = 0.1 . (4단계) net.fit() . net.fit(X,y,epochs=5000,verbose=0,batch_size=N) . &lt;keras.callbacks.History at 0x7ff915117550&gt; . net.weights . [&lt;tf.Variable &#39;dense_8/kernel:0&#39; shape=(2, 1) dtype=float32, numpy= array([[2.5842712], [3.9319096]], dtype=float32)&gt;] . &#50668;&#47084;&#44032;&#51648; &#54924;&#44480;&#47784;&#54805;&#51032; &#51201;&#54633;&#44284; &#54617;&#49845;&#44284;&#51221;&#51032; &#47784;&#45768;&#53552;&#47553; . &#50696;&#51228;1 . model: $y_i approx beta_0 + beta_1 x_i$ . np.random.seed(43052) N= 100 x= np.random.randn(N) epsilon = np.random.randn(N)*0.5 y= 2.5+4*x +epsilon . X= np.stack([np.ones(N),x],axis=1) y= y.reshape(N,1) . plt.plot(x,y,&#39;o&#39;) # 관측한 자료 . [&lt;matplotlib.lines.Line2D at 0x7ff914ed2250&gt;] . beta_hat = np.array([-3,-2]).reshape(2,1) . yhat = X@beta_hat . plt.plot(x,y,&#39;o&#39;) plt.plot(x,yhat.reshape(-1),&#39;-&#39;) . [&lt;matplotlib.lines.Line2D at 0x7ff914e5d690&gt;] . 더 좋은 적합선을 얻기위해서! . slope = (2*X.T@X@beta_hat - 2*X.T@y)/ N beta_hat2 = beta_hat - 0.1*slope yhat2 = X@beta_hat2 . plt.plot(x,y,&#39;o&#39;) plt.plot(x,yhat.reshape(-1),&#39;-&#39;) plt.plot(x,yhat2.reshape(-1),&#39;-&#39;) . [&lt;matplotlib.lines.Line2D at 0x7ff90f750ed0&gt;] . 초록색이 좀 더 나아보인다. . beta_hat = np.array([-3,-2]).reshape(2,1) beta_hats = beta_hat # beta_hats = beta_hat.copy() 가 더 안전한 코드입니다. for i in range(1,30): yhat = X@beta_hat slope = (2*X.T@X@beta_hat - 2*X.T@y) / N beta_hat = beta_hat - 1.0*slope # 0.1은 적당, 0.3은 쪼금빠르지만 그래도 적당, 0.9는 너무 나간것같음, 1.0 은 수렴안함, 1.2 beta_hats = np.concatenate([beta_hats,beta_hat],axis=1) . beta_hats . array([[-3. , 7.12238255, -1.2575366 , 5.73166742, -0.1555309 , 4.86767499, 0.51106397, 4.36611576, 0.87316777, 4.12348617, 1.01165173, 4.07771926, 0.97282343, 4.19586617, 0.77814101, 4.46653491, 0.4299822 , 4.89562729, -0.08537358, 5.50446319, -0.79684366, 6.32975688, -1.74933031, 7.42517729, -3.00603683, 8.86442507, -4.6523303 , 10.74592463, -6.80132547, 13.19938129], [-2. , 8.70824998, 0.16165717, 6.93399596, 1.62435964, 5.72089586, 2.63858056, 4.86387722, 3.37280529, 4.22385379, 3.94259478, 3.70397678, 4.43004465, 3.23363047, 4.89701606, 2.75741782, 5.39439054, 2.22728903, 5.96886945, 1.59655409, 6.66836857, 0.81489407, 7.54676324, -0.17628423, 8.66856437, -1.44867655, 10.11401544, -3.09256176, 11.98507323, -5.22340389]]) . b0hats = beta_hats[0].tolist() b1hats = beta_hats[1].tolist() . np.linalg.inv(X.T@X) @ X.T @ y . array([[2.5451404 ], [3.94818596]]) . from matplotlib import animation plt.rcParams[&quot;animation.html&quot;] = &quot;jshtml&quot; . fig = plt.figure(); fig.set_figheight(5); fig.set_figwidth(12) . &lt;Figure size 864x360 with 0 Axes&gt; . ax1= fig.add_subplot(1,2,1) ax2= fig.add_subplot(1,2,2,projection=&#39;3d&#39;) # ax1: 왼쪽그림 ax1.plot(x,y,&#39;o&#39;) line, = ax1.plot(x,b0hats[0] + b1hats[0]*x) # ax2: 오른쪽그림 β0,β1 = np.meshgrid(np.arange(-6,11,0.25),np.arange(-6,11,0.25),indexing=&#39;ij&#39;) β0=β0.reshape(-1) β1=β1.reshape(-1) loss_fn = lambda b0,b1: np.sum((y-b0-b1*x)**2) loss = list(map(loss_fn, β0,β1)) ax2.scatter(β0,β1,loss,alpha=0.02) ax2.scatter(2.5451404,3.94818596,loss_fn(2.5451404,3.94818596),s=200,marker=&#39;*&#39;) def animate(i): line.set_ydata(b0hats[i] + b1hats[i]*x) ax2.scatter(b0hats[i],b1hats[i],loss_fn(b0hats[i],b1hats[i]),color=&quot;grey&quot;) ani = animation.FuncAnimation(fig,animate,frames=30) ani . Output hidden; open in https://colab.research.google.com to view. . &#50696;&#51228;2 . model: $y_i approx beta_0 + beta_1 e^{-x_i}$ . np.random.seed(43052) N= 100 x= np.linspace(-1,1,N) epsilon = np.random.randn(N)*0.5 y= 2.5+4*np.exp(-x) +epsilon . plt.plot(x,y,&#39;o&#39;) . [&lt;matplotlib.lines.Line2D at 0x7ff90f521990&gt;] . X= np.stack([np.ones(N),np.exp(-x)],axis=1) y= y.reshape(N,1) . beta_hat = np.array([-3,-2]).reshape(2,1) beta_hats = beta_hat.copy() # shallow copy, deep copy &lt; 여름 방학 특강 for i in range(1,30): yhat = X@beta_hat slope = (2*X.T@X@beta_hat - 2*X.T@y) /N beta_hat = beta_hat - 0.05*slope beta_hats = np.concatenate([beta_hats,beta_hat],axis=1) . beta_hats . array([[-3. , -1.74671631, -0.82428979, -0.14453919, 0.35720029, 0.72834869, 1.0036803 , 1.20869624, 1.36209751, 1.47759851, 1.56525696, 1.63244908, 1.68458472, 1.72563174, 1.75850062, 1.78532638, 1.80767543, 1.82669717, 1.84323521, 1.85790889, 1.8711731 , 1.88336212, 1.89472176, 1.90543297, 1.91562909, 1.92540859, 1.93484428, 1.94399023, 1.9528867 , 1.96156382], [-2. , -0.25663415, 1.01939241, 1.95275596, 2.63488171, 3.13281171, 3.49570765, 3.75961951, 3.95098231, 4.08918044, 4.18842797, 4.2591476 , 4.30898175, 4.34353413, 4.36691339, 4.38213187, 4.39139801, 4.39633075, 4.39811673, 4.3976256 , 4.3954946 , 4.3921905 , 4.38805511, 4.3833386 , 4.37822393, 4.37284482, 4.36729887, 4.36165718, 4.35597148, 4.35027923]]) . b0hats= beta_hats[0].tolist() b1hats= beta_hats[1].tolist() . np.linalg.inv(X.T@X)@X.T@y . array([[2.46307644], [3.99681332]]) . fig = plt.figure(); fig.set_figheight(5); fig.set_figwidth(12) . &lt;Figure size 864x360 with 0 Axes&gt; . ax1= fig.add_subplot(1,2,1) ax2= fig.add_subplot(1,2,2,projection=&#39;3d&#39;) # ax1: 왼쪽그림 ax1.plot(x,y,&#39;o&#39;) line, = ax1.plot(x,b0hats[0] + b1hats[0]*np.exp(-x)) # ax2: 오른쪽그림 β0,β1 = np.meshgrid(np.arange(-6,11,0.25),np.arange(-6,11,0.25),indexing=&#39;ij&#39;) β0=β0.reshape(-1) β1=β1.reshape(-1) loss_fn = lambda b0,b1: np.sum((y-b0-b1*np.exp(-x))**2) loss = list(map(loss_fn, β0,β1)) ax2.scatter(β0,β1,loss,alpha=0.02) ax2.scatter(2.46307644,3.99681332,loss_fn(2.46307644,3.99681332),s=200,marker=&#39;*&#39;) def animate(i): line.set_ydata(b0hats[i] + b1hats[i]*np.exp(-x)) ax2.scatter(b0hats[i],b1hats[i],loss_fn(b0hats[i],b1hats[i]),color=&quot;grey&quot;) ani = animation.FuncAnimation(fig,animate,frames=30) ani . Output hidden; open in https://colab.research.google.com to view. . &#50696;&#51228;3 . model: $y_i approx beta_0 + beta_1 e^{-x_i} + beta_2 cos(5x_i)$ . np.random.seed(43052) N= 100 x= np.linspace(-1,1,N) epsilon = np.random.randn(N)*0.5 y= 2.5+4*np.exp(-x) + 5*np.cos(5*x) + epsilon . plt.plot(x,y,&#39;o&#39;) . [&lt;matplotlib.lines.Line2D at 0x7ff90f297690&gt;] . X=np.stack([np.ones(N),np.exp(-x),np.cos(5*x)],axis=1) y=y.reshape(N,1) . beta_hat = np.array([-3,-2,-1]).reshape(3,1) beta_hats = beta_hat.copy() for i in range(1,30): yhat = X@beta_hat slope = (2*X.T@X@beta_hat -2*X.T@y) /N beta_hat = beta_hat - 0.1 * slope beta_hats= np.concatenate([beta_hats,beta_hat],axis=1) . beta_hats . array([[-3. , -0.71767532, 0.36255782, 0.89072137, 1.16423101, 1.31925078, 1.41819551, 1.48974454, 1.54713983, 1.59655416, 1.64091846, 1.68167278, 1.71956758, 1.75503084, 1.78833646, 1.81968188, 1.84922398, 1.877096 , 1.90341567, 1.92828934, 1.95181415, 1.97407943, 1.99516755, 2.01515463, 2.0341111 , 2.05210214, 2.06918818, 2.08542523, 2.10086524, 2.11555643], [-2. , 1.16947474, 2.64116513, 3.33411605, 3.66880042, 3.83768856, 3.92897389, 3.98315095, 4.01888831, 4.04486085, 4.06516144, 4.08177665, 4.09571971, 4.10754954, 4.1176088 , 4.12613352, 4.13330391, 4.13926816, 4.14415391, 4.14807403, 4.15112966, 4.1534121 , 4.15500404, 4.15598045, 4.15640936, 4.15635249, 4.15586584, 4.15500014, 4.15380139, 4.1523112 ], [-1. , -0.95492718, -0.66119313, -0.27681968, 0.12788212, 0.52254445, 0.89491388, 1.24088224, 1.55993978, 1.85310654, 2.12199631, 2.36839745, 2.59408948, 2.8007666 , 2.99000967, 3.16327964, 3.32192026, 3.46716468, 3.60014318, 3.72189116, 3.83335689, 3.93540864, 4.02884144, 4.11438316, 4.19270026, 4.26440288, 4.33004965, 4.39015202, 4.44517824, 4.49555703]]) . b0hats,b1hats,b2hats = beta_hats . np.linalg.inv(X.T@X) @ X.T @ y . array([[2.46597526], [4.00095138], [5.04161877]]) . fig = plt.figure(); fig.set_figheight(5); fig.set_figwidth(12) . &lt;Figure size 864x360 with 0 Axes&gt; . ax1= fig.add_subplot(1,2,1) ax2= fig.add_subplot(1,2,2,projection=&#39;3d&#39;) # ax1: 왼쪽그림 ax1.plot(x,y,&#39;o&#39;) line, = ax1.plot(x,b0hats[0] + b1hats[0]*np.exp(-x) + b2hats[0]*np.cos(5*x)) # ax2: 오른쪽그림 # β0,β1 = np.meshgrid(np.arange(-6,11,0.25),np.arange(-6,11,0.25),indexing=&#39;ij&#39;) # β0=β0.reshape(-1) # β1=β1.reshape(-1) # loss_fn = lambda b0,b1: np.sum((y-b0-b1*np.exp(-x))**2) # loss = list(map(loss_fn, β0,β1)) # ax2.scatter(β0,β1,loss,alpha=0.02) # ax2.scatter(2.46307644,3.99681332,loss_fn(2.46307644,3.99681332),s=200,marker=&#39;*&#39;) def animate(i): line.set_ydata(b0hats[i] + b1hats[i]*np.exp(-x) + b2hats[i]*np.cos(5*x)) # ax2.scatter(b0hats[i],b1hats[i],loss_fn(b0hats[i],b1hats[i]),color=&quot;grey&quot;) ani = animation.FuncAnimation(fig,animate,frames=30) ani . Output hidden; open in https://colab.research.google.com to view. . &#50696;&#51228;3: &#52992;&#46972;&#49828;&#47196; &#54644;&#48372;&#51088;! . model: $y_i approx beta_0 + beta_1 e^{-x_i} + beta_2 cos(5x_i)$ . np.random.seed(43052) N= 100 x= np.linspace(-1,1,N) epsilon = np.random.randn(N)*0.5 y= 2.5+4*np.exp(-x) + 5*np.cos(5*x) + epsilon . X=np.stack([np.ones(N),np.exp(-x),np.cos(5*x)],axis=1) y=y.reshape(N,1) . net = tf.keras.Sequential() # 1: 네트워크 생성 net.add(tf.keras.layers.Dense(1,use_bias=False)) # 2: add layer net.compile(tf.optimizers.SGD(0.1), loss=&#39;mse&#39;) # 3: compile net.fit(X,y,epochs=30, batch_size=N) # 4: fit . Epoch 1/30 1/1 [==============================] - 0s 186ms/step - loss: 82.1027 Epoch 2/30 1/1 [==============================] - 0s 11ms/step - loss: 23.9512 Epoch 3/30 1/1 [==============================] - 0s 14ms/step - loss: 10.7256 Epoch 4/30 1/1 [==============================] - 0s 9ms/step - loss: 7.0664 Epoch 5/30 1/1 [==============================] - 0s 6ms/step - loss: 5.5521 Epoch 6/30 1/1 [==============================] - 0s 5ms/step - loss: 4.6075 Epoch 7/30 1/1 [==============================] - 0s 6ms/step - loss: 3.8836 Epoch 8/30 1/1 [==============================] - 0s 7ms/step - loss: 3.2909 Epoch 9/30 1/1 [==============================] - 0s 6ms/step - loss: 2.7971 Epoch 10/30 1/1 [==============================] - 0s 7ms/step - loss: 2.3838 Epoch 11/30 1/1 [==============================] - 0s 6ms/step - loss: 2.0374 Epoch 12/30 1/1 [==============================] - 0s 5ms/step - loss: 1.7471 Epoch 13/30 1/1 [==============================] - 0s 5ms/step - loss: 1.5038 Epoch 14/30 1/1 [==============================] - 0s 6ms/step - loss: 1.2998 Epoch 15/30 1/1 [==============================] - 0s 6ms/step - loss: 1.1288 Epoch 16/30 1/1 [==============================] - 0s 7ms/step - loss: 0.9854 Epoch 17/30 1/1 [==============================] - 0s 6ms/step - loss: 0.8652 Epoch 18/30 1/1 [==============================] - 0s 6ms/step - loss: 0.7645 Epoch 19/30 1/1 [==============================] - 0s 7ms/step - loss: 0.6800 Epoch 20/30 1/1 [==============================] - 0s 6ms/step - loss: 0.6092 Epoch 21/30 1/1 [==============================] - 0s 6ms/step - loss: 0.5499 Epoch 22/30 1/1 [==============================] - 0s 6ms/step - loss: 0.5001 Epoch 23/30 1/1 [==============================] - 0s 6ms/step - loss: 0.4584 Epoch 24/30 1/1 [==============================] - 0s 6ms/step - loss: 0.4234 Epoch 25/30 1/1 [==============================] - 0s 6ms/step - loss: 0.3941 Epoch 26/30 1/1 [==============================] - 0s 12ms/step - loss: 0.3695 Epoch 27/30 1/1 [==============================] - 0s 9ms/step - loss: 0.3489 Epoch 28/30 1/1 [==============================] - 0s 10ms/step - loss: 0.3316 Epoch 29/30 1/1 [==============================] - 0s 12ms/step - loss: 0.3171 Epoch 30/30 1/1 [==============================] - 0s 4ms/step - loss: 0.3050 . &lt;keras.callbacks.History at 0x7ff90f24bd10&gt; . net.weights . [&lt;tf.Variable &#39;dense_9/kernel:0&#39; shape=(3, 1) dtype=float32, numpy= array([[2.485702 ], [3.9252913], [4.6923084]], dtype=float32)&gt;] . plt.plot(x,y,&#39;o&#39;) plt.plot(x,(X@net.weights).reshape(-1),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7ff90b865550&gt;] . &#49689;&#51228; . &#50696;&#51228;2: &#52992;&#46972;&#49828;&#47484; &#51060;&#50857;&#54616;&#50668; &#50500;&#47000;&#47484; &#47564;&#51313;&#54616;&#45716; &#51201;&#51208;&#54620; $ beta_0$&#50752; $ beta_1$&#51012; &#44396;&#54616;&#46972;. &#51201;&#54633;&#44208;&#44284;&#47484; &#49884;&#44033;&#54868;&#54616;&#46972;. (&#50528;&#45768;&#47700;&#51060;&#49496; &#49884;&#44033;&#54868; X) . model: $y_i approx beta_0 + beta_1 e^{-x_i}$ . np.random.seed(43052) N= 100 x= np.linspace(-1,1,N) epsilon = np.random.randn(N)*0.5 y= 2.5+4*np.exp(-x) +epsilon .",
            "url": "https://gangcheol.github.io/big-data-analysis/2022/04/11/(6%EC%A3%BC%EC%B0%A8)-4%EC%9B%9411%EC%9D%BC.html",
            "relUrl": "/2022/04/11/(6%EC%A3%BC%EC%B0%A8)-4%EC%9B%9411%EC%9D%BC.html",
            "date": " • Apr 11, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "(5주차) 4월4일 -- lecture",
            "content": "imports . . /bin/bash: conda: command not found . import tensorflow as tf import numpy as np import matplotlib.pyplot as plt . import tensorflow.experimental.numpy as tnp . tnp.experimental_enable_numpy_behavior() . &#52572;&#51201;&#54868;&#51032; &#47928;&#51228; . - $loss=( frac{1}{2} beta-1)^2$ . - 기존에 했던 방법은 수식을 알고 있어야 한다는 단점이 있음 . - 그래서 확률적 경사하강법을 이용해서 최적해를 찾는 과제를 수행하였음 . - 그러나 다양한 loss 함수에서 위와 같이 간단히 최적해를 찾는 것은 매우 어려움 $ to$ 대부분의 함수는 비모수적이기 때문이다!! . - 그래서 오늘은 그것을 쉽게 해주는 tf.keras.optimizers를 사용해서 최적해를 찾을 거얌 . tf.keras.optimizers&#47484; &#51060;&#50857;&#54620; &#52572;&#51201;&#54868;&#48169;&#48277; . &#48169;&#48277;1: opt.apply_gradients()&#47484; &#51060;&#50857; . 이전까지의 방법 | . beta = tf.Variable(-10.0) alpha = 0.01/6 . with tf.GradientTape() as tape : tape.watch(beta) loss = (beta/2-1)**2 slope = tape.gradient(loss,beta) . beta.assign_sub(slope*alpha) . &lt;tf.Variable &#39;UnreadVariable&#39; shape=() dtype=float32, numpy=-9.99&gt; . 이전 방법 + 새로운 방법 ( - beta.assign_sub(slope*alpha)) | . opt = tf.keras.optimizers.SGD(alpha) . opt.apply_gradients() : 베타 값과 slope의 값을 받아 최적해를 구해준다. | . 즉, 위식을 통해 수식을 몰라도 최적해를 구할 수 있다. | . 아래 과정은 한번에 iteration 임 | . opt.apply_gradients([(slope,beta)]) # beta.assign_sub(slope*alpha) . &lt;tf.Variable &#39;UnreadVariable&#39; shape=() dtype=int64, numpy=1&gt; . beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=-9.98&gt; . iteration 2 | . with tf.GradientTape() as tape : tape.watch(beta) loss = (beta/2-1)**2 slope = tape.gradient(loss,beta) . opt.apply_gradients([(slope,beta)]) # beta.assign_sub(slope*alpha) . &lt;tf.Variable &#39;UnreadVariable&#39; shape=() dtype=int64, numpy=2&gt; . beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=-9.9700165&gt; . interation 1과 interation 2 의 결과가 달라진 것을 확인하였다. | . for 문을 이용한다면? | . alpha = 0.01/6 beta = tf.Variable(-10.0) opt = tf.keras.optimizers.SGD(alpha) . for epoc in range(10000) : with tf.GradientTape() as tape : tape.watch(beta) loss = (beta/2-1)**2 slope = tape.gradient(loss,beta) opt.apply_gradients([(slope,beta)]) . beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=1.9971251&gt; . 잠깐, 주소가 같지만 문법이 다름 근데 걍 그런갑다 하자 | . opt.learning_rate, opt.lr . (&lt;tf.Variable &#39;SGD/learning_rate:0&#39; shape=() dtype=float32, numpy=0.0016666667&gt;, &lt;tf.Variable &#39;SGD/learning_rate:0&#39; shape=() dtype=float32, numpy=0.0016666667&gt;) . id(opt.learning_rate), id( opt.lr) . (139921818271120, 139921818271120) . &#48169;&#48277;2: opt.minimize() . 이 함수를 이용하면 gradienttape를 안써도 된다. 핳 | . alpha = 0.01/6 beta = tf.Variable(-10.0) opt = tf.keras.optimizers.SGD(alpha) . loss &#54632;&#49688; &#51221;&#51032; . $$ left( frac 12 beta-1 right)^2$$ . loss_fn = lambda: (beta/2-1)**2 . - iter 1 . opt.minimize(loss_fn,beta) . &lt;tf.Variable &#39;UnreadVariable&#39; shape=() dtype=int64, numpy=1&gt; . beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=-9.99&gt; . - for . alpha = 0.01/6 beta = tf.Variable(-10.0) opt = tf.keras.optimizers.SGD(alpha) loss_fn = lambda: (beta/2-1)**2 for epoc in range(10000) : opt.minimize(loss_fn,beta) beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=1.9971251&gt; . beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=1.9971251&gt; . &#54924;&#44480;&#48516;&#49437; &#47928;&#51228; . - ${ bf y} approx 2.5 + 4 { bf x}$ . tnp.random.seed(43052) N = 200 x = tnp.linspace(0,1,N) epsilon = tnp.random.randn(N)*0.5 y = 2.5 + 4*x + epsilon y_hat = 2.5 + 4*x . plt.plot(x,y,&#39;.&#39;) plt.plot(x,y_hat,&#39;r--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f4215a0cd10&gt;] . &#51060;&#47200;&#51201; &#54400;&#51060; . &#54400;&#51060;1: &#49828;&#52860;&#46972;&#48260;&#51204; . - 포인트 . $S_{xx}= sum (x- bar x)^2$,$ quad S_{xy}= sum (x- bar x)(y- bar y)$ | $ hat{ beta}_0= bar y - hat{ beta_1} bar x$$, quad hat{ beta}_1= frac {S_{xy}}{S_{xx}} $ | . Sxx = sum((x-x.mean())**2) Sxy = sum((x-x.mean())*(y-y.mean())) . beta1_hat = Sxy/Sxx beta0_hat = y.mean() - beta1_hat*x.mean() . beta0_hat,beta1_hat . (&lt;tf.Tensor: shape=(), dtype=float64, numpy=2.583667211565867&gt;, &lt;tf.Tensor: shape=(), dtype=float64, numpy=3.933034516733169&gt;) . &#54400;&#51060;2: &#48289;&#53552;&#48260;&#51204; . X=tf.stack([tf.ones(N,dtype=&#39;float64&#39;),x],axis=1) y=y.reshape(N,1) . X.shape,y.shape . (TensorShape([200, 2]), TensorShape([200, 1])) . $ hat beta = (X^{T}X)^{-1} X^{T} y$ | . tf.linalg.inv(X.T@X)@ X.T @y . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[2.58366721], [3.93303452]])&gt; . &#54400;&#51060;3: &#48289;&#53552;&#48260;&#51204;, &#49552;&#49892;&#54632;&#49688;&#51032; &#46020;&#54632;&#49688;&#51060;&#50857; (&#44221;&#49324;&#54616;&#44053;&#48277;&#51012; &#51060;&#50857;&#54620; &#54400;&#51060;) . (단, 텐서플로우의 미분기능을 사용하지 않음 ) . X=tf.stack([tf.ones(N,dtype=&#39;float64&#39;),x],axis=1) y=y.reshape(N,1) X.shape,y.shape . (TensorShape([200, 2]), TensorShape([200, 1])) . - 초기값 설정 . beta_hat = tnp.array([-5.0,10.0]).reshape(2,1) . - 포인트 . $loss&#39;( beta)=-2X&#39;y +2X&#39;X beta$ | $ beta_{new} = beta_{old} - alpha times loss&#39;( beta_{old})$ | . slope = -2*X.T@y + 2*X.T@X@beta_hat . slope . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[-1820.07378797], [ -705.77222696]])&gt; . alpha = 0.001 . step = slope * alpha step . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[-1.82007379], [-0.70577223]])&gt; . for epoc in range(1000) : slope = -2*X.T@y + 2*X.T@X@beta_hat beta_hat = beta_hat - alpha*slope . beta_hat . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[2.58366721], [3.93303452]])&gt; . GradientTape&#47484; &#51060;&#50857; . &#54400;&#51060;1: &#48289;&#53552;&#48260;&#51204; . - 포인트 . ## 포인트코드1: 그레디언트 테입 with tf.GradientTape() as tape: loss = ## 포인트코드2: 미분 slope = tape.gradient(loss,beta_hat) ## 포인트코드3: update beta_hat.assign_sub(slope*alph) . &#54400;&#51060;2: &#49828;&#52860;&#46972;&#48260;&#51204; . - 포인트 . ## 포인트코드: 미분 slope0,slope1 = tape.gradient(loss,[beta0_hat,beta1_hat]) . GradientTape + opt.apply_gradients . &#54400;&#51060;1: &#48289;&#53552;&#48260;&#51204; . - 포인트 . ## 포인트코드: 업데이트 opt.apply_gradients([(slope,beta_hat)]) ## pair의 list가 입력 . &#54400;&#51060;2: &#49828;&#52860;&#46972;&#48260;&#51204; . - 포인트 . ## 포인트코드: 업데이트 opt.apply_gradients([(slope0,beta0_hat),(slope1,beta1_hat)]) ## pair의 list가 입력 . opt.minimize . &#54400;&#51060;1: &#48289;&#53552;&#48260;&#51204;, &#49324;&#50857;&#51088;&#51221;&#51032; &#49552;&#49892;&#54632;&#49688; with lambda . &#54400;&#51060;2: &#49828;&#52860;&#46972;&#48260;&#51204;, &#49324;&#50857;&#51088;&#51221;&#51032; &#49552;&#49892;&#54632;&#49688; with lambda . - 포인트 . ## 포인트코드: 미분 &amp; 업데이트 = minimize opt.minimize(loss_fn,[beta0_hat,beta1_hat]) . &#54400;&#51060;3: &#48289;&#53552;&#48260;&#51204;, &#49324;&#50857;&#51088;&#51221;&#51032; (&#51687;&#51008;) &#49552;&#49892;&#54632;&#49688; . - 포인트 . ## 포인트코드: 손실함수정의 def loss_fn(): return ?? . &#54400;&#51060;4: &#48289;&#53552;&#48260;&#51204;, &#49324;&#50857;&#51088;&#51221;&#51032; (&#44596;) &#49552;&#49892;&#54632;&#49688; . - 포인트 . ## 포인트코드: 손실함수정의 def loss_fn(): ?? ?? return ?? . &#54400;&#51060;5: &#48289;&#53552;&#48260;&#51204;, &#49324;&#50857;&#51088;&#51221;&#51032; &#49552;&#49892;&#54632;&#49688; &lt;- tf.losses.MSE . - 포인트 . ## 포인트코드: 미리구현되어있는 손실함수 이용 tf.losses.MSE(y,yhat) . &#54400;&#51060;6: &#48289;&#53552;&#48260;&#51204;, &#49324;&#50857;&#51088;&#51221;&#51032; &#49552;&#49892;&#54632;&#49688; &lt;- tf.losses.MeaSquaredError . - 포인트 . ## 포인트코드: 클래스로부터 손실함수 오브젝트 생성 (함수를 찍어내는 클래스) mse_fn = tf.losses.MeanSquaredError() mse_fn(y,yhat) . tf.keras.Sequential . - $ hat{y}_i= hat{ beta}_0+ hat{ beta}_1x_i$ 의 서로다른 표현 . import graphviz def gv(s): return graphviz.Source(&#39;digraph G{ rankdir=&quot;LR&quot;&#39;+s + &#39;; }&#39;) . gv(&#39;&#39;&#39; &quot;1&quot; -&gt; &quot;beta0_hat + x*beta1_hat, bias=False&quot;[label=&quot;* beta0_hat&quot;] &quot;x&quot; -&gt; &quot;beta0_hat + x*beta1_hat, bias=False&quot;[label=&quot;* beta1_hat&quot;] &quot;beta0_hat + x*beta1_hat, bias=False&quot; -&gt; &quot;yhat&quot;[label=&quot;indentity&quot;] &#39;&#39;&#39;) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G 1 1 beta0_hat + x*beta1_hat, &#160;&#160;&#160;bias=False beta0_hat + x*beta1_hat, &#160;&#160;&#160;bias=False 1&#45;&gt;beta0_hat + x*beta1_hat, &#160;&#160;&#160;bias=False * beta0_hat yhat yhat beta0_hat + x*beta1_hat, &#160;&#160;&#160;bias=False&#45;&gt;yhat indentity x x x&#45;&gt;beta0_hat + x*beta1_hat, &#160;&#160;&#160;bias=False * beta1_hat gv(&#39;&#39;&#39; &quot;x&quot; -&gt; &quot;x*beta1_hat, bias=True&quot;[label=&quot;*beta1_hat&quot;] ; &quot;x*beta1_hat, bias=True&quot; -&gt; &quot;yhat&quot;[label=&quot;indentity&quot;] &#39;&#39;&#39;) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G x x x*beta1_hat, &#160;&#160;&#160;bias=True x*beta1_hat, &#160;&#160;&#160;bias=True x&#45;&gt;x*beta1_hat, &#160;&#160;&#160;bias=True *beta1_hat yhat yhat x*beta1_hat, &#160;&#160;&#160;bias=True&#45;&gt;yhat indentity gv(&#39;&#39;&#39; &quot;X=[1 x]&quot; -&gt; &quot;X@beta_hat, bias=False&quot;[label=&quot;@beta_hat&quot;] ; &quot;X@beta_hat, bias=False&quot; -&gt; &quot;yhat&quot;[label=&quot;indentity&quot;] &#39;&#39;&#39;) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G X=[1 x] X=[1 x] X@beta_hat, &#160;&#160;&#160;bias=False X@beta_hat, &#160;&#160;&#160;bias=False X=[1 x]&#45;&gt;X@beta_hat, &#160;&#160;&#160;bias=False @beta_hat yhat yhat X@beta_hat, &#160;&#160;&#160;bias=False&#45;&gt;yhat indentity &#54400;&#51060;1: &#48289;&#53552;&#48260;&#51204;, &#49324;&#50857;&#51088;&#51221;&#51032; &#49552;&#49892;&#54632;&#49688; . - 포인트 . ## 포인트코드1: 네트워크 생성 net = tf.keras.Sequential() ## 포인트코드2: 네트워크의 아키텍처 설계 net.add(tf.keras.layers.Dense(1,input_shape=(2,),use_bias=False)) ## 포인트코드3: 네트워크 컴파일 = 아키텍처 + 손실함수 + 옵티마이저 net.compile(opt,loss=loss_fn2) ## 포인트코드4: 미분 &amp; update net.fit(X,y,epochs=1000,verbose=0,batch_size=N) .",
            "url": "https://gangcheol.github.io/big-data-analysis/2022/03/30/(5%EC%A3%BC%EC%B0%A8).html",
            "relUrl": "/2022/03/30/(5%EC%A3%BC%EC%B0%A8).html",
            "date": " • Mar 30, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "(4주차) 과제",
            "content": "아래식을 최소화하는 $x$를 확률적 경사하강법으로 찾고 애니메이션으로 찾고 시각화할 것 . $$y=(x-1)^2$$ . Solution . import . import matplotlib.pyplot as plt import tensorflow as tf import tensorflow.experimental.numpy as tnp plt.rcParams[&quot;animation.html&quot;]=&quot;jshtml&quot; from matplotlib import animation import numpy as np . tnp.experimental_enable_numpy_behavior() . &#52488;&#44592;&#44050; &#49444;&#51221; . x = tf.Variable(-3.0) alpha = 0.1 . &#50528;&#45768;&#47700;&#51060;&#49496; &#54632;&#49688; &#49373;&#49457; . def animate(i): pnts.set_xdata(x_lst[:(i+1)]) pnts.set_ydata(y_lst[:(i+1)]) . x&#44050;&#50640; &#48320;&#54868;&#50640; &#46384;&#47480; &#48320;&#54868;&#44050;&#51012; &#51200;&#51109; . $X_{t+1} = [x_{1}, ,x_{2} dots x_{t+1}], quad Y_{t+1} = [y_{1}, ,y_{2} dots y_{t+1}]$ . x_lst=[] x_lst.append(x.numpy()) y_lst=[] y_lst.append((x.numpy()-1)**2) for k in range(100) : with tf.GradientTape(persistent=True) as tape : tape.watch(x) y = (x-1)**2 x.assign_sub(alpha*tape.gradient(y,x)) x_lst.append(x.numpy()) y_lst.append((x.numpy()-1)**2) . &#48320;&#54868;&#47049;&#51060; &#50732;&#46972;&#53440;? &#44592; &#50948;&#54620; &#49828;&#52992;&#52824; &#49373;&#49457; . fig = plt.figure() _x= np.linspace(-4,6) ax = fig.add_subplot() ax.plot(_x,(_x-1)**2) pnts, = ax.plot(x_lst[0],y_lst[0],&#39;or&#39;) . &#50528;&#45768;&#47700;&#51060;&#49496; &#49373;&#49457; . ani =animation.FuncAnimation(fig, animate, frames=100) ani . &lt;/input&gt; Once Loop Reflect",
            "url": "https://gangcheol.github.io/big-data-analysis/2022/03/29/(4%EC%A3%BC%EC%B0%A8)-%EA%B3%BC%EC%A0%9C.html",
            "relUrl": "/2022/03/29/(4%EC%A3%BC%EC%B0%A8)-%EA%B3%BC%EC%A0%9C.html",
            "date": " • Mar 29, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "(4주차) 미분",
            "content": "&#44053;&#51032;&#50689;&#49345; . &#48120;&#48516; . tf.GradientTape() &#49324;&#50857;&#48169;&#48277; . - 예제9: 카페예제로 돌아오자. (1주차 강의) . - 자료 생성 . import matplotlib.pyplot as plt import tensorflow as tf import tensorflow.experimental.numpy as tnp . . tnp.experimental_enable_numpy_behavior() . x=tnp.array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4]) x . &lt;tf.Tensor: shape=(10,), dtype=float64, numpy=array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4])&gt; . tnp.random.seed(43052) y= 10.2+ x*2.2 + tnp.random.randn(10) y . &lt;tf.Tensor: shape=(10,), dtype=float64, numpy= array([54.98269924, 60.27348365, 61.27621687, 60.53495888, 62.9770905 , 66.32168996, 66.87781372, 71.0050025 , 72.63837337, 77.11143943])&gt; . &#51473;&#44036;&#44256;&#49324; &#44592;&#52636;&#47928;&#51228; . - loss 정의 . beta0= tf.Variable(9.0) beta1= tf.Variable(2.0) . with tf.GradientTape(persistent=True) as tape : loss = sum((y-beta0-beta1*x)**2) . tape.gradient(loss,beta0),tape.gradient(loss,beta1) . (&lt;tf.Tensor: shape=(), dtype=float32, numpy=-127.597534&gt;, &lt;tf.Tensor: shape=(), dtype=float32, numpy=-3214.2532&gt;) . - 계산이 맞는지 확인 . X= tnp.array([1]*10+ [20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4]).reshape(2,10).T X . &lt;tf.Tensor: shape=(10, 2), dtype=float64, numpy= array([[ 1. , 20.1], [ 1. , 22.2], [ 1. , 22.7], [ 1. , 23.3], [ 1. , 24.4], [ 1. , 25.1], [ 1. , 26.2], [ 1. , 27.3], [ 1. , 28.4], [ 1. , 30.4]])&gt; . beta_true = tnp.array([[10.2],[2.2]]) beta_true . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[10.2], [ 2.2]])&gt; . tnp.random.seed(43052) y= X@beta_true + tnp.random.randn(10).reshape(10,1) y . &lt;tf.Tensor: shape=(10, 1), dtype=float64, numpy= array([[54.98269924], [60.27348365], [61.27621687], [60.53495888], [62.9770905 ], [66.32168996], [66.87781372], [71.0050025 ], [72.63837337], [77.11143943]])&gt; . beta = tnp.array([[9.0],[2.0]]) beta . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[9.], [2.]])&gt; . with tf.GradientTape(persistent=True) as tape: tape.watch(beta) yhat = X@beta loss=(y-yhat).T @ (y-yhat) . - 미분 . tape.gradient(loss,beta) # 텐서플로우가 계산한 미분값 . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[ -127.59753624], [-3214.25306574]])&gt; . - 이론적인 값과 동일한지 확인 . $$loss = -2 X^{ prime}y + 2X^{ prime}X beta$$ . -2 * X.T @ y + 2* X.T @ X @ beta # 이론적인 값 . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[ -127.59753624], [-3214.25306574]])&gt; . - 베타 추정치 계산 . tf.linalg.inv(X.T @ X) @ X.T @ y . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[12.10040012], [ 2.13112662]])&gt; . - 예제11: 위의 예제에서 이론적인 $ boldsymbol{ beta}$의 최적값을 찾아보고 (즉 $ boldsymbol{ hat beta}$을 찾고) 그 지점에서 loss의 미분값(=접선의 기울기)를 구하라. 결과가 $ bf{0}$인지 확인하라. (단 ${ bf 0}$은 길이가 2이고 각 원소가 0인 벡터) . loss 값을 보아 제대로 추정되지 않았음 | . $ beta$의 최적값은 $(X^{ prime}X)^{-1}X^{ prime}y$ . beta_optimal = tf.linalg.inv(X.T @ X) @ X.T @ y beta_optimal . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[12.10040012], [ 2.13112662]])&gt; . beta_optimal을 미분했을 때 0이 나와야 실제 베타에 대한 최적의 추정치가 구해진거임 . with tf.GradientTape(persistent=True) as tape: tape.watch(beta_optimal) yhat = X@beta_optimal loss=(y-yhat).T @ (y-yhat) . tape.gradient(loss,beta_optimal) # 텐서플로우가 계산한 미분값 . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[-5.27222710e-12], [-1.33283323e-10]])&gt; . - 베타의 실제값과 베타 옵티말을 넣으면 어떨까? . with tf.GradientTape(persistent=True) as tape: tape.watch(beta_true) yhat = X@beta_true loss=(y-yhat).T @ (y-yhat) . tape.gradient(loss,beta_true) # 텐서플로우가 계산한 미분값 . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[ -3.55753624], [-76.87306574]])&gt; . 옵티말보다 loss가 최적보단 크다 그러나 표본의 수가 커질 경우 최적치와 실제가 점점 비슷해진다 | . 결국 beta_true $ approx$ beta_optimal 이라고 할 수 있다. | . &#44221;&#49324;&#54616;&#44053;&#48277; . - $loss = ( frac {1}{2} beta-1)^2$ 을 최소화 하는 $ beta$ 를 구해보자 . - 당연히 $ beta=2$일 떼 최솟값을 가질 것이다 . - 이것을 컴퓨터로 직접 구해보자 . &#52572;&#51201;&#54868;&#47928;&#51228; . &#48169;&#48277;1: grid search . - 단순히 베타를 개많이 만들고 loss를 최소화하는 베타를 찾자 . &#50508;&#44256;&#47532;&#51608; . &#44396;&#54788;&#53076;&#46300; . beta = tnp.linspace(-10,10,1000) #beta . loss = (1/2*beta-1)**2 . tnp.argmin(loss) . &lt;tf.Tensor: shape=(), dtype=int64, numpy=599&gt; . grid search로 알아봤을 때 최솟값이 2에 근사하게 나온다 | . beta[599] . &lt;tf.Tensor: shape=(), dtype=float64, numpy=1.9919919919919913&gt; . &#44536;&#47532;&#46300;&#49436;&#52824;&#51032; &#47928;&#51228;&#51216; . - 좀 정확하진 않지만 표본의 수를 늘리면 2의 근접한 값을 찾는다. . - 비판1: [-10,10]이외에 해가 존재하면? 즉 범위 밖에 존재할 수 가 있음 . 이 예제의 경우는 운좋게 [-10,10]에서 해가 존재했음 | 하지만 임의의 고정된 $x,y$에 대하여 $loss( beta)=(x beta-y)^2$ 의 형태의 해가 항상 [-10,10]에서 존재한다는 보장은 없음 | 해결책: 더 넓게 많은 범위를 탐색하자? $ to$ but, 무한대의 범위에서 할 수 없음 | . - 비판2: 위 해결책은 효율적이지 않음 . 알고리즘을 요약하면 결국 -10부터 10까지 작은 간격으로 조금씩 이동하며 loss를 조사하는 것이 grid search의 아이디어 | $ to$ 생각해보니까 $ beta=2$인 순간 $loss=( frac{1}{2} beta-1)^2=0$이 되어서 이것보다 작은 최소값은 존재하지 않는다(제곱은 항상 양수이어야 하므로) | $ to$ 따라서 $ beta=2$ 이후로는 탐색할 필요가 없다 | . &#48169;&#48277;2: gradient descent . (1) 임의의 초기값을 선정하고 $loss$를 계산한다 (초깃값 셋팅) . $ beta = -5 to loss(-5) = (-5/2-1)^2 = 12.55$ | . (-5/2-1)**2 . 12.25 . (2) 임의의 초기값에서 좌우로 약간씩 이동해보고 $loss$를 계한한다. (미분에서 최솟값을 찾는 과정) . $ to beta = -5.01, , beta = -4.99$ . (-5.01 /2 -1)**2,(-4.99 /2 -1)**2 . (12.285025, 12.215025) . (3) (2)의 결과를 보고 어느쪽으로 이동하는 것이 유리한지 따져본다. 그 후 유리한 방향으로 이동한다. . import matplotlib.pyplot as plt . plt.plot(beta,loss) . [&lt;matplotlib.lines.Line2D at 0x7f44a76f6850&gt;] . - (2) - (3)의 과정은 $ beta = -5$ 미분계수를 구한 후 미분계수가 양수이면 왼쪽으로 움직이고 음수이면 오른쪽으로 움직인다고 해석가능 . (4) (2) ~ (3) 과정을 반복 후, 어느쪽으로 가도 유리한 지점이 없다면 알고리즘을 멈춘다 . &#50508;&#44256;&#47532;&#51608; &#48516;&#49437; . - 알고리즘이 멈추는 지점은 $ beta=2$이다. 왜냐하면 이경우 왼쪽으로 가도, 오른쪽으로 가도 현재 손실함수값보다 크기 때문. . &#50812;&#51901;/&#50724;&#47480;&#51901;&#51473;&#50640; &#50612;&#46356;&#47196; &#44040;&#51648; &#50612;&#46523;&#44172; &#54032;&#45800;&#54616;&#45716; &#44284;&#51221;&#51012; &#49688;&#49885;&#54868;? . 오른쪽으로 0.01간다 $ to$ 미분계수가 음수일 때 | 왼쪽으로 0.01간다 $ to$ 미분계수가 양수일 때 | . - 그렇다면 . $ beta_{new} = beta_{old} + 0.01 to frac{d ,loss}{d , beta_{old}}$ 가 음수 일때 . $ beta_{new} = beta_{old} - 0.01 to frac{d ,loss}{d , beta_{old}}$ 가 양수 일때 . &#54841;&#49884; &#50508;&#44256;&#47532;&#51608;&#51012; &#51328; &#44060;&#49440;&#54624;&#49688; &#51080;&#51012;&#44620;? . - 동일하게 0.01씩 이동하는게 맞는지 의문 . import numpy as np . _beta = np.linspace(-10,5) plt.plot(_beta,(_beta/2-1)**2) . [&lt;matplotlib.lines.Line2D at 0x7f44a7b800d0&gt;] . - $ beta=-10$ 일 경우의 접선의 기울기? $ beta=-4$ 일때 접선의 기울기? . - $ beta= -10 to 기울기는 -6$ . - $ beta= -4 to 기울기는 -3$ . 위 같은 경우 $ beta = -10$ 에서 0.01만큼 이동했다면 $ beta=-4$ 에서 0.005만큼 이동해야함 | . 즉, 떨어진 만큼 비례해서 조금 더 자신있게 가자는 거임 | . $$ beta_{new}= beta_{old} - alpha left[ frac {∂}{ ,∂ beta}loss( beta) right], quad alpha&gt;0$$ . &#44396;&#54788;&#53076;&#46300; . iter1: $ beta=-10$ 출발 . beta = tf.Variable(-10.0) . with tf.GradientTape(persistent=True) as tape: loss = (beta/2-1)**2 . tape.gradient(loss,beta) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=-6.0&gt; . alpha=0.01/6 . beta.assign_sub(alpha*tape.gradient(loss,beta)) ## variable 로 벼수 선언시 assign을 사용하면 초괴화가 가능 . &lt;tf.Variable &#39;UnreadVariable&#39; shape=() dtype=float32, numpy=-9.99&gt; . iter2 $ beta=-9.99$ . beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=-9.99&gt; . with tf.GradientTape(persistent=True) as tape: loss = (beta/2-1)**2 . beta.assign_sub(alpha*tape.gradient(loss,beta)) . &lt;tf.Variable &#39;UnreadVariable&#39; shape=() dtype=float32, numpy=-9.980008&gt; . for 문을 이용 (시도1 : 100번) . beta = tf.Variable(-10.0) alpha=0.01/6 . for k in range(100): with tf.GradientTape(persistent=True) as tape: loss = (beta/2-1)**2 beta.assign_sub(alpha*tape.gradient(loss,beta)) . beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=-9.040152&gt; . for 문을 이용 (시도2 : 10,000번) . beta = tf.Variable(-10.0) alpha=0.01/6 . for k in range(10000): with tf.GradientTape(persistent=True) as tape: loss = (beta/2-1)**2 beta.assign_sub(alpha*tape.gradient(loss,beta)) . beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=1.9971251&gt; . 사실 10,000 번까지 갈 필요도 없다 그냥 $ alpha$ 값을 크게 키우면 된다. -&gt; 사실 이것도 분류 모델에서는 과적합 문제로 이어질 수 있다. | . &#54617;&#49845;&#47456; . - $ alpha$ 값의 변화에 따라서 최적해가 어떻게 수렴하는지 시각화 해보자. . [&#49884;&#44033;&#54868; &#53076;&#46300; &#50696;&#48708;&#54617;&#49845;] . plt.plot([1,2,3],[3,4,5],&quot;ro&quot;) . [&lt;matplotlib.lines.Line2D at 0x7f44a79e9a50&gt;] . 도화지 생성 | . fig = plt.figure() . &lt;Figure size 432x288 with 0 Axes&gt; . 도화지 안의 어떤 틀을 생성 | . ax = fig.add_subplot() . fig . fig.axes[0] . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f44a7583850&gt; . type(fig.axes[0]) . matplotlib.axes._subplots.AxesSubplot . id(fig.axes[0]) . 139932842080336 . - 네모틀(ax)의 특수기능(=메소드)중에는 plot이 있음. 이것은 또 어떤 오브젝트를 생성함 . python . 길이가 1인 튜플 a,=1 . pnts, = ax.plot([1,2,3],[3,4,5],&#39;or&#39;) pnts . &lt;matplotlib.lines.Line2D at 0x7f44a7565750&gt; . fig . - pnts 오브젝트: x,y data를 변경해보자. . pnts.get_xdata(),pnts.get_ydata() . (array([1, 2, 3]), array([3, 4, 5])) . pnts.get_ydata() . pnts.set_ydata([5,5,5]) . pnts.get_ydata() . [5, 5, 5] . fig . 응용 숫자에 변화에 따른 animation을 구현해보자 . plt.rcParams[&quot;animation.html&quot;]=&quot;jshtml&quot; from matplotlib import animation . def animate(i): if i%2 == 0: pnts.set_ydata([3,4,5]) else: pnts.set_ydata([4,4,4]) . ani=animation.FuncAnimation(fig,animate,frames=10) ani . &lt;/input&gt; Once Loop Reflect - 위를 적용하여 최적해를 찾는 과정 즉, 수렴과정을 시각화하자 . $ beta :-10 to-9 to-8$ 이런식으로 이동한다고 하자 | 그에 대한 $loss$도 저장 | . beta_lst = [-10.0,-9.00,-8.00] loss_lst = [(-10.0/2-1)**2,(-9.00/2-1)**2,(-8.00/2-1)**2] . fig = plt.figure() ax = fig.add_subplot() . 그 후 $ beta$ 가 이동할 경로를 그려주자 | . _beta= np.linspace(-15,19) ax.plot(_beta,(_beta/2-1)**2) . [&lt;matplotlib.lines.Line2D at 0x7f44a5c91cd0&gt;] . fig . pnts, = ax.plot(beta_lst[0],loss_lst[0],&#39;ro&#39;) . animation 구현 | . def animate(i): pnts.set_xdata(beta_lst[:(i+1)]) pnts.set_ydata(loss_lst[:(i+1)]) . ani=animation.FuncAnimation(fig,animate,frames=3) ## frame은 리스트 길이라고 생각 ani . &lt;/input&gt; Once Loop Reflect 이제 목적대로 구현해보자 | . 1 초깃값 설정 . beta = tf.Variable(10.0) . beta.numpy() alpha=0.01/6 . beta_lst=[] beta_lst.append(beta.numpy()) . loss_lst=[] loss_lst.append((beta.numpy()/2-1)**2) . 2 초깃값을 설정했으니 최적해를 찾아보자 . for k in range(100) : with tf.GradientTape(persistent=True) as tape : tape.watch(beta) loss = (beta/2-1)**2 beta.assign_sub(alpha*tape.gradient(loss,beta)) beta_lst.append(beta.numpy()) loss_lst.append((beta.numpy()/2-1)**2) . fig = plt.figure() # fig 는 도화지 . &lt;Figure size 432x288 with 0 Axes&gt; . ax = fig.add_subplot() ax.plot(_beta,(_beta/2-1)**2) pnts, = ax.plot(beta_lst[0],loss_lst[0],&#39;or&#39;) . ani=animation.FuncAnimation(fig,animate,frames=100) ani . &lt;/input&gt; Once Loop Reflect 3 $ alpha$ 값을 살펴보니 깂이 너무 작아서 이동의 폭이 너무 짧음 $ alpha to 0.1$ 로 변경 . beta = tf.Variable(10.0) beta.numpy() alpha=0.1 beta_lst=[] beta_lst.append(beta.numpy()) loss_lst=[] loss_lst.append((beta.numpy()/2-1)**2) for k in range(100) : with tf.GradientTape(persistent=True) as tape : tape.watch(beta) loss = (beta/2-1)**2 beta.assign_sub(alpha*tape.gradient(loss,beta)) beta_lst.append(beta.numpy()) loss_lst.append((beta.numpy()/2-1)**2) . fig = plt.figure() # fig 는 도화지 ax = fig.add_subplot() ax.plot(_beta,(_beta/2-1)**2) pnts, = ax.plot(beta_lst[0],loss_lst[0],&#39;or&#39;) . ani=animation.FuncAnimation(fig,animate,frames=100) ani . &lt;/input&gt; Once Loop Reflect 오 아까보단 빠르당 | . 4 $ alpha to 1$ 로 변경 . beta = tf.Variable(10.0) beta.numpy() alpha=1 beta_lst=[] beta_lst.append(beta.numpy()) loss_lst=[] loss_lst.append((beta.numpy()/2-1)**2) for k in range(100) : with tf.GradientTape(persistent=True) as tape : tape.watch(beta) loss = (beta/2-1)**2 beta.assign_sub(alpha*tape.gradient(loss,beta)) beta_lst.append(beta.numpy()) loss_lst.append((beta.numpy()/2-1)**2) . fig = plt.figure() # fig 는 도화지 ax = fig.add_subplot() ax.plot(_beta,(_beta/2-1)**2) pnts, = ax.plot(beta_lst[0],loss_lst[0],&#39;or&#39;) . ani=animation.FuncAnimation(fig,animate,frames=100) ani . &lt;/input&gt; Once Loop Reflect",
            "url": "https://gangcheol.github.io/big-data-analysis/2022/03/28/(4%EC%A3%BC%EC%B0%A8).html",
            "relUrl": "/2022/03/28/(4%EC%A3%BC%EC%B0%A8).html",
            "date": " • Mar 28, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "(3주차) 3월21일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . imports . import tensorflow as tf import numpy as np . tf.config.experimental.list_physical_devices(&#39;GPU&#39;) . [PhysicalDevice(name=&#39;/physical_device:GPU:0&#39;, device_type=&#39;GPU&#39;)] . &#51648;&#45212;&#44053;&#51032; &#48372;&#52649; . - max, min, sum, mean . a= tf.constant([1.0,2.0,3.0,4.0]) a . &lt;tf.Tensor: shape=(4,), dtype=float32, numpy=array([1., 2., 3., 4.], dtype=float32)&gt; . tf.reduce_mean(a) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=2.5&gt; . concat, stack . - 예제: (2,3,4,5) stack (2,3,4,5) -&gt; (?,?,?,?,?) . a = tf.reshape(tf.constant(range(2*3*4*5)),(2,3,4,5)) b = -a . case1 (1,2,3,4,5) stack (1,2,3,4,5) --&gt; (2,2,3,4,5) # axis=0 . tf.stack([a,b],axis=0) . &lt;tf.Tensor: shape=(2, 2, 3, 4, 5), dtype=int32, numpy= array([[[[[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [ 10, 11, 12, 13, 14], [ 15, 16, 17, 18, 19]], [[ 20, 21, 22, 23, 24], [ 25, 26, 27, 28, 29], [ 30, 31, 32, 33, 34], [ 35, 36, 37, 38, 39]], [[ 40, 41, 42, 43, 44], [ 45, 46, 47, 48, 49], [ 50, 51, 52, 53, 54], [ 55, 56, 57, 58, 59]]], [[[ 60, 61, 62, 63, 64], [ 65, 66, 67, 68, 69], [ 70, 71, 72, 73, 74], [ 75, 76, 77, 78, 79]], [[ 80, 81, 82, 83, 84], [ 85, 86, 87, 88, 89], [ 90, 91, 92, 93, 94], [ 95, 96, 97, 98, 99]], [[ 100, 101, 102, 103, 104], [ 105, 106, 107, 108, 109], [ 110, 111, 112, 113, 114], [ 115, 116, 117, 118, 119]]]], [[[[ 0, -1, -2, -3, -4], [ -5, -6, -7, -8, -9], [ -10, -11, -12, -13, -14], [ -15, -16, -17, -18, -19]], [[ -20, -21, -22, -23, -24], [ -25, -26, -27, -28, -29], [ -30, -31, -32, -33, -34], [ -35, -36, -37, -38, -39]], [[ -40, -41, -42, -43, -44], [ -45, -46, -47, -48, -49], [ -50, -51, -52, -53, -54], [ -55, -56, -57, -58, -59]]], [[[ -60, -61, -62, -63, -64], [ -65, -66, -67, -68, -69], [ -70, -71, -72, -73, -74], [ -75, -76, -77, -78, -79]], [[ -80, -81, -82, -83, -84], [ -85, -86, -87, -88, -89], [ -90, -91, -92, -93, -94], [ -95, -96, -97, -98, -99]], [[-100, -101, -102, -103, -104], [-105, -106, -107, -108, -109], [-110, -111, -112, -113, -114], [-115, -116, -117, -118, -119]]]]], dtype=int32)&gt; . case2 (2,1,3,4,5) stack (2,1,3,4,5) --&gt; (2,2,3,4,5) # axis=1 . tf.stack([a,b],axis=1) . &lt;tf.Tensor: shape=(2, 2, 3, 4, 5), dtype=int32, numpy= array([[[[[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [ 10, 11, 12, 13, 14], [ 15, 16, 17, 18, 19]], [[ 20, 21, 22, 23, 24], [ 25, 26, 27, 28, 29], [ 30, 31, 32, 33, 34], [ 35, 36, 37, 38, 39]], [[ 40, 41, 42, 43, 44], [ 45, 46, 47, 48, 49], [ 50, 51, 52, 53, 54], [ 55, 56, 57, 58, 59]]], [[[ 0, -1, -2, -3, -4], [ -5, -6, -7, -8, -9], [ -10, -11, -12, -13, -14], [ -15, -16, -17, -18, -19]], [[ -20, -21, -22, -23, -24], [ -25, -26, -27, -28, -29], [ -30, -31, -32, -33, -34], [ -35, -36, -37, -38, -39]], [[ -40, -41, -42, -43, -44], [ -45, -46, -47, -48, -49], [ -50, -51, -52, -53, -54], [ -55, -56, -57, -58, -59]]]], [[[[ 60, 61, 62, 63, 64], [ 65, 66, 67, 68, 69], [ 70, 71, 72, 73, 74], [ 75, 76, 77, 78, 79]], [[ 80, 81, 82, 83, 84], [ 85, 86, 87, 88, 89], [ 90, 91, 92, 93, 94], [ 95, 96, 97, 98, 99]], [[ 100, 101, 102, 103, 104], [ 105, 106, 107, 108, 109], [ 110, 111, 112, 113, 114], [ 115, 116, 117, 118, 119]]], [[[ -60, -61, -62, -63, -64], [ -65, -66, -67, -68, -69], [ -70, -71, -72, -73, -74], [ -75, -76, -77, -78, -79]], [[ -80, -81, -82, -83, -84], [ -85, -86, -87, -88, -89], [ -90, -91, -92, -93, -94], [ -95, -96, -97, -98, -99]], [[-100, -101, -102, -103, -104], [-105, -106, -107, -108, -109], [-110, -111, -112, -113, -114], [-115, -116, -117, -118, -119]]]]], dtype=int32)&gt; . case3 (2,3,1,4,5) stack (2,3,1,4,5) --&gt; (2,3,2,4,5) # axis=2 . tf.stack([a,b],axis=2) . &lt;tf.Tensor: shape=(2, 3, 2, 4, 5), dtype=int32, numpy= array([[[[[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [ 10, 11, 12, 13, 14], [ 15, 16, 17, 18, 19]], [[ 0, -1, -2, -3, -4], [ -5, -6, -7, -8, -9], [ -10, -11, -12, -13, -14], [ -15, -16, -17, -18, -19]]], [[[ 20, 21, 22, 23, 24], [ 25, 26, 27, 28, 29], [ 30, 31, 32, 33, 34], [ 35, 36, 37, 38, 39]], [[ -20, -21, -22, -23, -24], [ -25, -26, -27, -28, -29], [ -30, -31, -32, -33, -34], [ -35, -36, -37, -38, -39]]], [[[ 40, 41, 42, 43, 44], [ 45, 46, 47, 48, 49], [ 50, 51, 52, 53, 54], [ 55, 56, 57, 58, 59]], [[ -40, -41, -42, -43, -44], [ -45, -46, -47, -48, -49], [ -50, -51, -52, -53, -54], [ -55, -56, -57, -58, -59]]]], [[[[ 60, 61, 62, 63, 64], [ 65, 66, 67, 68, 69], [ 70, 71, 72, 73, 74], [ 75, 76, 77, 78, 79]], [[ -60, -61, -62, -63, -64], [ -65, -66, -67, -68, -69], [ -70, -71, -72, -73, -74], [ -75, -76, -77, -78, -79]]], [[[ 80, 81, 82, 83, 84], [ 85, 86, 87, 88, 89], [ 90, 91, 92, 93, 94], [ 95, 96, 97, 98, 99]], [[ -80, -81, -82, -83, -84], [ -85, -86, -87, -88, -89], [ -90, -91, -92, -93, -94], [ -95, -96, -97, -98, -99]]], [[[ 100, 101, 102, 103, 104], [ 105, 106, 107, 108, 109], [ 110, 111, 112, 113, 114], [ 115, 116, 117, 118, 119]], [[-100, -101, -102, -103, -104], [-105, -106, -107, -108, -109], [-110, -111, -112, -113, -114], [-115, -116, -117, -118, -119]]]]], dtype=int32)&gt; . case4 (2,3,4,1,5) stack (2,3,4,1,5) --&gt; (2,3,4,2,5) # axis=3 . tf.stack([a,b],axis=-2) . &lt;tf.Tensor: shape=(2, 3, 4, 2, 5), dtype=int32, numpy= array([[[[[ 0, 1, 2, 3, 4], [ 0, -1, -2, -3, -4]], [[ 5, 6, 7, 8, 9], [ -5, -6, -7, -8, -9]], [[ 10, 11, 12, 13, 14], [ -10, -11, -12, -13, -14]], [[ 15, 16, 17, 18, 19], [ -15, -16, -17, -18, -19]]], [[[ 20, 21, 22, 23, 24], [ -20, -21, -22, -23, -24]], [[ 25, 26, 27, 28, 29], [ -25, -26, -27, -28, -29]], [[ 30, 31, 32, 33, 34], [ -30, -31, -32, -33, -34]], [[ 35, 36, 37, 38, 39], [ -35, -36, -37, -38, -39]]], [[[ 40, 41, 42, 43, 44], [ -40, -41, -42, -43, -44]], [[ 45, 46, 47, 48, 49], [ -45, -46, -47, -48, -49]], [[ 50, 51, 52, 53, 54], [ -50, -51, -52, -53, -54]], [[ 55, 56, 57, 58, 59], [ -55, -56, -57, -58, -59]]]], [[[[ 60, 61, 62, 63, 64], [ -60, -61, -62, -63, -64]], [[ 65, 66, 67, 68, 69], [ -65, -66, -67, -68, -69]], [[ 70, 71, 72, 73, 74], [ -70, -71, -72, -73, -74]], [[ 75, 76, 77, 78, 79], [ -75, -76, -77, -78, -79]]], [[[ 80, 81, 82, 83, 84], [ -80, -81, -82, -83, -84]], [[ 85, 86, 87, 88, 89], [ -85, -86, -87, -88, -89]], [[ 90, 91, 92, 93, 94], [ -90, -91, -92, -93, -94]], [[ 95, 96, 97, 98, 99], [ -95, -96, -97, -98, -99]]], [[[ 100, 101, 102, 103, 104], [-100, -101, -102, -103, -104]], [[ 105, 106, 107, 108, 109], [-105, -106, -107, -108, -109]], [[ 110, 111, 112, 113, 114], [-110, -111, -112, -113, -114]], [[ 115, 116, 117, 118, 119], [-115, -116, -117, -118, -119]]]]], dtype=int32)&gt; . case5 (2,3,4,5,1) stack (2,3,4,5,1) --&gt; (2,3,4,5,2) # axis=4 . tf.stack([a,b],axis=-1) . &lt;tf.Tensor: shape=(2, 3, 4, 5, 2), dtype=int32, numpy= array([[[[[ 0, 0], [ 1, -1], [ 2, -2], [ 3, -3], [ 4, -4]], [[ 5, -5], [ 6, -6], [ 7, -7], [ 8, -8], [ 9, -9]], [[ 10, -10], [ 11, -11], [ 12, -12], [ 13, -13], [ 14, -14]], [[ 15, -15], [ 16, -16], [ 17, -17], [ 18, -18], [ 19, -19]]], [[[ 20, -20], [ 21, -21], [ 22, -22], [ 23, -23], [ 24, -24]], [[ 25, -25], [ 26, -26], [ 27, -27], [ 28, -28], [ 29, -29]], [[ 30, -30], [ 31, -31], [ 32, -32], [ 33, -33], [ 34, -34]], [[ 35, -35], [ 36, -36], [ 37, -37], [ 38, -38], [ 39, -39]]], [[[ 40, -40], [ 41, -41], [ 42, -42], [ 43, -43], [ 44, -44]], [[ 45, -45], [ 46, -46], [ 47, -47], [ 48, -48], [ 49, -49]], [[ 50, -50], [ 51, -51], [ 52, -52], [ 53, -53], [ 54, -54]], [[ 55, -55], [ 56, -56], [ 57, -57], [ 58, -58], [ 59, -59]]]], [[[[ 60, -60], [ 61, -61], [ 62, -62], [ 63, -63], [ 64, -64]], [[ 65, -65], [ 66, -66], [ 67, -67], [ 68, -68], [ 69, -69]], [[ 70, -70], [ 71, -71], [ 72, -72], [ 73, -73], [ 74, -74]], [[ 75, -75], [ 76, -76], [ 77, -77], [ 78, -78], [ 79, -79]]], [[[ 80, -80], [ 81, -81], [ 82, -82], [ 83, -83], [ 84, -84]], [[ 85, -85], [ 86, -86], [ 87, -87], [ 88, -88], [ 89, -89]], [[ 90, -90], [ 91, -91], [ 92, -92], [ 93, -93], [ 94, -94]], [[ 95, -95], [ 96, -96], [ 97, -97], [ 98, -98], [ 99, -99]]], [[[ 100, -100], [ 101, -101], [ 102, -102], [ 103, -103], [ 104, -104]], [[ 105, -105], [ 106, -106], [ 107, -107], [ 108, -108], [ 109, -109]], [[ 110, -110], [ 111, -111], [ 112, -112], [ 113, -113], [ 114, -114]], [[ 115, -115], [ 116, -116], [ 117, -117], [ 118, -118], [ 119, -119]]]]], dtype=int32)&gt; . - 예제: (2,3,4), (2,3,4), (2,3,4) . a= tf.reshape(tf.constant(range(2*3*4)),(2,3,4)) b= -a c= 2*a . (예시1) (2,3,4), (2,3,4), (2,3,4) $ to$ (6,3,4) . tf.concat([a,b,c],axis=0) . &lt;tf.Tensor: shape=(6, 3, 4), dtype=int32, numpy= array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[ 12, 13, 14, 15], [ 16, 17, 18, 19], [ 20, 21, 22, 23]], [[ 0, -1, -2, -3], [ -4, -5, -6, -7], [ -8, -9, -10, -11]], [[-12, -13, -14, -15], [-16, -17, -18, -19], [-20, -21, -22, -23]], [[ 0, 2, 4, 6], [ 8, 10, 12, 14], [ 16, 18, 20, 22]], [[ 24, 26, 28, 30], [ 32, 34, 36, 38], [ 40, 42, 44, 46]]], dtype=int32)&gt; . (예시2) (2,3,4), (2,3,4), (2,3,4) $ to$ (2,9,4) . tf.concat([a,b,c],axis=1) . &lt;tf.Tensor: shape=(2, 9, 4), dtype=int32, numpy= array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [ 0, -1, -2, -3], [ -4, -5, -6, -7], [ -8, -9, -10, -11], [ 0, 2, 4, 6], [ 8, 10, 12, 14], [ 16, 18, 20, 22]], [[ 12, 13, 14, 15], [ 16, 17, 18, 19], [ 20, 21, 22, 23], [-12, -13, -14, -15], [-16, -17, -18, -19], [-20, -21, -22, -23], [ 24, 26, 28, 30], [ 32, 34, 36, 38], [ 40, 42, 44, 46]]], dtype=int32)&gt; . (예시3) (2,3,4), (2,3,4), (2,3,4) $ to$ (2,3,12) . tf.concat([a,b,c],axis=-1) . &lt;tf.Tensor: shape=(2, 3, 12), dtype=int32, numpy= array([[[ 0, 1, 2, 3, 0, -1, -2, -3, 0, 2, 4, 6], [ 4, 5, 6, 7, -4, -5, -6, -7, 8, 10, 12, 14], [ 8, 9, 10, 11, -8, -9, -10, -11, 16, 18, 20, 22]], [[ 12, 13, 14, 15, -12, -13, -14, -15, 24, 26, 28, 30], [ 16, 17, 18, 19, -16, -17, -18, -19, 32, 34, 36, 38], [ 20, 21, 22, 23, -20, -21, -22, -23, 40, 42, 44, 46]]], dtype=int32)&gt; . (예시4) (2,3,4), (2,3,4), (2,3,4) $ to$ (3,2,3,4) . tf.stack([a,b,c],axis=0) . &lt;tf.Tensor: shape=(3, 2, 3, 4), dtype=int32, numpy= array([[[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[ 12, 13, 14, 15], [ 16, 17, 18, 19], [ 20, 21, 22, 23]]], [[[ 0, -1, -2, -3], [ -4, -5, -6, -7], [ -8, -9, -10, -11]], [[-12, -13, -14, -15], [-16, -17, -18, -19], [-20, -21, -22, -23]]], [[[ 0, 2, 4, 6], [ 8, 10, 12, 14], [ 16, 18, 20, 22]], [[ 24, 26, 28, 30], [ 32, 34, 36, 38], [ 40, 42, 44, 46]]]], dtype=int32)&gt; . (예시5) (2,3,4), (2,3,4), (2,3,4) $ to$ (2,3,3,4) . tf.stack([a,b,c],axis=1) . &lt;tf.Tensor: shape=(2, 3, 3, 4), dtype=int32, numpy= array([[[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[ 0, -1, -2, -3], [ -4, -5, -6, -7], [ -8, -9, -10, -11]], [[ 0, 2, 4, 6], [ 8, 10, 12, 14], [ 16, 18, 20, 22]]], [[[ 12, 13, 14, 15], [ 16, 17, 18, 19], [ 20, 21, 22, 23]], [[-12, -13, -14, -15], [-16, -17, -18, -19], [-20, -21, -22, -23]], [[ 24, 26, 28, 30], [ 32, 34, 36, 38], [ 40, 42, 44, 46]]]], dtype=int32)&gt; . (예시6) (2,3,4), (2,3,4), (2,3,4) $ to$ (2,3,3,4) . tf.stack([a,b,c],axis=2) . &lt;tf.Tensor: shape=(2, 3, 3, 4), dtype=int32, numpy= array([[[[ 0, 1, 2, 3], [ 0, -1, -2, -3], [ 0, 2, 4, 6]], [[ 4, 5, 6, 7], [ -4, -5, -6, -7], [ 8, 10, 12, 14]], [[ 8, 9, 10, 11], [ -8, -9, -10, -11], [ 16, 18, 20, 22]]], [[[ 12, 13, 14, 15], [-12, -13, -14, -15], [ 24, 26, 28, 30]], [[ 16, 17, 18, 19], [-16, -17, -18, -19], [ 32, 34, 36, 38]], [[ 20, 21, 22, 23], [-20, -21, -22, -23], [ 40, 42, 44, 46]]]], dtype=int32)&gt; . (예시7) (2,3,4), (2,3,4), (2,3,4) $ to$ (2,3,4,3) . tf.stack([a,b,c],axis=-1) . &lt;tf.Tensor: shape=(2, 3, 4, 3), dtype=int32, numpy= array([[[[ 0, 0, 0], [ 1, -1, 2], [ 2, -2, 4], [ 3, -3, 6]], [[ 4, -4, 8], [ 5, -5, 10], [ 6, -6, 12], [ 7, -7, 14]], [[ 8, -8, 16], [ 9, -9, 18], [ 10, -10, 20], [ 11, -11, 22]]], [[[ 12, -12, 24], [ 13, -13, 26], [ 14, -14, 28], [ 15, -15, 30]], [[ 16, -16, 32], [ 17, -17, 34], [ 18, -18, 36], [ 19, -19, 38]], [[ 20, -20, 40], [ 21, -21, 42], [ 22, -22, 44], [ 23, -23, 46]]]], dtype=int32)&gt; . - 예제: (2,3,4) (4,3,4) $ to$ (6,3,4) . a=tf.reshape(tf.constant(range(2*3*4)),(2,3,4)) b=tf.reshape(-tf.constant(range(4*3*4)),(4,3,4)) . tf.concat([a,b],axis=0) . &lt;tf.Tensor: shape=(6, 3, 4), dtype=int32, numpy= array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[ 12, 13, 14, 15], [ 16, 17, 18, 19], [ 20, 21, 22, 23]], [[ 0, -1, -2, -3], [ -4, -5, -6, -7], [ -8, -9, -10, -11]], [[-12, -13, -14, -15], [-16, -17, -18, -19], [-20, -21, -22, -23]], [[-24, -25, -26, -27], [-28, -29, -30, -31], [-32, -33, -34, -35]], [[-36, -37, -38, -39], [-40, -41, -42, -43], [-44, -45, -46, -47]]], dtype=int32)&gt; . tf.concat([a,b],axis=1) . InvalidArgumentError Traceback (most recent call last) Input In [22], in &lt;cell line: 1&gt;() -&gt; 1 tf.concat([a,b],axis=1) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: ConcatOp : Dimensions of inputs should match: shape[0] = [2,3,4] vs. shape[1] = [4,3,4] [Op:ConcatV2] name: concat . tf.concat([a,b],axis=2) . InvalidArgumentError Traceback (most recent call last) Input In [23], in &lt;cell line: 1&gt;() -&gt; 1 tf.concat([a,b],axis=2) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: ConcatOp : Dimensions of inputs should match: shape[0] = [2,3,4] vs. shape[1] = [4,3,4] [Op:ConcatV2] name: concat . - (2,2) @ (2,) 의 연산? . numpy . np.array([[1,0],[0,1]]) @ np.array([77,-88]) . array([ 77, -88]) . np.array([77,-88]) @ np.array([[1,0],[0,1]]) . array([ 77, -88]) . np.array([[1,0],[0,1]]) @ np.array([77,-88]).reshape(2,1) . array([[ 77], [-88]]) . np.array([77,-88]).reshape(2,1) @ np.array([[1,0],[0,1]]) . ValueError Traceback (most recent call last) Input In [27], in &lt;cell line: 1&gt;() -&gt; 1 np.array([77,-88]).reshape(2,1) @ np.array([[1,0],[0,1]]) ValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)-&gt;(n?,m?) (size 2 is different from 1) . np.array([77,-88]).reshape(1,2) @ np.array([[1,0],[0,1]]) . array([[ 77, -88]]) . tensorflow . I = tf.constant([[1.0,0.0],[0.0,1.0]]) x = tf.constant([77.0,-88.0]) . I @ x . InvalidArgumentError Traceback (most recent call last) Input In [30], in &lt;cell line: 1&gt;() -&gt; 1 I @ x File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: In[0] and In[1] has different ndims: [2,2] vs. [2] [Op:MatMul] . x @ I . InvalidArgumentError Traceback (most recent call last) Input In [31], in &lt;cell line: 1&gt;() -&gt; 1 x @ I File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: In[0] and In[1] has different ndims: [2] vs. [2,2] [Op:MatMul] . I @ tf.reshape(x,(2,1)) . &lt;tf.Tensor: shape=(2, 1), dtype=float32, numpy= array([[ 77.], [-88.]], dtype=float32)&gt; . tf.reshape(x,(1,2)) @ I . &lt;tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 77., -88.]], dtype=float32)&gt; . . tf.Variable . &#49440;&#50616; . - tf.Variable()로 선언 . tf.Variable([1,2,3,4]) . &lt;tf.Variable &#39;Variable:0&#39; shape=(4,) dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt; . tf.Variable([1.0,2.0,3.0,4.0]) . &lt;tf.Variable &#39;Variable:0&#39; shape=(4,) dtype=float32, numpy=array([1., 2., 3., 4.], dtype=float32)&gt; . - tf.constant() 선언후 변환 . tf.Variable(tf.constant([1,2,3,4])) . &lt;tf.Variable &#39;Variable:0&#39; shape=(4,) dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt; . - np 등으로 선언후 변환 . tf.Variable(np.array([1,2,3,4])) . &lt;tf.Variable &#39;Variable:0&#39; shape=(4,) dtype=int64, numpy=array([1, 2, 3, 4])&gt; . &#53440;&#51077; . type(tf.Variable([1,2,3,4])) . tensorflow.python.ops.resource_variable_ops.ResourceVariable . &#51064;&#45937;&#49905; . a=tf.Variable([1,2,3,4]) a . &lt;tf.Variable &#39;Variable:0&#39; shape=(4,) dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt; . a[:2] . &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)&gt; . &#50672;&#49328;&#44032;&#45733; . a=tf.Variable([1,2,3,4]) b=tf.Variable([-1,-2,-3,-4]) . a+b . &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 0, 0, 0], dtype=int32)&gt; . tf.Variable&#46020; &#50416;&#44592; &#48520;&#54200;&#54632; . tf.Variable([1,2])+tf.Variable([3.14,3.14]) . InvalidArgumentError Traceback (most recent call last) Input In [43], in &lt;cell line: 1&gt;() -&gt; 1 tf.Variable([1,2])+tf.Variable([3.14,3.14]) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/ops/variables.py:1078, in Variable._OverloadOperator.&lt;locals&gt;._run_op(a, *args, **kwargs) 1076 def _run_op(a, *args, **kwargs): 1077 # pylint: disable=protected-access -&gt; 1078 return tensor_oper(a.value(), *args, **kwargs) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: cannot compute AddV2 as input #1(zero-based) was expected to be a int32 tensor but is a float tensor [Op:AddV2] . tnp&#51032; &#51008;&#52509;&#46020; &#51068;&#48512;&#47564; &#44032;&#45733; . import tensorflow.experimental.numpy as tnp tnp.experimental_enable_numpy_behavior() . - 알아서 형 변환 . tf.Variable([1,2])+tf.Variable([3.14,3.14]) . &lt;tf.Tensor: shape=(2,), dtype=float64, numpy=array([4.1400001, 5.1400001])&gt; . - .reshape 메소드 . tf.Variable([1,2,3,4]).reshape(2,2) . AttributeError Traceback (most recent call last) Input In [46], in &lt;cell line: 1&gt;() -&gt; 1 tf.Variable([1,2,3,4]).reshape(2,2) AttributeError: &#39;ResourceVariable&#39; object has no attribute &#39;reshape&#39; . &#45824;&#48512;&#48516;&#51032; &#46041;&#51089;&#51008; tf.constant&#46993; &#53360; &#52264;&#51060;&#47484; &#47784;&#47476;&#44192;&#51020; . - tf.concat . a= tf.Variable([[1,2],[3,4]]) b= tf.Variable([[-1,-2],[-3,-4]]) tf.concat([a,b],axis=0) . &lt;tf.Tensor: shape=(4, 2), dtype=int32, numpy= array([[ 1, 2], [ 3, 4], [-1, -2], [-3, -4]], dtype=int32)&gt; . - tf.stack . a= tf.Variable([[1,2],[3,4]]) b= tf.Variable([[-1,-2],[-3,-4]]) tf.stack([a,b],axis=0) . &lt;tf.Tensor: shape=(2, 2, 2), dtype=int32, numpy= array([[[ 1, 2], [ 3, 4]], [[-1, -2], [-3, -4]]], dtype=int32)&gt; . &#48320;&#49688;&#44050;&#48320;&#44221;&#44032;&#45733;(?) . a= tf.Variable([1,2,3,4]) id(a) . 140652736059120 . a.assign_add([-1,-2,-3,-4]) id(a) . 140652736059120 . &#50836;&#50557; . - tf.Variable()로 만들어야 하는 뚜렷한 차이는 모르겠음. . - 애써 tf.Variable()로 만들어도 간단한연산을 하면 그 결과는 tf.constant()로 만든 오브젝트와 동일해짐. . &#48120;&#48516; . &#47784;&#54000;&#48652; . - 예제: 컴퓨터를 이용하여 $x=2$에서 $y=3x^2$의 접선의 기울기를 구해보자. . (손풀이) . $$ frac{dy}{dx}=6x$$ . 이므로 $x=2$를 대입하면 12이다. . (컴퓨터를 이용한 풀이) . 단계1 . x1=2 y1= 3*x1**2 . x2=2+0.000000001 y2= 3*x2**2 . (y2-y1)/(x2-x1) . 12.0 . 단계2 . def f(x): return(3*x**2) . f(3) . 27 . def d(f,x): return (f(x+0.000000001)-f(x))/0.000000001 . d(f,2) . 12.000000992884452 . 단계3 . d(lambda x: 3*x**2 ,2) . 12.000000992884452 . d(lambda x: x**2 ,0) . 1e-09 . 단계4 . $$f(x,y)= x^2 +3y$$ . def f(x,y): return(x**2 +3*y) . d(f,(2,3)) . TypeError Traceback (most recent call last) Input In [61], in &lt;cell line: 1&gt;() -&gt; 1 d(f,(2,3)) Input In [56], in d(f, x) 1 def d(f,x): -&gt; 2 return (f(x+0.000000001)-f(x))/0.000000001 TypeError: can only concatenate tuple (not &#34;float&#34;) to tuple . tf.GradientTape() &#49324;&#50857;&#48169;&#48277; . - 예제1: $x=2$에서 $y=3x^2$의 도함수값을 구하라. . x=tf.Variable(2.0) a=tf.constant(3.0) . mytape=tf.GradientTape() mytape.__enter__() # 기록 시작 y=a*x**2 # y=ax^2 = 3x^2 mytape.__exit__(None,None,None) # 기록 끝 . mytape.gradient(y,x) # y를 x로 미분하라. . &lt;tf.Tensor: shape=(), dtype=float32, numpy=12.0&gt; . - 예제2: 조금 다른예제 . x=tf.Variable(2.0) #a=tf.constant(3.0) mytape=tf.GradientTape() mytape.__enter__() # 기록 시작 a=(x/2)*3 ## a=(3/2)x y=a*x**2 ## y=ax^2 = (3/2)x^3 mytape.__exit__(None,None,None) # 기록 끝 mytape.gradient(y,x) # y를 x로 미분하라. . &lt;tf.Tensor: shape=(), dtype=float32, numpy=18.0&gt; . $$a= frac{3}{2}x$$ $$y=ax^2= frac{3}{2}x^3$$ . $$ frac{dy}{dx}= frac{3}{2} 3x^2$$ . 3/2*3*4 . 18.0 . - 테이프의 개념 ($ star$) . (상황) . 우리가 어려운 미분계산을 컴퓨터에게 부탁하는 상황임. (예를들면 $y=3x^2$) 컴퓨터에게 부탁을 하기 위해서는 연습장(=테이프)에 $y=3x^2$이라는 수식을 써서 보여줘야하는데 이때 컴퓨터에게 target이 무엇인지 그리고 무엇으로 미분하고 싶은 것인지를 명시해야함. . (1) mytape = tf.GradientTape(): tf.GradientTape()는 연습장을 만드는 명령어, 만들어진 연습장을 mytape라고 이름을 붙인다. . (2) mytape.__enter__(): 만들어진 공책을 연다 (=기록할수 있는 상태로 만든다) . (3) a=x/2*3; y=a*x**2: 컴퓨터에게 전달할 수식을 쓴다 . (4) mytape.__exit__(None,None,None): 공책을 닫는다. . (5) mytape.gradient(y,x): $y$를 $x$로 미분하라는 메모를 남기고 컴퓨터에게 전달한다. . - 예제3: 연습장을 언제 열고 닫을지 결정하는건 중요하다. . x=tf.Variable(2.0) a=(x/2)*3 ## a=(3/2)x mytape=tf.GradientTape() mytape.__enter__() # 기록 시작 y=a*x**2 ## y=ax^2 = (3/2)x^3 mytape.__exit__(None,None,None) # 기록 끝 mytape.gradient(y,x) # y를 x로 미분하라. . &lt;tf.Tensor: shape=(), dtype=float32, numpy=12.0&gt; . - 예제4: with문과 함께 쓰는 tf.GradientTape() . x=tf.Variable(2.0) a=(x/2)*3 . with tf.GradientTape() as mytape: ## with문 시작 y=a*x**2 ## with문 끝 . mytape.gradient(y,x) # y를 x로 미분하라. . &lt;tf.Tensor: shape=(), dtype=float32, numpy=12.0&gt; . (문법해설) . 아래와 같이 쓴다. . with expression as myname: ## with문 시작: myname.__enter__() blabla ~ yadiyadi !! ## with문 끝: myname.__exit__() . (1) expression 의 실행결과 오브젝트가 생성, 생성된 오브젝트는 myname라고 이름붙임. 이 오브젝트는 .__enter__()와 .__exit__()를 숨겨진 기능으로 포함해야 한다. . (2) with문이 시작되면서 myname.__enter__()이 실행된다. . (3) 블라블라와 야디야디가 실행된다. . (4) with문이 종료되면서 myname.__exit__()이 실행된다. . - 예제5: 예제2를 with문과 함께 구현 . x=tf.Variable(2.0) with tf.GradientTape() as mytape: a=(x/2)*3 ## a=(3/2)x y=a*x**2 ## y=ax^2 = (3/2)x^3 mytape.gradient(y,x) # y를 x로 미분하라. . &lt;tf.Tensor: shape=(), dtype=float32, numpy=18.0&gt; . - 예제6: persistent = True . (관찰1) . x=tf.Variable(2.0) with tf.GradientTape() as mytape: a=(x/2)*3 ## a=(3/2)x y=a*x**2 ## y=ax^2 = (3/2)x^3 . mytape.gradient(y,x) # 2번이상 실행해서 에러를 관측하라 . RuntimeError Traceback (most recent call last) Input In [90], in &lt;cell line: 1&gt;() -&gt; 1 mytape.gradient(y,x) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py:1032, in GradientTape.gradient(self, target, sources, output_gradients, unconnected_gradients) 1002 &#34;&#34;&#34;Computes the gradient using operations recorded in context of this tape. 1003 1004 Note: Unless you set `persistent=True` a GradientTape can only be used to (...) 1029 called with an unknown value. 1030 &#34;&#34;&#34; 1031 if self._tape is None: -&gt; 1032 raise RuntimeError(&#34;A non-persistent GradientTape can only be used to &#34; 1033 &#34;compute one set of gradients (or jacobians)&#34;) 1034 if self._recording: 1035 if not self._persistent: RuntimeError: A non-persistent GradientTape can only be used to compute one set of gradients (or jacobians) . (관찰2) . x=tf.Variable(2.0) with tf.GradientTape(persistent=True) as mytape: a=(x/2)*3 ## a=(3/2)x y=a*x**2 ## y=ax^2 = (3/2)x^3 . mytape.gradient(y,x) # 2번이상실행해도 에러가 나지않음 . &lt;tf.Tensor: shape=(), dtype=float32, numpy=18.0&gt; . - 예제7: watch . (관찰1) . x=tf.constant(2.0) with tf.GradientTape(persistent=True) as mytape: a=(x/2)*3 ## a=(3/2)x y=a*x**2 ## y=ax^2 = (3/2)x^3 . print(mytape.gradient(y,x)) . None . (관찰2) . x=tf.constant(2.0) with tf.GradientTape(persistent=True) as mytape: mytape.watch(x) # 수동감시 a=(x/2)*3 ## a=(3/2)x y=a*x**2 ## y=ax^2 = (3/2)x^3 . print(mytape.gradient(y,x)) . tf.Tensor(18.0, shape=(), dtype=float32) . (관찰3) . x=tf.Variable(2.0) with tf.GradientTape(persistent=True,watch_accessed_variables=False) as mytape: # 자동감시 모드 해제 a=(x/2)*3 ## a=(3/2)x y=a*x**2 ## y=ax^2 = (3/2)x^3 . print(mytape.gradient(y,x)) . None . (관찰4) . x=tf.Variable(2.0) with tf.GradientTape(persistent=True,watch_accessed_variables=False) as mytape: # 자동감시 모드 해제 mytape.watch(x) a=(x/2)*3 ## a=(3/2)x y=a*x**2 ## y=ax^2 = (3/2)x^3 . print(mytape.gradient(y,x)) . tf.Tensor(18.0, shape=(), dtype=float32) . (관찰5) . x=tf.Variable(2.0) with tf.GradientTape(persistent=True) as mytape: mytape.watch(x) a=(x/2)*3 ## a=(3/2)x y=a*x**2 ## y=ax^2 = (3/2)x^3 . print(mytape.gradient(y,x)) . tf.Tensor(18.0, shape=(), dtype=float32) . - 예제9: 카페예제로 돌아오자. . - 예제10: 카페예제의 매트릭스 버전 . - 예제11: 위의 예제에서 이론적인 $ boldsymbol{ beta}$의 최적값을 찾아보고 (즉 $ hat{ boldsymbol{ beta}}$을 찾고) 그곳에서 loss의 미분을 구하라. 구한결과가 $ begin{bmatrix}0 0 end{bmatrix}$ 임을 확인하라. .",
            "url": "https://gangcheol.github.io/big-data-analysis/2022/03/21/(3%EC%A3%BC%EC%B0%A8).html",
            "relUrl": "/2022/03/21/(3%EC%A3%BC%EC%B0%A8).html",
            "date": " • Mar 21, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "(2주차) Tensorflow",
            "content": "1&#51452;&#52264; &#48373;&#49845; . &#54924;&#44480;&#44228;&#49688; &#52628;&#51221;&#52824; . 단순선형회귀의 경우 일반적인 베타계수의 추정치 | . $ hat { beta_0} = bar y - beta _1 bar x, quad hat { beta_1} = frac {S_{xy}} {S_{xx}}$ . 다중 회귀의 경우 | . $$L=loss =({ bf y}-{ bf X}{ boldsymbol beta})^ top({ bf y}-{ bf X}{ boldsymbol beta})$$ . $$ L= { bf y}^ top { bf y} - { bf y}^ top { bf X}{ boldsymbol beta} - { boldsymbol beta}^ top { bf X}^ top { bf y} + { boldsymbol beta}^ top { bf X}^ top { bf X} { boldsymbol beta}$$ . 위를 미분하면 | . $$ frac{ partial}{ partial boldsymbol{ beta}} L = frac{ partial}{ partial boldsymbol{ beta}} { bf y}^ top { bf y} - frac{ partial}{ partial boldsymbol{ beta}} { bf y}^ top { bf X}{ boldsymbol beta} - frac{ partial}{ partial boldsymbol{ beta}} { boldsymbol beta}^ top { bf X}^ top { bf y} + frac{ partial}{ partial boldsymbol{ beta}} { boldsymbol beta}^ top { bf X}^ top { bf X} { boldsymbol beta} $$ . $$ frac{ partial}{ partial boldsymbol{ beta}} L=- mathbf{X^{ top}y} - mathbf{X^{ top}y} + 2 mathbf{X^{ top}X boldsymbol beta}$$ . 따라서 . $$ quad bf{X^{ top}X} beta = bf{X^{ top}Y}$$ $$ therefore quad hat { beta} = left( bf{X^{ top}X}^{-1} right)XY$$ . . 2&#51452;&#52264; &#49688;&#50629;&#49884;&#51089;~~ . import tensorflow as tf import numpy as np . . tensorflow&#51032; GPU &#50672;&#44208;&#48277; . tf.config.experimental.list_physical_devices(&#39;GPU&#39;) . [PhysicalDevice(name=&#39;/physical_device:GPU:0&#39;, device_type=&#39;GPU&#39;)] . tf.constant . &#50696;&#48708;&#54617;&#49845;: &#51473;&#52393;&#47532;&#49828;&#53944; . - 리스트 . lst = list(range(6)) lst . [0, 1, 2, 3, 4, 5] . lst[-1] . 5 . - 리스트 안에 리스트 생성 . lst =[[1,2,],[3,4]] lst . [[1, 2], [3, 4]] . print(lst[1][0],lst[0][0]) . 3 1 . - 위 같은 2차원의 리스트 구조를 행렬로 생각할 수 있다 . 1 2 3 4 . 또는 . 1 2 3 4 . - (4,1) 행렬 느낌의 리스트 . lst = [[1],[2],[3],[4]] lst . [[1], [2], [3], [4]] . np.array(lst) . array([[1], [2], [3], [4]]) . - (1,4) 행렬 느낌의 리스트 . lst = [1,2,3,4] lst . [1, 2, 3, 4] . np.array(lst) . array([1, 2, 3, 4]) . &#48320;&#49688; &#49440;&#50616; . &#49828;&#52860;&#46972; . tf.constant(3.14) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=3.14&gt; . tf.constant(3.14) + tf.constant(3.14) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=6.28&gt; . &#48289;&#53552; . _vector = tf.constant([1,2,3 ]) _vector . &lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 2, 3], dtype=int32)&gt; . _vector[0] . &lt;tf.Tensor: shape=(), dtype=int32, numpy=1&gt; . &#47588;&#53944;&#47533;&#49828; &#49373;&#49457; . _matrix = tf.constant([[1,0],[0,1]]) _matrix . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 0], [0, 1]], dtype=int32)&gt; . &#53584;&#49436; == 3&#52264;&#50896; &#51060;&#49345;&#51032; &#48176;&#50676; . np.array([[[0,1],[1,2]],[[0,1],[1,2]]]) . array([[[0, 1], [1, 2]], [[0, 1], [1, 2]]]) . tf.constant([[[0,1],[1,2]],[[0,1],[1,2]]]) . &lt;tf.Tensor: shape=(2, 2, 2), dtype=int32, numpy= array([[[0, 1], [1, 2]], [[0, 1], [1, 2]]], dtype=int32)&gt; . &#53440;&#51077; . type(tf.constant([[[0,1],[1,2]],[[0,1],[1,2]]])) . tensorflow.python.framework.ops.EagerTensor . - 끝에 EagerTensor 가 나오는 것을 기억하자 . &#51064;&#45937;&#49905; . _matrix = tf.constant([[1,2],[3,4]]) _matrix . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 2], [3, 4]], dtype=int32)&gt; . _matrix[0] . &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)&gt; . _matrix[0,:] . &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)&gt; . _matrix[0,0] . &lt;tf.Tensor: shape=(), dtype=int32, numpy=1&gt; . _matrix[0][0] . &lt;tf.Tensor: shape=(), dtype=int32, numpy=1&gt; . tf.constant&#45716; &#48520;&#54200;&#54616;&#45796;. . - 각 컬럼의 데이터 타입이 전부 동일하여야 한다. . - 원소 수정이 불가능함. . a= tf.constant([1,22,33]) a . &lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([ 1, 22, 33], dtype=int32)&gt; . a[0] =11 . TypeError Traceback (most recent call last) &lt;ipython-input-47-cf0e5bd1fc84&gt; in &lt;module&gt;() -&gt; 1 a[0] =11 TypeError: &#39;tensorflow.python.framework.ops.EagerTensor&#39; object does not support item assignment . - 묵시적(간접적) 형변환이 불가능하다. . 1+3.14 . 4.140000000000001 . tf.constant(1) + tf.constant(3.14) . InvalidArgumentError Traceback (most recent call last) &lt;ipython-input-54-51b23ac3bfb0&gt; in &lt;module&gt;() -&gt; 1 tf.constant(1) + tf.constant(3.14) /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py in error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name) 7184 def raise_from_not_ok_status(e, name): 7185 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7186 raise core._status_to_exception(e) from None # pylint: disable=protected-access 7187 7188 InvalidArgumentError: cannot compute AddV2 as input #1(zero-based) was expected to be a int32 tensor but is a float tensor [Op:AddV2] . - 같은 float 도 안되는 경우가 있음 . tf.constant(1.0, dtype= tf.float64) + tf.constant(3.14) . InvalidArgumentError Traceback (most recent call last) &lt;ipython-input-57-4390fbcde8ad&gt; in &lt;module&gt;() -&gt; 1 tf.constant(1.0, dtype= tf.float64) + tf.constant(3.14) /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py in error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name) 7184 def raise_from_not_ok_status(e, name): 7185 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7186 raise core._status_to_exception(e) from None # pylint: disable=protected-access 7187 7188 InvalidArgumentError: cannot compute AddV2 as input #1(zero-based) was expected to be a double tensor but is a float tensor [Op:AddV2] . tf.constant $ to$ &#45336;&#54028;&#51060; . np.array(tf.constant(1)) . array(1, dtype=int32) . a = tf.constant(3.14) type(a) . tensorflow.python.framework.ops.EagerTensor . a.numpy() . 3.14 . &#50672;&#49328; . &#45908;&#54616;&#44592; . a = tf.constant([1,2]) b = tf.constant([3,4]) a+b . &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([4, 6], dtype=int32)&gt; . tf.add(a,b) ## 이건 예전버전 . &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([4, 6], dtype=int32)&gt; . &#44273;&#54616;&#44592; . - 결과가 조금 이상하다. 일반적인 행렬연사이 아니다 . a = tf.constant([[1,2],[3,4]]) b = tf.constant([[5,6],[7,8]]) a*b . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[ 5, 12], [21, 32]], dtype=int32)&gt; . - but matrix의 곱은 . a = tf.constant([[1,0],[0,1]]) b = tf.constant([[5],[7]]) a@b . &lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy= array([[5], [7]], dtype=int32)&gt; . tf.matmul(a,b) ## 위와 같은 표현 . &#50669;&#54665;&#47148; . a = tf.constant([[1,0],[0,2]]) a . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 0], [0, 2]], dtype=int32)&gt; . tf.linalg.inv(a) . InvalidArgumentError Traceback (most recent call last) &lt;ipython-input-70-fd9ade66c025&gt; in &lt;module&gt;() -&gt; 1 tf.linalg.inv(a) /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_linalg_ops.py in matrix_inverse(input, adjoint, name) 1504 return _result 1505 except _core._NotOkStatusException as e: -&gt; 1506 _ops.raise_from_not_ok_status(e, name) 1507 except _core._FallbackException: 1508 pass /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name) 7184 def raise_from_not_ok_status(e, name): 7185 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7186 raise core._status_to_exception(e) from None # pylint: disable=protected-access 7187 7188 InvalidArgumentError: Value for attr &#39;T&#39; of int32 is not in the list of allowed values: double, float, half, complex64, complex128 ; NodeDef: {{node MatrixInverse}}; Op&lt;name=MatrixInverse; signature=input:T -&gt; output:T; attr=adjoint:bool,default=false; attr=T:type,allowed=[DT_DOUBLE, DT_FLOAT, DT_HALF, DT_COMPLEX64, DT_COMPLEX128]&gt; [Op:MatrixInverse] . 위의 경우는 자료가 int 형이여서 안되는 거임 | . ?tf.constant . 아래오 같이 자료형을 선언해 주어야함 | . a = tf.constant([[1,0],[0,2]],dtype=float) tf.linalg.inv(a) . &lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy= array([[1. , 0. ], [0. , 0.5]], dtype=float32)&gt; . determinant . a = tf.constant([[1,2],[3,4]],dtype=float) print(a) tf.linalg.det(a) . tf.Tensor( [[1. 2.] [3. 4.]], shape=(2, 2), dtype=float32) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=-2.0&gt; . Trace . tf.linalg.trace(a) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=5.0&gt; . &#54805;&#53468;&#48320;&#54872; . - 1 x 4 행렬을 $ to$ 4 x 1 . a = tf.constant([1,2,3,4]) a . &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt; . tf.reshape(a,(2,2)) . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 2], [3, 4]], dtype=int32)&gt; . - 3차원으로도 변경이 가능 . tf.reshape(a,(2,2,1)) . &lt;tf.Tensor: shape=(2, 2, 1), dtype=int32, numpy= array([[[1], [2]], [[3], [4]]], dtype=int32)&gt; . - 다차원의 경우 적용 . a = tf.constant(list(range(1,13))) a . &lt;tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], dtype=int32)&gt; . tf.reshape(a,(2,2,3)) . &lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy= array([[[ 1, 2, 3], [ 4, 5, 6]], [[ 7, 8, 9], [10, 11, 12]]], dtype=int32)&gt; . tf.reshape(a,(4,3)) . &lt;tf.Tensor: shape=(4, 3), dtype=int32, numpy= array([[ 1, 2, 3], [ 4, 5, 6], [ 7, 8, 9], [10, 11, 12]], dtype=int32)&gt; . -1 을 기입하면 남은 차원 수를 알아서 기입해줌 -1 = ? 라고 생각 | . tf.reshape(a,(4,-1)) . &lt;tf.Tensor: shape=(4, 3), dtype=int32, numpy= array([[ 1, 2, 3], [ 4, 5, 6], [ 7, 8, 9], [10, 11, 12]], dtype=int32)&gt; . b= tf.reshape(a,(2,2,-1)) b . &lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy= array([[[ 1, 2, 3], [ 4, 5, 6]], [[ 7, 8, 9], [10, 11, 12]]], dtype=int32)&gt; . - 다시 일차원으로 되돌림 . tf.reshape(b,-1) . &lt;tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], dtype=int32)&gt; . &#49440;&#50616;&#44256;&#44553; . - 리스트나, 넘파이로 만들고 output을 tensor로 변경하는 것도 좋은 방법이다. . ㅣ = [1,2,3,4] tf.constant(np.diag(ㅣ)) . &lt;tf.Tensor: shape=(4, 4), dtype=int64, numpy= array([[1, 0, 0, 0], [0, 2, 0, 0], [0, 0, 3, 0], [0, 0, 0, 4]])&gt; . - tf.ones, tf.zeros . tf.zeros([3,3]) . &lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy= array([[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]], dtype=float32)&gt; . - tf.linspace(0,1,10) . tf.linspace(0,1,10) . &lt;tf.Tensor: shape=(10,), dtype=float64, numpy= array([0. , 0.11111111, 0.22222222, 0.33333333, 0.44444444, 0.55555556, 0.66666667, 0.77777778, 0.88888889, 1. ])&gt; . tf.concat . a = tf.constant([1,2]) b = tf.constant([3,4]) a,b . (&lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)&gt;, &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([3, 4], dtype=int32)&gt;) . a = tf.constant([[1],[2]]) b = tf.constant([[3],[4]]) a,b . (&lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy= array([[1], [2]], dtype=int32)&gt;, &lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy= array([[3], [4]], dtype=int32)&gt;) . tf.concat([a,b],axis=0) . &lt;tf.Tensor: shape=(4, 1), dtype=int32, numpy= array([[1], [2], [3], [4]], dtype=int32)&gt; . a = tf.constant([[1],[2]]) b = tf.constant([[3],[4]]) a,b . (&lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy= array([[1], [2]], dtype=int32)&gt;, &lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy= array([[3], [4]], dtype=int32)&gt;) . tf.concat([a,b],axis=1) . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 3], [2, 4]], dtype=int32)&gt; . a = tf.constant([1,2]) b = tf.constant([3,4]) a,b tf.concat([a,b],axis=0) . &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt; . a = tf.constant([[1,2]]) b = tf.constant([[3,4]]) a,b tf.concat([a,b],axis=0) . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 2], [3, 4]], dtype=int32)&gt; . &#52264;&#50896; &#49688; &#51613;&#44032; . (2,3,4,5) concat (2,3,4,5) =&gt; (4,3,4,5) | . a=tf.reshape(tf.constant(range(120)),(2,3,4,5)) b= -a . tf.concat([a,b],axis=0) . &lt;tf.Tensor: shape=(4, 3, 4, 5), dtype=int32, numpy= array([[[[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [ 10, 11, 12, 13, 14], [ 15, 16, 17, 18, 19]], [[ 20, 21, 22, 23, 24], [ 25, 26, 27, 28, 29], [ 30, 31, 32, 33, 34], [ 35, 36, 37, 38, 39]], [[ 40, 41, 42, 43, 44], [ 45, 46, 47, 48, 49], [ 50, 51, 52, 53, 54], [ 55, 56, 57, 58, 59]]], [[[ 60, 61, 62, 63, 64], [ 65, 66, 67, 68, 69], [ 70, 71, 72, 73, 74], [ 75, 76, 77, 78, 79]], [[ 80, 81, 82, 83, 84], [ 85, 86, 87, 88, 89], [ 90, 91, 92, 93, 94], [ 95, 96, 97, 98, 99]], [[ 100, 101, 102, 103, 104], [ 105, 106, 107, 108, 109], [ 110, 111, 112, 113, 114], [ 115, 116, 117, 118, 119]]], [[[ 0, -1, -2, -3, -4], [ -5, -6, -7, -8, -9], [ -10, -11, -12, -13, -14], [ -15, -16, -17, -18, -19]], [[ -20, -21, -22, -23, -24], [ -25, -26, -27, -28, -29], [ -30, -31, -32, -33, -34], [ -35, -36, -37, -38, -39]], [[ -40, -41, -42, -43, -44], [ -45, -46, -47, -48, -49], [ -50, -51, -52, -53, -54], [ -55, -56, -57, -58, -59]]], [[[ -60, -61, -62, -63, -64], [ -65, -66, -67, -68, -69], [ -70, -71, -72, -73, -74], [ -75, -76, -77, -78, -79]], [[ -80, -81, -82, -83, -84], [ -85, -86, -87, -88, -89], [ -90, -91, -92, -93, -94], [ -95, -96, -97, -98, -99]], [[-100, -101, -102, -103, -104], [-105, -106, -107, -108, -109], [-110, -111, -112, -113, -114], [-115, -116, -117, -118, -119]]]], dtype=int32)&gt; . (2,3,4,5) concat (2,3,4,5) =&gt; (2,6,4,5) | . tf.concat([a,b],axis=1) . &lt;tf.Tensor: shape=(2, 6, 4, 5), dtype=int32, numpy= array([[[[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [ 10, 11, 12, 13, 14], [ 15, 16, 17, 18, 19]], [[ 20, 21, 22, 23, 24], [ 25, 26, 27, 28, 29], [ 30, 31, 32, 33, 34], [ 35, 36, 37, 38, 39]], [[ 40, 41, 42, 43, 44], [ 45, 46, 47, 48, 49], [ 50, 51, 52, 53, 54], [ 55, 56, 57, 58, 59]], [[ 0, -1, -2, -3, -4], [ -5, -6, -7, -8, -9], [ -10, -11, -12, -13, -14], [ -15, -16, -17, -18, -19]], [[ -20, -21, -22, -23, -24], [ -25, -26, -27, -28, -29], [ -30, -31, -32, -33, -34], [ -35, -36, -37, -38, -39]], [[ -40, -41, -42, -43, -44], [ -45, -46, -47, -48, -49], [ -50, -51, -52, -53, -54], [ -55, -56, -57, -58, -59]]], [[[ 60, 61, 62, 63, 64], [ 65, 66, 67, 68, 69], [ 70, 71, 72, 73, 74], [ 75, 76, 77, 78, 79]], [[ 80, 81, 82, 83, 84], [ 85, 86, 87, 88, 89], [ 90, 91, 92, 93, 94], [ 95, 96, 97, 98, 99]], [[ 100, 101, 102, 103, 104], [ 105, 106, 107, 108, 109], [ 110, 111, 112, 113, 114], [ 115, 116, 117, 118, 119]], [[ -60, -61, -62, -63, -64], [ -65, -66, -67, -68, -69], [ -70, -71, -72, -73, -74], [ -75, -76, -77, -78, -79]], [[ -80, -81, -82, -83, -84], [ -85, -86, -87, -88, -89], [ -90, -91, -92, -93, -94], [ -95, -96, -97, -98, -99]], [[-100, -101, -102, -103, -104], [-105, -106, -107, -108, -109], [-110, -111, -112, -113, -114], [-115, -116, -117, -118, -119]]]], dtype=int32)&gt; . (2,3,4,5) concat (2,3,4,5) =&gt; (2,3,8,5) | . tf.concat([a,b],axis=2) . &lt;tf.Tensor: shape=(2, 3, 8, 5), dtype=int32, numpy= array([[[[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [ 10, 11, 12, 13, 14], [ 15, 16, 17, 18, 19], [ 0, -1, -2, -3, -4], [ -5, -6, -7, -8, -9], [ -10, -11, -12, -13, -14], [ -15, -16, -17, -18, -19]], [[ 20, 21, 22, 23, 24], [ 25, 26, 27, 28, 29], [ 30, 31, 32, 33, 34], [ 35, 36, 37, 38, 39], [ -20, -21, -22, -23, -24], [ -25, -26, -27, -28, -29], [ -30, -31, -32, -33, -34], [ -35, -36, -37, -38, -39]], [[ 40, 41, 42, 43, 44], [ 45, 46, 47, 48, 49], [ 50, 51, 52, 53, 54], [ 55, 56, 57, 58, 59], [ -40, -41, -42, -43, -44], [ -45, -46, -47, -48, -49], [ -50, -51, -52, -53, -54], [ -55, -56, -57, -58, -59]]], [[[ 60, 61, 62, 63, 64], [ 65, 66, 67, 68, 69], [ 70, 71, 72, 73, 74], [ 75, 76, 77, 78, 79], [ -60, -61, -62, -63, -64], [ -65, -66, -67, -68, -69], [ -70, -71, -72, -73, -74], [ -75, -76, -77, -78, -79]], [[ 80, 81, 82, 83, 84], [ 85, 86, 87, 88, 89], [ 90, 91, 92, 93, 94], [ 95, 96, 97, 98, 99], [ -80, -81, -82, -83, -84], [ -85, -86, -87, -88, -89], [ -90, -91, -92, -93, -94], [ -95, -96, -97, -98, -99]], [[ 100, 101, 102, 103, 104], [ 105, 106, 107, 108, 109], [ 110, 111, 112, 113, 114], [ 115, 116, 117, 118, 119], [-100, -101, -102, -103, -104], [-105, -106, -107, -108, -109], [-110, -111, -112, -113, -114], [-115, -116, -117, -118, -119]]]], dtype=int32)&gt; . (2,3,4,5) concat (2,3,4,5) =&gt; (2,3,4,10) | . tf.concat([a,b],axis=3) . &lt;tf.Tensor: shape=(2, 3, 4, 10), dtype=int32, numpy= array([[[[ 0, 1, 2, 3, 4, 0, -1, -2, -3, -4], [ 5, 6, 7, 8, 9, -5, -6, -7, -8, -9], [ 10, 11, 12, 13, 14, -10, -11, -12, -13, -14], [ 15, 16, 17, 18, 19, -15, -16, -17, -18, -19]], [[ 20, 21, 22, 23, 24, -20, -21, -22, -23, -24], [ 25, 26, 27, 28, 29, -25, -26, -27, -28, -29], [ 30, 31, 32, 33, 34, -30, -31, -32, -33, -34], [ 35, 36, 37, 38, 39, -35, -36, -37, -38, -39]], [[ 40, 41, 42, 43, 44, -40, -41, -42, -43, -44], [ 45, 46, 47, 48, 49, -45, -46, -47, -48, -49], [ 50, 51, 52, 53, 54, -50, -51, -52, -53, -54], [ 55, 56, 57, 58, 59, -55, -56, -57, -58, -59]]], [[[ 60, 61, 62, 63, 64, -60, -61, -62, -63, -64], [ 65, 66, 67, 68, 69, -65, -66, -67, -68, -69], [ 70, 71, 72, 73, 74, -70, -71, -72, -73, -74], [ 75, 76, 77, 78, 79, -75, -76, -77, -78, -79]], [[ 80, 81, 82, 83, 84, -80, -81, -82, -83, -84], [ 85, 86, 87, 88, 89, -85, -86, -87, -88, -89], [ 90, 91, 92, 93, 94, -90, -91, -92, -93, -94], [ 95, 96, 97, 98, 99, -95, -96, -97, -98, -99]], [[ 100, 101, 102, 103, 104, -100, -101, -102, -103, -104], [ 105, 106, 107, 108, 109, -105, -106, -107, -108, -109], [ 110, 111, 112, 113, 114, -110, -111, -112, -113, -114], [ 115, 116, 117, 118, 119, -115, -116, -117, -118, -119]]]], dtype=int32)&gt; . 아래와 같은 방법도 있긴하나 난 안할래 | . tf.concat([a,b],axis=-1) . &lt;tf.Tensor: shape=(2, 3, 4, 10), dtype=int32, numpy= array([[[[ 0, 1, 2, 3, 4, 0, -1, -2, -3, -4], [ 5, 6, 7, 8, 9, -5, -6, -7, -8, -9], [ 10, 11, 12, 13, 14, -10, -11, -12, -13, -14], [ 15, 16, 17, 18, 19, -15, -16, -17, -18, -19]], [[ 20, 21, 22, 23, 24, -20, -21, -22, -23, -24], [ 25, 26, 27, 28, 29, -25, -26, -27, -28, -29], [ 30, 31, 32, 33, 34, -30, -31, -32, -33, -34], [ 35, 36, 37, 38, 39, -35, -36, -37, -38, -39]], [[ 40, 41, 42, 43, 44, -40, -41, -42, -43, -44], [ 45, 46, 47, 48, 49, -45, -46, -47, -48, -49], [ 50, 51, 52, 53, 54, -50, -51, -52, -53, -54], [ 55, 56, 57, 58, 59, -55, -56, -57, -58, -59]]], [[[ 60, 61, 62, 63, 64, -60, -61, -62, -63, -64], [ 65, 66, 67, 68, 69, -65, -66, -67, -68, -69], [ 70, 71, 72, 73, 74, -70, -71, -72, -73, -74], [ 75, 76, 77, 78, 79, -75, -76, -77, -78, -79]], [[ 80, 81, 82, 83, 84, -80, -81, -82, -83, -84], [ 85, 86, 87, 88, 89, -85, -86, -87, -88, -89], [ 90, 91, 92, 93, 94, -90, -91, -92, -93, -94], [ 95, 96, 97, 98, 99, -95, -96, -97, -98, -99]], [[ 100, 101, 102, 103, 104, -100, -101, -102, -103, -104], [ 105, 106, 107, 108, 109, -105, -106, -107, -108, -109], [ 110, 111, 112, 113, 114, -110, -111, -112, -113, -114], [ 115, 116, 117, 118, 119, -115, -116, -117, -118, -119]]]], dtype=int32)&gt; . &#52264;&#50896;&#51012; &#54620;&#48264; &#51460;&#50668;&#48372;&#51088; . (4,) -&gt; (8,) | . a=tf.constant([1,2,3,4]) b=-a a,b . (&lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt;, &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([-1, -2, -3, -4], dtype=int32)&gt;) . tf.concat([a,b],axis=0) . &lt;tf.Tensor: shape=(8,), dtype=int32, numpy=array([ 1, 2, 3, 4, -1, -2, -3, -4], dtype=int32)&gt; . (4,) -&gt; (4,2) | . - 에러가 뜬다 . tf.concat([a,b],axis=1) . InvalidArgumentError Traceback (most recent call last) &lt;ipython-input-145-6f52ef50c654&gt; in &lt;module&gt;() -&gt; 1 tf.concat([a,b],axis=1) /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py in error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name) 7184 def raise_from_not_ok_status(e, name): 7185 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7186 raise core._status_to_exception(e) from None # pylint: disable=protected-access 7187 7188 InvalidArgumentError: ConcatOp : Expected concatenating dimensions in the range [-1, 1), but got 1 [Op:ConcatV2] name: concat . tf.stack . (4,) -&gt; (4,2) | . a=tf.constant([1,2,3,4]) b=-a a,b . (&lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt;, &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([-1, -2, -3, -4], dtype=int32)&gt;) . tf.stack([a,b],axis=1) . &lt;tf.Tensor: shape=(4, 2), dtype=int32, numpy= array([[ 1, -1], [ 2, -2], [ 3, -3], [ 4, -4]], dtype=int32)&gt; . tf.einsum . tnp . - 텐서만들기가 너무 힘듬 . import tensorflow.experimental.numpy as tnp tnp.experimental_enable_numpy_behavior() ## tnf 를 numpy 처럼 사용할 수 있도록 해줌 . tnp &#49324;&#50857;&#48169;&#48277; (&#48520;&#47564;&#54644;&#44208;&#48169;&#48277;) . - int 와 float 을 더할 수 있음 . tnp.array([1,2,3]) + tnp.array([1.0,2.0,3.0]) . &lt;tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 4., 6.])&gt; . 심지어 | . tnp.array(1) + tnp.array([1.0,2.0,3.0]) . &lt;tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 3., 4.])&gt; . tnp.array([1,2,3]) + tf.constant([1.0,2.0,3.0]) . &lt;tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 4., 6.])&gt; . a = tnp.diag([1,2,3]) type(a) . tensorflow.python.framework.ops.EagerTensor . a.min(),a.max() . (&lt;tf.Tensor: shape=(), dtype=int64, numpy=0&gt;, &lt;tf.Tensor: shape=(), dtype=int64, numpy=3&gt;) . a.reshape(9,1) . &lt;tf.Tensor: shape=(9, 1), dtype=int64, numpy= array([[1], [0], [0], [0], [2], [0], [0], [0], [3]])&gt; . &#49440;&#50616;, &#49440;&#50616;&#44256;&#44553; . np.random.randn(5) . array([-1.79271696, -0.17190837, 1.01536417, 0.10096996, 0.6384037 ]) . tnp.random.randn(5) . &lt;tf.Tensor: shape=(5,), dtype=float64, numpy=array([ 0.68371875, -0.77886642, -0.78283853, -1.91862598, -0.36602414])&gt; . &#53440;&#51077; . type(tnp.random.randn(5)) . tensorflow.python.framework.ops.EagerTensor . tf.contant&#47196; &#47564;&#46308;&#50612;&#46020; &#47560;&#52824; &#45336;&#54028;&#51060;&#51064;&#46319; &#50416;&#45716; &#44592;&#45733;&#46308; . - 묵시적 형변환이 가능해짐 . - 메소드를 쓸 수 있음. . &#44536;&#47111;&#51648;&#47564; np.array&#45716; &#50500;&#45784; . 여전히 값을 바꾸는 것은 허용하지 않는다. | . a = tf.constant([1,2,3]) . a[0]=11 . TypeError Traceback (most recent call last) &lt;ipython-input-181-b909b2ec59d1&gt; in &lt;module&gt;() -&gt; 1 a[0]=11 TypeError: &#39;tensorflow.python.framework.ops.EagerTensor&#39; object does not support item assignment . tf.Variable . &#49440;&#50616; . &#53440;&#51077; . &#51064;&#45937;&#49905; . tf.Variable $ to$ &#45336;&#54028;&#51060; . tf.Variable &#46020; &#48520;&#54200;&#54616;&#45796;. . &#50672;&#49328; . &#54805;&#53468;&#48320;&#54872; . &#49440;&#50616;&#44256;&#44553; . tf.concat . tf.stack . &#49900;&#51648;&#50612; tf.Variable()&#47196; &#47564;&#46308;&#50612;&#51652; &#50724;&#48652;&#51229;&#53944;&#45716; tnp&#51032; &#54952;&#44284;(&#51008;&#52509;)&#46020; &#48155;&#51648; &#47803;&#54632; .",
            "url": "https://gangcheol.github.io/big-data-analysis/python/2022/03/14/(2%EC%A3%BC%EC%B0%A8).html",
            "relUrl": "/python/2022/03/14/(2%EC%A3%BC%EC%B0%A8).html",
            "date": " • Mar 14, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "(1주차) 단순선형회귀",
            "content": "&#47196;&#46300;&#47605; . - 오늘수업할내용: 단순선형회귀 . - 단순선형회귀를 배우는 이유? . 우리가 배우고싶은것: 심층신경망(DNN) $ to$ 합성곱신경망(CNN) $ to$ 적대적생성신경망(GAN) | 심층신경망을 바로 이해하기 어려움 | 다음의 과정으로 이해해야함: 선형대수학 $ to$ 회귀분석 $ to$ 로지스틱회귀분석 $ to$ 심층신경망 | . &#49440;&#54805;&#54924;&#44480; . - 상황극 . 날이 더울수록 아이스아메리카노의 판매량이 증가함 | 이를 바탕으로 일기예보의 온도자료를 이용하여 다음과 같은 수식을 이용해 아이스아메리카노의 판매량을 예측할 수 있음 | . $$아이스아메리카노 = beta_1 times 온도 + epsilon$$ . - 가짜자료 생성 . import matplotlib.pyplot as plt import tensorflow as tf . . 온도 ${ bf x}$가 아래와 같다고 하자.(tf.constant 함수를 이용하여 상수 텐서를 생성) . x=tf.constant([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4]) # 기온 x . . &lt;tf.Tensor: shape=(10,), dtype=float32, numpy= array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4], dtype=float32)&gt; . 아이스아메리카노의 판매량 ${ bf y}$이 아래와 같다고 하자. (판매량은 정수로 나오겠지만 편의상 소수점도 가능하다고 생각하자) . $${ bf y} approx 10.2 +2.2 { bf x}$$ . 여기에서 10.2, 2.2 의 숫자는 임의로 정한 $ beta_0, , beta_1$ | 식의의미: 온도가 0일때 10.2잔정도 팔림 + 온도가 1도 증가하면 2.2잔정도 더 팔림 | . tf.random.set_seed(43052) epsilon=tf.random.normal([10]) ## 오차항 생성 y=10.2 + 2.2*x + epsilon y . . &lt;tf.Tensor: shape=(10,), dtype=float32, numpy= array([55.418365, 58.194283, 61.230827, 62.312557, 63.107002, 63.69569 , 67.247055, 71.4365 , 73.1013 , 77.84988 ], dtype=float32)&gt; . - 우리는 아래와 같은 자료를 모았다고 생각하자. . - tensorflow 문법에 관한 내용은 이후 수업에서 다루니 크게 신경쓰지 말자 . tf.transpose(tf.concat([[x],[y]],0)) . . &lt;tf.Tensor: shape=(10, 2), dtype=float32, numpy= array([[20.1 , 55.418365], [22.2 , 58.194283], [22.7 , 61.230827], [23.3 , 62.312557], [24.4 , 63.107002], [25.1 , 63.69569 ], [26.2 , 67.247055], [27.3 , 71.4365 ], [28.4 , 73.1013 ], [30.4 , 77.84988 ]], dtype=float32)&gt; . - 그려보자. . plt.figure(figsize=(12,6)) plt.plot(x,y,&#39;.&#39;) # 파란점, 관측한 데이터 plt.plot(x,10.2 + 2.2*x, &#39;--&#39;) # 주황색점선, 세상의 법칙 . . [&lt;matplotlib.lines.Line2D at 0x2e12f41df70&gt;] . - 우리의 목표: 파란색점(관측값)에 기반하여 온도에 따른 아이스크림 판매량에 대한 일반화 식을 만드는 것 . - 아이디어: 데이터를 보니까 $x$와 $y$가 선형의 관계에 있는듯 보인다. 즉 모든 $i=1,2, dots, 10$에 대하여 아래를 만족하는 적당한 $ beta_0, beta_1$가 존재할것 같다. . $$y_{i} approx beta_1 x_{i}+ beta_0$$ . - 어림짐작으로 $ beta_0, beta_1$를 알아내보자. . 데이터를 살펴보자. . tf.transpose(tf.concat([[x],[y]],0)) . . &lt;tf.Tensor: shape=(10, 2), dtype=float32, numpy= array([[20.1 , 55.418365], [22.2 , 58.194283], [22.7 , 61.230827], [23.3 , 62.312557], [24.4 , 63.107002], [25.1 , 63.69569 ], [26.2 , 67.247055], [27.3 , 71.4365 ], [28.4 , 73.1013 ], [30.4 , 77.84988 ]], dtype=float32)&gt; . 적당히 왼쪽*2+15 = 오른쪽의 관계가 성립하는것 같다. . 따라서 $ beta_0=15, beta_1=2$ 로 추론할 수 있겠다. . - 누군가가 $( beta_0, beta_1)=(14,2)$ 이라고 주장할 수 있다. (어차피 지금은 감각으로 추론하는 과정이니까) . - 새로운 주장으로 인해서 $( beta_0, beta_1)=(15,2)$ 로 볼 수도 있고 $( beta_0, beta_1)=(14,2)$ 로 볼 수도 있다. 이중에서 어떠한 추정치가 좋은지 판단할 수 있을까? . 후보1: $( beta_0, beta_1)=(15,2)$ | 후보2: $( beta_0, beta_1)=(14,2)$ | . - 가능한 $y_i approx beta_0 + beta_1 x_i$ 이 되도록 만드는 $( beta_0, beta_1)$ 이 좋을 것이다. $ to$ 후보 1,2를 비교해보자. . (관찰에 의한 비교) . 후보1에 대해서 $i=1,2$를 넣고 관찰하여 보자. . 20.1 * 2 + 15 , 55.418365 # i=1 . (55.2, 55.418365) . 22.2 * 2 + 15 , 58.194283 # i=2 . (59.4, 58.194283) . 후보2에 대하여 $i=1,2$를 넣고 관찰하여 보자. . 20.1 * 2 + 14 , 55.418365 # i=1 . (54.2, 55.418365) . 22.2 * 2 + 14 , 58.194283 # i=2 . (58.4, 58.194283) . $i=1$인 경우에는 후보1이 더 잘맞는것 같은데 $i=2$인 경우는 후보2가 더 잘맞는것 같다. . (좀 더 체계적인 비교) . $i=1,2,3, dots, 10$ 에서 후보1과 후보2중 어떤것이 더 좋은지 비교하는 체계적인 방법을 생각해보자. . 후보 1,2에 대하여 $ sum_{i=1}^{10} (y_i - beta_0 - beta_1 x_i)^2$를 계산하여 비교해보자. . sum1=0 for i in range(10): sum1=sum1+(y[i]-15-2*x[i])**2 . sum2=0 for i in range(10): sum2=sum2+(y[i]-14-2*x[i])**2 . sum1,sum2 . (&lt;tf.Tensor: shape=(), dtype=float32, numpy=14.734169&gt;, &lt;tf.Tensor: shape=(), dtype=float32, numpy=31.521088&gt;) . 후보1이 더 $ sum_{i=1}^{10} (y_i - beta_0 - beta_1 x_i)^2$의 값이 작다. . 후보1이 종합적으로 후보2에 비하여 좋다. 이 과정을 무한번 반복하면 최적의 추정치를 찾을 수 있다. . - 그런데 이 알고리즘은 현실적으로 구현이 불가능하다. (무한번 계산하기도 힘들고, 언제 멈출지도 애매함) . - 수학을 이용해서 좀 더 체계적으로 찾아보자. 결국 아래식을 가장 작게 만드는 $ beta_0, beta_1$을 찾으면 된다. . $$ sum_{i=1}^{10} (y_i - beta_0 - beta_1 x_i)^2$$ . 그런데 결국 $ beta_0, beta_1$에 대한 이차식인데 이 식을 최소화하는 $ beta_0, beta_1$을 구하기 위해서는 아래를 연립하여 풀면된다. . $$ begin{cases} frac{ partial}{ partial beta_0} sum_{i=1}^{10} (y_i - beta_0 - beta_1 x_i)^2=0 frac{ partial}{ partial beta_1} sum_{i=1}^{10} (y_i - beta_0 - beta_1 x_i)^2=0 end{cases}$$ - 풀어보자. . $$ begin{cases} sum_{i=1}^{10} -2(y_i - beta_0 - beta_1 x_i)=0 sum_{i=1}^{10} -2x_i(y_i - beta_0 - beta_1 x_i)=0 end{cases}$$ 정리하면 . $$ hat{ beta}_0= bar{y}- hat{ beta}_1 bar{x}$$ . $$ hat{ beta}_1= frac{S_{xy}}{S_{xx}}= frac{ sum_{i=1}^{n}(x_i- bar{x})(y_i- bar{y})}{ sum_{i=1}^{n}(x_i- bar{x})^2}$$ . - 따라서 최적의 추정치 $( hat{ beta}_0, hat{ beta}_1)$를 이용한 추세선을 아래와 같이 계산할 수 있음. . Sxx= sum((x-sum(x)/10)**2) Sxx . &lt;tf.Tensor: shape=(), dtype=float32, numpy=87.848976&gt; . Sxy= sum((x-sum(x)/10)*(y-sum(y)/10)) Sxy . &lt;tf.Tensor: shape=(), dtype=float32, numpy=194.64737&gt; . beta1_estimated = Sxy/Sxx beta1_estimated . &lt;tf.Tensor: shape=(), dtype=float32, numpy=2.2157044&gt; . beta0_estimated = sum(y)/10 - beta1_estimated * sum(x)/10 beta0_estimated . &lt;tf.Tensor: shape=(), dtype=float32, numpy=9.944572&gt; . plt.figure(figsize=(12,6)) plt.plot(x,y,&#39;.&#39;,label= &quot;observed value&quot;) plt.plot(x,beta0_estimated + beta1_estimated * x, &#39;--&#39;,label=&quot;hat y&quot;) # 주황색선: 세상의 법칙을 추정한선 plt.plot(x,10.2 + 2.2* x, &#39;--&#39;,label = &quot;rule of the world&quot;) # 초록색선: ture, 세상의법칙 plt.legend() . . &lt;matplotlib.legend.Legend at 0x2e12f3afd90&gt; . . Note: 샘플수가 커질수록 주황색선은 점점 초록색선으로 가까워진다. (대수의 법칙은 항상 성립하는 듯?) . - 꽤 훌륭한 도구임. 그런데 약간의 단점이 존재한다. . (1) 공식이 좀 복잡함.. . (2) $x$가 여러개일 경우 확장이 어려움 . - 단점을 극복하기 위해서 우리가 지금까지 했던논의를 매트릭스로 바꾸어서 다시 써보자. . &#47784;&#54805;&#51032; &#47588;&#53944;&#47533;&#49828;&#54868; . 모형을 행렬로 표현하면 변수가 여러개인 multiple linear regression 에서도 단순형태로 표현이 가능하다. | . 우리의 모형은 아래와 같다. . $y_i = beta_0 + beta_1 x_i + epsilon_i, quad i=1,2, dots,10$ . 풀어서 쓰면 . $ begin{cases} y_1 = beta_0 + beta_1 x_1 + epsilon_1 y_2 = beta_0 + beta_1 x_2 + epsilon_2 dots y_{10} = beta_0 + beta_1 x_{10} + epsilon_{10} end{cases}$ . 아래와 같이 쓸 수 있다. . $ begin{bmatrix} y_1 y_2 dots y_{10} end{bmatrix} = begin{bmatrix} 1 &amp; x_1 1 &amp; x_2 dots &amp; dots 1 &amp; x_{10} end{bmatrix} begin{bmatrix} beta_0 beta_1 end{bmatrix} + begin{bmatrix} epsilon_1 epsilon_2 dots epsilon_{10} end{bmatrix} $ . 벡터와 매트릭스 형태로 정리하면 . ${ bf y} = { bf X} { boldsymbol beta} + boldsymbol{ epsilon}$ . - 손실함수의 매트릭스화: 우리가 최소화 하려던 손실함수는 아래와 같다. . $loss= sum_{i=1}^{n}(y_i- beta_0- beta_1x_i)^2$ . 이것을 벡터표현으로 하면 아래와 같다. . $loss= sum_{i=1}^{n}(y_i- beta_0- beta_1x_i)^2=({ bf y}-{ bf X}{ boldsymbol beta})^ top({ bf y}-{ bf X}{ boldsymbol beta})$ . 풀어보면 . $loss=({ bf y}-{ bf X}{ boldsymbol beta})^ top({ bf y}-{ bf X}{ boldsymbol beta})={ bf y}^ top { bf y} - { bf y}^ top { bf X}{ boldsymbol beta} - { boldsymbol beta}^ top { bf X}^ top { bf y} + { boldsymbol beta}^ top { bf X}^ top { bf X} { boldsymbol beta}$ . - 미분하는 과정의 매트릭스화 . loss를 최소화하는 ${ boldsymbol beta}$를 구해야하므로 loss를 ${ boldsymbol beta}$로 미분한식을 0이라고 놓고 풀면 된다. . $ frac{ partial}{ partial boldsymbol{ beta}} loss = frac{ partial}{ partial boldsymbol{ beta}} { bf y}^ top { bf y} - frac{ partial}{ partial boldsymbol{ beta}} { bf y}^ top { bf X}{ boldsymbol beta} - frac{ partial}{ partial boldsymbol{ beta}} { boldsymbol beta}^ top { bf X}^ top { bf y} + frac{ partial}{ partial boldsymbol{ beta}} { boldsymbol beta}^ top { bf X}^ top { bf X} { boldsymbol beta}$ . $= 0 - { bf X}^ top { bf y}- { bf X}^ top { bf y} + 2{ bf X}^ top { bf X}{ boldsymbol beta} $ . 따라서 $ frac{ partial}{ partial boldsymbol{ beta}}loss=0$을 풀면 아래와 같다. . $ boldsymbol{ hat beta}= ({ bf X}^ top { bf X})^{-1}{ bf X}^ top { bf y} $ . - 공식도 매트릭스로 표현하면 : $ left( boldsymbol{ hat beta}= ({ bf X}^ top { bf X})^{-1}{ bf X}^ top { bf y} right) leftarrow$ 외우자 이건.. . - 적용을 해보자. . (X를 만드는 방법1) . X=tf.transpose(tf.concat([[[1.0]*10],[x]],0)) # X . &lt;tf.Tensor: shape=(10, 2), dtype=float32, numpy= array([[ 1. , 20.1], [ 1. , 22.2], [ 1. , 22.7], [ 1. , 23.3], [ 1. , 24.4], [ 1. , 25.1], [ 1. , 26.2], [ 1. , 27.3], [ 1. , 28.4], [ 1. , 30.4]], dtype=float32)&gt; . (X를 만드는 방법2) . tf.concat([[[1.0]*10],[x]],0) . &lt;tf.Tensor: shape=(2, 10), dtype=float32, numpy= array([[ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ], [20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4]], dtype=float32)&gt; . tf.concat([[[1.0]*10],[x]],0).T . AttributeError Traceback (most recent call last) ~ AppData Local Temp/ipykernel_45688/2773875106.py in &lt;module&gt; -&gt; 1 tf.concat([[[1.0]*10],[x]],0).T ~ Anaconda3 envs r4-base lib site-packages tensorflow python framework ops.py in __getattr__(self, name) 506 &#34;tolist&#34;, &#34;data&#34;}: 507 # TODO(wangpeng): Export the enable_numpy_behavior knob --&gt; 508 raise AttributeError(&#34;&#34;&#34; 509 &#39;{}&#39; object has no attribute &#39;{}&#39;. 510 If you are looking for numpy-related methods, please run the following: AttributeError: &#39;EagerTensor&#39; object has no attribute &#39;T&#39;. If you are looking for numpy-related methods, please run the following: from tensorflow.python.ops.numpy_ops import np_config np_config.enable_numpy_behavior() . 위 처럼 하면 error가 남 | error를 읽어보면 numpy 스타일로 구성하고 싶을 경우 np_config.enable_numpy_behavior() 을 이용하라는 문구가 나옴 | . from tensorflow.python.ops.numpy_ops import np_config np_config.enable_numpy_behavior() . X = tf.concat([[[1.0]*10],[x]],0).T . 오 이제된다. . X . &lt;tf.Tensor: shape=(10, 2), dtype=float32, numpy= array([[ 1. , 20.1], [ 1. , 22.2], [ 1. , 22.7], [ 1. , 23.3], [ 1. , 24.4], [ 1. , 25.1], [ 1. , 26.2], [ 1. , 27.3], [ 1. , 28.4], [ 1. , 30.4]], dtype=float32)&gt; . tf.linalg.inv(X.T @ X) @ X.T @ y . &lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([9.945015 , 2.2156935], dtype=float32)&gt; . 결과를 보면 $ left( beta_0, beta_1 right) = (9.94...,2.21...)$ 로 산출되었다. . - 잘 구해진다. . - 그런데.. . beta0_estimated,beta1_estimated . (&lt;tf.Tensor: shape=(), dtype=float32, numpy=9.94458&gt;, &lt;tf.Tensor: shape=(), dtype=float32, numpy=2.2157042&gt;) . 값이 좀 다르다..? . - 같은 값입니다! 신경쓰지 마세요! 텐서플로우가 좀 대충계산합니다. . - 실제로 조금 더 정확히 계산하기 위해서는 tensorflow 안에 내장된 numpy 를 사용한다. . import tensorflow.experimental.numpy as tnp . x=tnp.array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4]) y=10.2 + 2.2*x + epsilon . beta1_estimated = sum((x-sum(x)/10)*(y-sum(y)/10)) / sum((x-sum(x)/10)**2) beta0_estimated = sum(y)/10 - beta1_estimated * sum(x)/10 . beta0_estimated, beta1_estimated . (&lt;tf.Tensor: shape=(), dtype=float64, numpy=9.944573294798559&gt;, &lt;tf.Tensor: shape=(), dtype=float64, numpy=2.2157046054834106&gt;) . X=tnp.concatenate([[tnp.array([1.0]*10)],[x]],0).T tf.linalg.inv(X.T @ X) @ X.T @ y . &lt;tf.Tensor: shape=(2,), dtype=float64, numpy=array([9.94457329, 2.21570461])&gt; . do next . - 선형대수학의 미분이론.. . - tensorflow에서 매트릭스 연산을 자유롭게 다루기 . - 정규방정식을 이용하여 베타계수 추정하는 법하고 벡터 미분 정리해서 추가하자 . Extra . &#54924;&#44480;&#44228;&#49688; &#52628;&#51221; . $$Loss = sum (y- beta_0- beta_1 x)$$ . $ beta_0$ 추정 | . $$L=Loss = sum (y- beta_0- beta_1 x)$$ . $$ frac {d L}{d beta_0} = -2 sum(y- beta_0- beta_1x) = 0$$ . $$ therefore , , hat beta_0 = bar y - beta_1 bar x$$ . $ beta_1$ 추정 | . $$ begin{aligned} frac {d L}{d beta_1} &amp;= sum x left(y- beta_0- beta_1x right) nonumber &amp; = left ( sum xy - beta_1 x^2 right )- n bar x left ( bar y - beta_1 bar x right) nonumber &amp;= left ( sum xy - bar x bar y right ) - beta_1 left ( sum x^2 - ( bar x)^2 right ) nonumber end{aligned}$$ $$ begin{aligned} therefore , , hat beta_1 &amp;= , , frac{ sum xy - bar x bar y}{ sum x^2 - ( bar x)^2} nonumber &amp;= frac {S_{xy}}{S_{xx}} nonumber end {aligned}$$ &#48289;&#53552; &#48120;&#48516; / &#47588;&#53944;&#47533;&#49828; &#48120;&#48516; . $$L=loss=({ bf y}-{ bf X}{ boldsymbol beta})^ top({ bf y}-{ bf X}{ boldsymbol beta})$$ . &#48289;&#53552; &#48120;&#48516; . $$ begin{aligned} x^{ top}y &amp;= begin{bmatrix}x_1 dots x_n end{bmatrix} begin{bmatrix} y_1 dots y_n end{bmatrix} nonumber &amp;= x_1y_1 + x_2y_2+ dots x_ny_n nonumber end{aligned}$$ 위를 미분하면 . $$ begin{aligned} frac {d x^{ top}y}{d x} &amp;= , begin{bmatrix} frac {d}{d x_1} dots frac {d}{d x_n} end{bmatrix} ( x_1y_1 + x_2y_2+ dots x_ny_n) nonumber &amp;= , begin{bmatrix} y_1 dots y_n end{bmatrix}=y nonumber end{aligned}$$ &#48289;&#53552; &#48120;&#48516;&#51032; &#45796;&#47480;&#54400;&#51060; . (1) . $$ frac {d x^{ top}y}{d x} = left ( frac {d x^{ top}}{d x} right) y=y$$ . $$ begin{aligned} left( frac {d x^{ top}}{d x} right) = begin{bmatrix} frac {d}{d x_1} dots frac {d}{d x_{n}} end {bmatrix} begin{bmatrix} x_1 dots x_n end{bmatrix} = begin{bmatrix} frac {d x_1}{d x_1} &amp; dots &amp; frac {d x_n}{d x_1} dots &amp; dots &amp; dots frac {d x_1}{d x_n} &amp; dots &amp; frac {d x_n}{d x_n} end{bmatrix} = mathbf{I} nonumber end{aligned}$$ $$ therefore quad frac {d x^{ top}y}{d x} = left ( frac {d x^{ top}}{d x} right) y = mathbf{I} y = y$$ . (2) . $$ frac {d y^{ top}x}{d x} = left ( frac {d y^{ top}x}{d x} right) =y $$ . $ y^{ top} x $는 $1 times 1$ 차원이므로 인간이면 이해할 수 있을 듯? . (3) . $$ frac {d}{d boldsymbol{ beta}} left ( mathbf{y^{ top}X boldsymbol beta} right ) = mathbf{X^{ top}y}$$ . $ mathbf{y^{ top}X boldsymbol beta}$ 는 $1 times 1$ 인 스칼라 이므로 . $$ mathbf{y^{ top}X boldsymbol beta} = left ( mathbf{y^{ top}X boldsymbol beta} right)^{ top} = boldsymbol beta^{ top} mathbf{X^{ top} y}$$ . 따라서 . $$ begin{aligned} frac {d}{d boldsymbol{ beta}} left ( mathbf{y^{ top}X boldsymbol beta} right ) &amp;= frac {d}{d boldsymbol{ beta}} left ( boldsymbol beta^{ top} mathbf{X^{ top} y} right ) nonumber &amp;= left ( frac{d}{d boldsymbol{ beta}} boldsymbol{ beta}^{ top} right) mathbf{X^{ top y}} nonumber &amp;= mathbf{I ,X^{ top}y} nonumber &amp;= mathbf{X^{ top}y} nonumber end{aligned}$$ (4) . $$ frac {d }{d mathbf {y}} ( mathbf{y^{ top}y}) = 2 mathbf{y}$$ . $$ d , mathbf{y} = left [ frac {d}{y_1}, , frac {d}{y_2} dots dots frac {d}{y_n} right ]$$ . $$ mathbf{y^{ top}y} = sum{y_i}^2$$ . $$ therefore quad frac {d }{d mathbf {y}} ( mathbf{y^{ top}y}) = 2 mathbf{y}$$ . &#52280;&#44256;&#54624; &#53952;&#47536;&#54400;&#51060; . 아래와 같은 풀이는 1번의 벡터 미분의 다른 풀이처럼 풀면 안된다. | . $$ frac {d }{d mathbf {y}} ( mathbf{y^{ top}y}) = mathbf{y}$$ . $$ frac {d , mathbf{y^{ top}y}}{d , mathbf{y}} = left ( frac {d , mathbf{y^{ top}y}}{d , mathbf y} right) mathbf y= mathbf{I} y neq y$$ . because 스칼라 경우를 생각해보자 . (틀린풀이 )$ quad frac {d }{d ,y} y^2 = left ( frac {d}{d ,y } yy right) = y$ ? . (올바른 풀이) $ quad frac {d }{d ,y} y^2 = left ( frac {d}{d ,y } y_1 right)+ left ( frac {d}{d ,y } y_2 right) = 2y$ . 스칼라를 예제로 들었는데 벡터에서 이런 느낌이라고 생각하자 이게 표준적으로 사용되는 설명은 아니지만 이해적? 으로는 간편한 듯 . 다른풀이 (3)번의 경우도 원래는 안되는데 값이 스칼라 이므로 틀린풀이 처럼 안되는 경우이나 1 x 1 행렬이므로 가능한 것이다. . 다시 벡터로 돌아오면 . (올바른 풀이) $ quad left ( frac {d}{d , mathbf{y}} ( mathbf{y^{ top}y}) right ) = A + B$ . $A = f( mathbf y)$ , $B = g( mathbf y)$ 라고 생각하자 . $ frac {d}{d , mathbf y} A = left ( frac {d}{d , mathbf y} mathbf {y^{ top}y} right ) = mathbf {I ,y} = mathbf {I ,y} $ . $B$ 의 경우도 위와 동일하므로 . $$ frac {d ,(A+B)}{d , mathbf{y}} = {f( mathbf {y}) }^{ prime} + {g( mathbf {y}) }^{ prime} = 2 mathbf{y}$$ . (5) . $$ frac{ partial}{ partial boldsymbol{ beta}} { boldsymbol beta}^ top { bf X}^ top { bf X} { boldsymbol beta} = 2 mathbf{X^{ top}X} boldsymbol beta$$ . 4번의 원리를 이용하면 이지이지 . loss&#47484; &#48120;&#48516; . $$L=loss=({ bf y}-{ bf X}{ boldsymbol beta})^ top({ bf y}-{ bf X}{ boldsymbol beta})$$ . $$L = { bf y}^ top { bf y} - { bf y}^ top { bf X}{ boldsymbol beta} - { boldsymbol beta}^ top { bf X}^ top { bf y} + { boldsymbol beta}^ top { bf X}^ top { bf X} { boldsymbol beta}$$ . $L$ 을 미분하면 . $$ begin{aligned} frac{ partial}{ partial boldsymbol{ beta}} L &amp;= frac{ partial}{ partial boldsymbol{ beta}} { bf y}^ top { bf y} - frac{ partial}{ partial boldsymbol{ beta}} { bf y}^ top { bf X}{ boldsymbol beta} - frac{ partial}{ partial boldsymbol{ beta}} { boldsymbol beta}^ top { bf X}^ top { bf y} + frac{ partial}{ partial boldsymbol{ beta}} { boldsymbol beta}^ top { bf X}^ top { bf X} { boldsymbol beta} nonumber &amp;= 0 - mathbf{X^{ top}y} - mathbf{X^{ top}y} + 2 mathbf{X^{ top}X boldsymbol beta} nonumber end{aligned} $$ 따라서 아래와 같은식이 성립한다. . $$ mathbf{X^{ top}y}= mathbf{X^{ top} X} boldsymbol beta $$ . $$ hat { boldsymbol beta} = mathbf{ left (X^{ top}X right)^{-1}Xy} $$ .",
            "url": "https://gangcheol.github.io/big-data-analysis/2022/03/08/(1%EC%A3%BC%EC%B0%A8).html",
            "relUrl": "/2022/03/08/(1%EC%A3%BC%EC%B0%A8).html",
            "date": " • Mar 8, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "(6주차) 과제",
            "content": "- 케라스를 이용하여 아래를 만족하는 적절한 $ beta_0$와 $ beta_1$을 구하라. 적합결과를 시각화하라. (애니메이션 시각화 X) . python . np.random.seed(43052) N= 100 x= np.linspace(-1,1,N) epsilon = np.random.randn(N)*0.5 y= 2.5+4*np.exp(-x) +epsilon . $$ hat {y} approx 2.5+ 4e^{-x} $$ . Solution . import numpy as np import matplotlib.pyplot as plt import tensorflow as tf import tensorflow.experimental.numpy as tnp . tnp.experimental_enable_numpy_behavior() . np.random.seed(43052) N= 100 x= np.linspace(-1,1,N) epsilon = np.random.randn(N)*0.5 y= 2.5+4*np.exp(-x) +epsilon . X= np.stack([np.ones(N),np.exp(-x)],axis=1) y= y.reshape(N,1) . net = tf.keras.Sequential() . net.add(tf.keras.layers.Dense(1,use_bias=False)) net.compile(tf.keras.optimizers.SGD(0.1), loss=&#39;mse&#39;) . net.fit(X,y,epochs=1000,verbose=0,batch_size=N) . &lt;keras.callbacks.History at 0x7f77b5e4c250&gt; . net.weights . [&lt;tf.Variable &#39;dense_8/kernel:0&#39; shape=(2, 1) dtype=float32, numpy= array([[2.4630797], [3.996811 ]], dtype=float32)&gt;] . beta_hat = net.weights y_hat = (X @ beta_hat).reshape(-1) . import pandas as pd import matplotlib.pyplot as plt . plt.plot(x,y,&quot;.&quot;) plt.plot(x,y_hat,&quot;--&quot;) . [&lt;matplotlib.lines.Line2D at 0x7f77b5a00b10&gt;] .",
            "url": "https://gangcheol.github.io/big-data-analysis/2020/04/12/(6%EC%A3%BC%EC%B0%A8)-%EA%B3%BC%EC%A0%9C.html",
            "relUrl": "/2020/04/12/(6%EC%A3%BC%EC%B0%A8)-%EA%B3%BC%EC%A0%9C.html",
            "date": " • Apr 12, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://gangcheol.github.io/big-data-analysis/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Github . github . Soundcloud . C.I.C . NLP . NLP . Data Mining . Data Mining . Bigdata Analysis . Bigdata Analysis [^1]:a blogging platform that natively supports Jupyter notebooks in addition to other formats. .",
          "url": "https://gangcheol.github.io/big-data-analysis/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://gangcheol.github.io/big-data-analysis/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}