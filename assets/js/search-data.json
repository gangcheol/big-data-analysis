{
  
    
        "post0": {
            "title": "(4주차) 3월28일 -- draft",
            "content": "&#44053;&#51032;&#50689;&#49345; . &#48120;&#48516; . tf.GradientTape() &#49324;&#50857;&#48169;&#48277; . - 예제9: 카페예제로 돌아오자. (1주차 강의) . - 자료 생성 . import matplotlib.pyplot as plt import tensorflow as tf import tensorflow.experimental.numpy as tnp . . tnp.experimental_enable_numpy_behavior() . x=tnp.array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4]) x . &lt;tf.Tensor: shape=(10,), dtype=float64, numpy=array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4])&gt; . tnp.random.seed(43052) y= 10.2+ x*2.2 + tnp.random.randn(10) y . &lt;tf.Tensor: shape=(10,), dtype=float64, numpy= array([54.98269924, 60.27348365, 61.27621687, 60.53495888, 62.9770905 , 66.32168996, 66.87781372, 71.0050025 , 72.63837337, 77.11143943])&gt; . &#51473;&#44036;&#44256;&#49324; &#44592;&#52636;&#47928;&#51228; . - loss 정의 . beta0= tf.Variable(9.0) beta1= tf.Variable(2.0) . with tf.GradientTape(persistent=True) as tape : loss = sum((y-beta0-beta1*x)**2) . tape.gradient(loss,beta0),tape.gradient(loss,beta1) . (&lt;tf.Tensor: shape=(), dtype=float32, numpy=-127.597534&gt;, &lt;tf.Tensor: shape=(), dtype=float32, numpy=-3214.2532&gt;) . - 계산이 맞는지 확인 . X= tnp.array([1]*10+ [20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4]).reshape(2,10).T X . &lt;tf.Tensor: shape=(10, 2), dtype=float64, numpy= array([[ 1. , 20.1], [ 1. , 22.2], [ 1. , 22.7], [ 1. , 23.3], [ 1. , 24.4], [ 1. , 25.1], [ 1. , 26.2], [ 1. , 27.3], [ 1. , 28.4], [ 1. , 30.4]])&gt; . beta_true = tnp.array([[10.2],[2.2]]) beta_true . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[10.2], [ 2.2]])&gt; . tnp.random.seed(43052) y= X@beta_true + tnp.random.randn(10).reshape(10,1) y . &lt;tf.Tensor: shape=(10, 1), dtype=float64, numpy= array([[54.98269924], [60.27348365], [61.27621687], [60.53495888], [62.9770905 ], [66.32168996], [66.87781372], [71.0050025 ], [72.63837337], [77.11143943]])&gt; . with tf.GradientTape(persistent=True) as tape: tape.watch(beta) yhat = X@beta loss=(y-yhat).T @ (y-yhat) . - 미분 . tape.gradient(loss,beta) # 텐서플로우가 계산한 미분값 . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[ -127.59753624], [-3214.25306574]])&gt; . - 이론적인 값과 동일한지 확인 . $$loss = -2 X^{ prime}y + 2X^{ prime}X beta$$ . -2 * X.T @ y + 2* X.T @ X @ beta # 이론적인 값 . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[ -127.59753624], [-3214.25306574]])&gt; . - 베타 추정치 계산 . tf.linalg.inv(X.T @ X) @ X.T @ y . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[12.10040012], [ 2.13112662]])&gt; . - 예제11: 위의 예제에서 이론적인 $ boldsymbol{ beta}$의 최적값을 찾아보고 (즉 $ boldsymbol{ hat beta}$을 찾고) 그 지점에서 loss의 미분값(=접선의 기울기)를 구하라. 결과가 $ bf{0}$인지 확인하라. (단 ${ bf 0}$은 길이가 2이고 각 원소가 0인 벡터) . loss 값을 보아 제대로 추정되지 않았음 | . $ beta$의 최적값은 $(X^{ prime}X)^{-1}X^{ prime}y$ . beta_optimal = tf.linalg.inv(X.T @ X) @ X.T @ y beta_optimal . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[12.10040012], [ 2.13112662]])&gt; . beta_optimal을 미분했을 때 0이 나와야 실제 베타에 대한 최적의 추정치가 구해진거임 . with tf.GradientTape(persistent=True) as tape: tape.watch(beta_optimal) yhat = X@beta_optimal loss=(y-yhat).T @ (y-yhat) . tape.gradient(loss,beta_optimal) # 텐서플로우가 계산한 미분값 . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[-5.27222710e-12], [-1.33283323e-10]])&gt; . - 베타의 실제값과 베타 옵티말을 넣으면 어떨까? . with tf.GradientTape(persistent=True) as tape: tape.watch(beta_true) yhat = X@beta_true loss=(y-yhat).T @ (y-yhat) . tape.gradient(loss,beta_true) # 텐서플로우가 계산한 미분값 . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[ -3.55753624], [-76.87306574]])&gt; . 옵티말보다 loss가 최적보단 크다 그러나 표본의 수가 커질 경우 최적치와 실제가 점점 비슷해진다 | . 결국 beta_true $ approx$ beta_optimal 이라고 할 수 있다. | . &#44221;&#49324;&#54616;&#44053;&#48277; . - $loss = ( frac {1}{2} beta-1)^2$ 을 최소화 하는 $ beta$ 를 구해보자 . - 당연히 $ beta=2$일 떼 최솟값을 가질 것이다 . - 이것을 컴퓨터로 직접 구해보자 . &#52572;&#51201;&#54868;&#47928;&#51228; . &#48169;&#48277;1: grid search . - 단순히 베타를 개많이 만들고 loss를 최소화하는 베타를 찾자 . &#50508;&#44256;&#47532;&#51608; . &#44396;&#54788;&#53076;&#46300; . beta = tnp.linspace(-10,10,1000) #beta . loss = (1/2*beta-1)**2 . tnp.argmin(loss) . &lt;tf.Tensor: shape=(), dtype=int64, numpy=599&gt; . grid search로 알아봤을 때 최솟값이 2에 근사하게 나온다 | . beta[599] . &lt;tf.Tensor: shape=(), dtype=float64, numpy=1.9919919919919913&gt; . &#44536;&#47532;&#46300;&#49436;&#52824;&#51032; &#47928;&#51228;&#51216; . - 좀 정확하진 않지만 표본의 수를 늘리면 2의 근접한 값을 찾는다. . - 비판1: [-10,10]이외에 해가 존재하면? 즉 범위 밖에 존재할 수 가 있음 . 이 예제의 경우는 운좋게 [-10,10]에서 해가 존재했음 | 하지만 임의의 고정된 $x,y$에 대하여 $loss( beta)=(x beta-y)^2$ 의 형태의 해가 항상 [-10,10]에서 존재한다는 보장은 없음 | 해결책: 더 넓게 많은 범위를 탐색하자? $ to$ but, 무한대의 범위에서 할 수 없음 | . - 비판2: 위 해결책은 효율적이지 않음 . 알고리즘을 요약하면 결국 -10부터 10까지 작은 간격으로 조금씩 이동하며 loss를 조사하는 것이 grid search의 아이디어 | $ to$ 생각해보니까 $ beta=2$인 순간 $loss=( frac{1}{2} beta-1)^2=0$이 되어서 이것보다 작은 최소값은 존재하지 않는다(제곱은 항상 양수이어야 하므로) | $ to$ 따라서 $ beta=2$ 이후로는 탐색할 필요가 없다 | . &#48169;&#48277;2: gradient descent . (1) 임의의 초기값을 선정하고 $loss$를 계산한다 (초깃값 셋팅) . $ beta = -5 to loss(-5) = (-5/2-1)^2 = 12.55$ | . (-5/2-1)**2 . 12.25 . (2) 임의의 초기값에서 좌우로 약간씩 이동해보고 $loss$를 계한한다. (미분에서 최솟값을 찾는 과정) . $ to beta = -5.01, , beta = -4.99$ . (-5.01 /2 -1)**2,(-4.99 /2 -1)**2 . (12.285025, 12.215025) . (3) (2)의 결과를 보고 어느쪽으로 이동하는 것이 유리한지 따져본다. 그 후 유리한 방향으로 이동한다. . import matplotlib.pyplot as plt . plt.plot(beta,loss) . [&lt;matplotlib.lines.Line2D at 0x7f44a80e87d0&gt;] . - (2) - (3)의 과정은 $ beta = -5$ 미분계수를 구한 후 미분계수가 양수이면 왼쪽으로 움직이고 음수이면 오른쪽으로 움직인다고 해석가능 . (4) (2) ~ (3) 과정을 반복 후, 어느쪽으로 가도 유리한 지점이 없다면 알고리즘을 멈춘다 . &#50508;&#44256;&#47532;&#51608; &#48516;&#49437; . - 알고리즘이 멈추는 지점은 $ beta=2$이다. 왜냐하면 이경우 왼쪽으로 가도, 오른쪽으로 가도 현재 손실함수값보다 크기 때문. . &#50812;&#51901;/&#50724;&#47480;&#51901;&#51473;&#50640; &#50612;&#46356;&#47196; &#44040;&#51648; &#50612;&#46523;&#44172; &#54032;&#45800;&#54616;&#45716; &#44284;&#51221;&#51012; &#49688;&#49885;&#54868;? . 오른쪽으로 0.01간다 $ to$ 미분계수가 음수일 때 | 왼쪽으로 0.01간다 $ to$ 미분계수가 양수일 때 | . - 그렇다면 . $ beta_{new} = beta_{old} + 0.01 to frac{d ,loss}{d , beta_{old}}$ 가 음수 일때 . $ beta_{new} = beta_{old} - 0.01 to frac{d ,loss}{d , beta_{old}}$ 가 양수 일때 . &#54841;&#49884; &#50508;&#44256;&#47532;&#51608;&#51012; &#51328; &#44060;&#49440;&#54624;&#49688; &#51080;&#51012;&#44620;? . - 동일하게 0.01씩 이동하는게 맞는지 의문 . import numpy as np . _beta = np.linspace(-10,5) plt.plot(_beta,(_beta/2-1)**2) . [&lt;matplotlib.lines.Line2D at 0x7f44a7ac5a50&gt;] . - $ beta=-10$ 일 경우의 접선의 기울기? $ beta=-4$ 일때 접선의 기울기? . - $ beta= -10 to 기울기는 -6$ . - $ beta= -4 to 기울기는 -3$ . 위 같은 경우 $ beta = -10$ 에서 0.01만큼 이동했다면 $ beta=-4$ 에서 0.005만큼 이동해야함 | . 즉, 떨어진 만큼 비례해서 조금 더 자신있게 가자는 거임 | . $$ beta_{new}= beta_{old} pm alpha left[ frac {∂}{ ,∂ beta}loss( beta) right], quad alpha&gt;0$$ . &#44396;&#54788;&#53076;&#46300; . &#54617;&#49845;&#47456; . [&#49884;&#44033;&#54868; &#53076;&#46300; &#50696;&#48708;&#54617;&#49845;] .",
            "url": "https://gangcheol.github.io/big-data-analysis/2022/03/28/(4%EC%A3%BC%EC%B0%A8)-3%EC%9B%9428%EC%9D%BC.html",
            "relUrl": "/2022/03/28/(4%EC%A3%BC%EC%B0%A8)-3%EC%9B%9428%EC%9D%BC.html",
            "date": " • Mar 28, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "(2주차) Tensorflow",
            "content": "1&#51452;&#52264; &#48373;&#49845; . &#54924;&#44480;&#44228;&#49688; &#52628;&#51221;&#52824; . 단순선형회귀의 경우 일반적인 베타계수의 추정치 | . $ hat { beta_0} = bar y - beta _1 bar x, quad hat { beta_1} = frac {S_{xy}} {S_{xx}}$ . 다중 회귀의 경우 | . $$L=loss =({ bf y}-{ bf X}{ boldsymbol beta})^ top({ bf y}-{ bf X}{ boldsymbol beta})$$ . $$ L= { bf y}^ top { bf y} - { bf y}^ top { bf X}{ boldsymbol beta} - { boldsymbol beta}^ top { bf X}^ top { bf y} + { boldsymbol beta}^ top { bf X}^ top { bf X} { boldsymbol beta}$$ . 위를 미분하면 | . $$ frac{ partial}{ partial boldsymbol{ beta}} L = frac{ partial}{ partial boldsymbol{ beta}} { bf y}^ top { bf y} - frac{ partial}{ partial boldsymbol{ beta}} { bf y}^ top { bf X}{ boldsymbol beta} - frac{ partial}{ partial boldsymbol{ beta}} { boldsymbol beta}^ top { bf X}^ top { bf y} + frac{ partial}{ partial boldsymbol{ beta}} { boldsymbol beta}^ top { bf X}^ top { bf X} { boldsymbol beta} $$ . $$ frac{ partial}{ partial boldsymbol{ beta}} L=- mathbf{X^{ top}y} - mathbf{X^{ top}y} + 2 mathbf{X^{ top}X boldsymbol beta}$$ . 따라서 . $$ quad bf{X^{ top}X} beta = bf{X^{ top}Y}$$ $$ therefore quad hat { beta} = left( bf{X^{ top}X}^{-1} right)XY$$ . . 2&#51452;&#52264; &#49688;&#50629;&#49884;&#51089;~~ . import tensorflow as tf import numpy as np . . tensorflow&#51032; GPU &#50672;&#44208;&#48277; . tf.config.experimental.list_physical_devices(&#39;GPU&#39;) . [PhysicalDevice(name=&#39;/physical_device:GPU:0&#39;, device_type=&#39;GPU&#39;)] . tf.constant . &#50696;&#48708;&#54617;&#49845;: &#51473;&#52393;&#47532;&#49828;&#53944; . - 리스트 . lst = list(range(6)) lst . [0, 1, 2, 3, 4, 5] . lst[-1] . 5 . - 리스트 안에 리스트 생성 . lst =[[1,2,],[3,4]] lst . [[1, 2], [3, 4]] . print(lst[1][0],lst[0][0]) . 3 1 . - 위 같은 2차원의 리스트 구조를 행렬로 생각할 수 있다 . 1 2 3 4 . 또는 . 1 2 3 4 . - (4,1) 행렬 느낌의 리스트 . lst = [[1],[2],[3],[4]] lst . [[1], [2], [3], [4]] . np.array(lst) . array([[1], [2], [3], [4]]) . - (1,4) 행렬 느낌의 리스트 . lst = [1,2,3,4] lst . [1, 2, 3, 4] . np.array(lst) . array([1, 2, 3, 4]) . &#48320;&#49688; &#49440;&#50616; . &#49828;&#52860;&#46972; . tf.constant(3.14) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=3.14&gt; . tf.constant(3.14) + tf.constant(3.14) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=6.28&gt; . &#48289;&#53552; . _vector = tf.constant([1,2,3 ]) _vector . &lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 2, 3], dtype=int32)&gt; . _vector[0] . &lt;tf.Tensor: shape=(), dtype=int32, numpy=1&gt; . &#47588;&#53944;&#47533;&#49828; &#49373;&#49457; . _matrix = tf.constant([[1,0],[0,1]]) _matrix . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 0], [0, 1]], dtype=int32)&gt; . &#53584;&#49436; == 3&#52264;&#50896; &#51060;&#49345;&#51032; &#48176;&#50676; . np.array([[[0,1],[1,2]],[[0,1],[1,2]]]) . array([[[0, 1], [1, 2]], [[0, 1], [1, 2]]]) . tf.constant([[[0,1],[1,2]],[[0,1],[1,2]]]) . &lt;tf.Tensor: shape=(2, 2, 2), dtype=int32, numpy= array([[[0, 1], [1, 2]], [[0, 1], [1, 2]]], dtype=int32)&gt; . &#53440;&#51077; . type(tf.constant([[[0,1],[1,2]],[[0,1],[1,2]]])) . tensorflow.python.framework.ops.EagerTensor . - 끝에 EagerTensor 가 나오는 것을 기억하자 . &#51064;&#45937;&#49905; . _matrix = tf.constant([[1,2],[3,4]]) _matrix . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 2], [3, 4]], dtype=int32)&gt; . _matrix[0] . &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)&gt; . _matrix[0,:] . &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)&gt; . _matrix[0,0] . &lt;tf.Tensor: shape=(), dtype=int32, numpy=1&gt; . _matrix[0][0] . &lt;tf.Tensor: shape=(), dtype=int32, numpy=1&gt; . tf.constant&#45716; &#48520;&#54200;&#54616;&#45796;. . - 각 컬럼의 데이터 타입이 전부 동일하여야 한다. . - 원소 수정이 불가능함. . a= tf.constant([1,22,33]) a . &lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([ 1, 22, 33], dtype=int32)&gt; . a[0] =11 . TypeError Traceback (most recent call last) &lt;ipython-input-47-cf0e5bd1fc84&gt; in &lt;module&gt;() -&gt; 1 a[0] =11 TypeError: &#39;tensorflow.python.framework.ops.EagerTensor&#39; object does not support item assignment . - 묵시적(간접적) 형변환이 불가능하다. . 1+3.14 . 4.140000000000001 . tf.constant(1) + tf.constant(3.14) . InvalidArgumentError Traceback (most recent call last) &lt;ipython-input-54-51b23ac3bfb0&gt; in &lt;module&gt;() -&gt; 1 tf.constant(1) + tf.constant(3.14) /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py in error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name) 7184 def raise_from_not_ok_status(e, name): 7185 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7186 raise core._status_to_exception(e) from None # pylint: disable=protected-access 7187 7188 InvalidArgumentError: cannot compute AddV2 as input #1(zero-based) was expected to be a int32 tensor but is a float tensor [Op:AddV2] . - 같은 float 도 안되는 경우가 있음 . tf.constant(1.0, dtype= tf.float64) + tf.constant(3.14) . InvalidArgumentError Traceback (most recent call last) &lt;ipython-input-57-4390fbcde8ad&gt; in &lt;module&gt;() -&gt; 1 tf.constant(1.0, dtype= tf.float64) + tf.constant(3.14) /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py in error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name) 7184 def raise_from_not_ok_status(e, name): 7185 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7186 raise core._status_to_exception(e) from None # pylint: disable=protected-access 7187 7188 InvalidArgumentError: cannot compute AddV2 as input #1(zero-based) was expected to be a double tensor but is a float tensor [Op:AddV2] . tf.constant $ to$ &#45336;&#54028;&#51060; . np.array(tf.constant(1)) . array(1, dtype=int32) . a = tf.constant(3.14) type(a) . tensorflow.python.framework.ops.EagerTensor . a.numpy() . 3.14 . &#50672;&#49328; . &#45908;&#54616;&#44592; . a = tf.constant([1,2]) b = tf.constant([3,4]) a+b . &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([4, 6], dtype=int32)&gt; . tf.add(a,b) ## 이건 예전버전 . &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([4, 6], dtype=int32)&gt; . &#44273;&#54616;&#44592; . - 결과가 조금 이상하다. 일반적인 행렬연사이 아니다 . a = tf.constant([[1,2],[3,4]]) b = tf.constant([[5,6],[7,8]]) a*b . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[ 5, 12], [21, 32]], dtype=int32)&gt; . - but matrix의 곱은 . a = tf.constant([[1,0],[0,1]]) b = tf.constant([[5],[7]]) a@b . &lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy= array([[5], [7]], dtype=int32)&gt; . tf.matmul(a,b) ## 위와 같은 표현 . &#50669;&#54665;&#47148; . a = tf.constant([[1,0],[0,2]]) a . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 0], [0, 2]], dtype=int32)&gt; . tf.linalg.inv(a) . InvalidArgumentError Traceback (most recent call last) &lt;ipython-input-70-fd9ade66c025&gt; in &lt;module&gt;() -&gt; 1 tf.linalg.inv(a) /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_linalg_ops.py in matrix_inverse(input, adjoint, name) 1504 return _result 1505 except _core._NotOkStatusException as e: -&gt; 1506 _ops.raise_from_not_ok_status(e, name) 1507 except _core._FallbackException: 1508 pass /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name) 7184 def raise_from_not_ok_status(e, name): 7185 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7186 raise core._status_to_exception(e) from None # pylint: disable=protected-access 7187 7188 InvalidArgumentError: Value for attr &#39;T&#39; of int32 is not in the list of allowed values: double, float, half, complex64, complex128 ; NodeDef: {{node MatrixInverse}}; Op&lt;name=MatrixInverse; signature=input:T -&gt; output:T; attr=adjoint:bool,default=false; attr=T:type,allowed=[DT_DOUBLE, DT_FLOAT, DT_HALF, DT_COMPLEX64, DT_COMPLEX128]&gt; [Op:MatrixInverse] . 위의 경우는 자료가 int 형이여서 안되는 거임 | . ?tf.constant . 아래오 같이 자료형을 선언해 주어야함 | . a = tf.constant([[1,0],[0,2]],dtype=float) tf.linalg.inv(a) . &lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy= array([[1. , 0. ], [0. , 0.5]], dtype=float32)&gt; . determinant . a = tf.constant([[1,2],[3,4]],dtype=float) print(a) tf.linalg.det(a) . tf.Tensor( [[1. 2.] [3. 4.]], shape=(2, 2), dtype=float32) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=-2.0&gt; . Trace . tf.linalg.trace(a) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=5.0&gt; . &#54805;&#53468;&#48320;&#54872; . - 1 x 4 행렬을 $ to$ 4 x 1 . a = tf.constant([1,2,3,4]) a . &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt; . tf.reshape(a,(2,2)) . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 2], [3, 4]], dtype=int32)&gt; . - 3차원으로도 변경이 가능 . tf.reshape(a,(2,2,1)) . &lt;tf.Tensor: shape=(2, 2, 1), dtype=int32, numpy= array([[[1], [2]], [[3], [4]]], dtype=int32)&gt; . - 다차원의 경우 적용 . a = tf.constant(list(range(1,13))) a . &lt;tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], dtype=int32)&gt; . tf.reshape(a,(2,2,3)) . &lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy= array([[[ 1, 2, 3], [ 4, 5, 6]], [[ 7, 8, 9], [10, 11, 12]]], dtype=int32)&gt; . tf.reshape(a,(4,3)) . &lt;tf.Tensor: shape=(4, 3), dtype=int32, numpy= array([[ 1, 2, 3], [ 4, 5, 6], [ 7, 8, 9], [10, 11, 12]], dtype=int32)&gt; . -1 을 기입하면 남은 차원 수를 알아서 기입해줌 -1 = ? 라고 생각 | . tf.reshape(a,(4,-1)) . &lt;tf.Tensor: shape=(4, 3), dtype=int32, numpy= array([[ 1, 2, 3], [ 4, 5, 6], [ 7, 8, 9], [10, 11, 12]], dtype=int32)&gt; . b= tf.reshape(a,(2,2,-1)) b . &lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy= array([[[ 1, 2, 3], [ 4, 5, 6]], [[ 7, 8, 9], [10, 11, 12]]], dtype=int32)&gt; . - 다시 일차원으로 되돌림 . tf.reshape(b,-1) . &lt;tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], dtype=int32)&gt; . &#49440;&#50616;&#44256;&#44553; . - 리스트나, 넘파이로 만들고 output을 tensor로 변경하는 것도 좋은 방법이다. . ㅣ = [1,2,3,4] tf.constant(np.diag(ㅣ)) . &lt;tf.Tensor: shape=(4, 4), dtype=int64, numpy= array([[1, 0, 0, 0], [0, 2, 0, 0], [0, 0, 3, 0], [0, 0, 0, 4]])&gt; . - tf.ones, tf.zeros . tf.zeros([3,3]) . &lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy= array([[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]], dtype=float32)&gt; . - tf.linspace(0,1,10) . tf.linspace(0,1,10) . &lt;tf.Tensor: shape=(10,), dtype=float64, numpy= array([0. , 0.11111111, 0.22222222, 0.33333333, 0.44444444, 0.55555556, 0.66666667, 0.77777778, 0.88888889, 1. ])&gt; . tf.concat . a = tf.constant([1,2]) b = tf.constant([3,4]) a,b . (&lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)&gt;, &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([3, 4], dtype=int32)&gt;) . a = tf.constant([[1],[2]]) b = tf.constant([[3],[4]]) a,b . (&lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy= array([[1], [2]], dtype=int32)&gt;, &lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy= array([[3], [4]], dtype=int32)&gt;) . tf.concat([a,b],axis=0) . &lt;tf.Tensor: shape=(4, 1), dtype=int32, numpy= array([[1], [2], [3], [4]], dtype=int32)&gt; . a = tf.constant([[1],[2]]) b = tf.constant([[3],[4]]) a,b . (&lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy= array([[1], [2]], dtype=int32)&gt;, &lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy= array([[3], [4]], dtype=int32)&gt;) . tf.concat([a,b],axis=1) . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 3], [2, 4]], dtype=int32)&gt; . a = tf.constant([1,2]) b = tf.constant([3,4]) a,b tf.concat([a,b],axis=0) . &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt; . a = tf.constant([[1,2]]) b = tf.constant([[3,4]]) a,b tf.concat([a,b],axis=0) . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 2], [3, 4]], dtype=int32)&gt; . &#52264;&#50896; &#49688; &#51613;&#44032; . (2,3,4,5) concat (2,3,4,5) =&gt; (4,3,4,5) | . a=tf.reshape(tf.constant(range(120)),(2,3,4,5)) b= -a . tf.concat([a,b],axis=0) . &lt;tf.Tensor: shape=(4, 3, 4, 5), dtype=int32, numpy= array([[[[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [ 10, 11, 12, 13, 14], [ 15, 16, 17, 18, 19]], [[ 20, 21, 22, 23, 24], [ 25, 26, 27, 28, 29], [ 30, 31, 32, 33, 34], [ 35, 36, 37, 38, 39]], [[ 40, 41, 42, 43, 44], [ 45, 46, 47, 48, 49], [ 50, 51, 52, 53, 54], [ 55, 56, 57, 58, 59]]], [[[ 60, 61, 62, 63, 64], [ 65, 66, 67, 68, 69], [ 70, 71, 72, 73, 74], [ 75, 76, 77, 78, 79]], [[ 80, 81, 82, 83, 84], [ 85, 86, 87, 88, 89], [ 90, 91, 92, 93, 94], [ 95, 96, 97, 98, 99]], [[ 100, 101, 102, 103, 104], [ 105, 106, 107, 108, 109], [ 110, 111, 112, 113, 114], [ 115, 116, 117, 118, 119]]], [[[ 0, -1, -2, -3, -4], [ -5, -6, -7, -8, -9], [ -10, -11, -12, -13, -14], [ -15, -16, -17, -18, -19]], [[ -20, -21, -22, -23, -24], [ -25, -26, -27, -28, -29], [ -30, -31, -32, -33, -34], [ -35, -36, -37, -38, -39]], [[ -40, -41, -42, -43, -44], [ -45, -46, -47, -48, -49], [ -50, -51, -52, -53, -54], [ -55, -56, -57, -58, -59]]], [[[ -60, -61, -62, -63, -64], [ -65, -66, -67, -68, -69], [ -70, -71, -72, -73, -74], [ -75, -76, -77, -78, -79]], [[ -80, -81, -82, -83, -84], [ -85, -86, -87, -88, -89], [ -90, -91, -92, -93, -94], [ -95, -96, -97, -98, -99]], [[-100, -101, -102, -103, -104], [-105, -106, -107, -108, -109], [-110, -111, -112, -113, -114], [-115, -116, -117, -118, -119]]]], dtype=int32)&gt; . (2,3,4,5) concat (2,3,4,5) =&gt; (2,6,4,5) | . tf.concat([a,b],axis=1) . &lt;tf.Tensor: shape=(2, 6, 4, 5), dtype=int32, numpy= array([[[[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [ 10, 11, 12, 13, 14], [ 15, 16, 17, 18, 19]], [[ 20, 21, 22, 23, 24], [ 25, 26, 27, 28, 29], [ 30, 31, 32, 33, 34], [ 35, 36, 37, 38, 39]], [[ 40, 41, 42, 43, 44], [ 45, 46, 47, 48, 49], [ 50, 51, 52, 53, 54], [ 55, 56, 57, 58, 59]], [[ 0, -1, -2, -3, -4], [ -5, -6, -7, -8, -9], [ -10, -11, -12, -13, -14], [ -15, -16, -17, -18, -19]], [[ -20, -21, -22, -23, -24], [ -25, -26, -27, -28, -29], [ -30, -31, -32, -33, -34], [ -35, -36, -37, -38, -39]], [[ -40, -41, -42, -43, -44], [ -45, -46, -47, -48, -49], [ -50, -51, -52, -53, -54], [ -55, -56, -57, -58, -59]]], [[[ 60, 61, 62, 63, 64], [ 65, 66, 67, 68, 69], [ 70, 71, 72, 73, 74], [ 75, 76, 77, 78, 79]], [[ 80, 81, 82, 83, 84], [ 85, 86, 87, 88, 89], [ 90, 91, 92, 93, 94], [ 95, 96, 97, 98, 99]], [[ 100, 101, 102, 103, 104], [ 105, 106, 107, 108, 109], [ 110, 111, 112, 113, 114], [ 115, 116, 117, 118, 119]], [[ -60, -61, -62, -63, -64], [ -65, -66, -67, -68, -69], [ -70, -71, -72, -73, -74], [ -75, -76, -77, -78, -79]], [[ -80, -81, -82, -83, -84], [ -85, -86, -87, -88, -89], [ -90, -91, -92, -93, -94], [ -95, -96, -97, -98, -99]], [[-100, -101, -102, -103, -104], [-105, -106, -107, -108, -109], [-110, -111, -112, -113, -114], [-115, -116, -117, -118, -119]]]], dtype=int32)&gt; . (2,3,4,5) concat (2,3,4,5) =&gt; (2,3,8,5) | . tf.concat([a,b],axis=2) . &lt;tf.Tensor: shape=(2, 3, 8, 5), dtype=int32, numpy= array([[[[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [ 10, 11, 12, 13, 14], [ 15, 16, 17, 18, 19], [ 0, -1, -2, -3, -4], [ -5, -6, -7, -8, -9], [ -10, -11, -12, -13, -14], [ -15, -16, -17, -18, -19]], [[ 20, 21, 22, 23, 24], [ 25, 26, 27, 28, 29], [ 30, 31, 32, 33, 34], [ 35, 36, 37, 38, 39], [ -20, -21, -22, -23, -24], [ -25, -26, -27, -28, -29], [ -30, -31, -32, -33, -34], [ -35, -36, -37, -38, -39]], [[ 40, 41, 42, 43, 44], [ 45, 46, 47, 48, 49], [ 50, 51, 52, 53, 54], [ 55, 56, 57, 58, 59], [ -40, -41, -42, -43, -44], [ -45, -46, -47, -48, -49], [ -50, -51, -52, -53, -54], [ -55, -56, -57, -58, -59]]], [[[ 60, 61, 62, 63, 64], [ 65, 66, 67, 68, 69], [ 70, 71, 72, 73, 74], [ 75, 76, 77, 78, 79], [ -60, -61, -62, -63, -64], [ -65, -66, -67, -68, -69], [ -70, -71, -72, -73, -74], [ -75, -76, -77, -78, -79]], [[ 80, 81, 82, 83, 84], [ 85, 86, 87, 88, 89], [ 90, 91, 92, 93, 94], [ 95, 96, 97, 98, 99], [ -80, -81, -82, -83, -84], [ -85, -86, -87, -88, -89], [ -90, -91, -92, -93, -94], [ -95, -96, -97, -98, -99]], [[ 100, 101, 102, 103, 104], [ 105, 106, 107, 108, 109], [ 110, 111, 112, 113, 114], [ 115, 116, 117, 118, 119], [-100, -101, -102, -103, -104], [-105, -106, -107, -108, -109], [-110, -111, -112, -113, -114], [-115, -116, -117, -118, -119]]]], dtype=int32)&gt; . (2,3,4,5) concat (2,3,4,5) =&gt; (2,3,4,10) | . tf.concat([a,b],axis=3) . &lt;tf.Tensor: shape=(2, 3, 4, 10), dtype=int32, numpy= array([[[[ 0, 1, 2, 3, 4, 0, -1, -2, -3, -4], [ 5, 6, 7, 8, 9, -5, -6, -7, -8, -9], [ 10, 11, 12, 13, 14, -10, -11, -12, -13, -14], [ 15, 16, 17, 18, 19, -15, -16, -17, -18, -19]], [[ 20, 21, 22, 23, 24, -20, -21, -22, -23, -24], [ 25, 26, 27, 28, 29, -25, -26, -27, -28, -29], [ 30, 31, 32, 33, 34, -30, -31, -32, -33, -34], [ 35, 36, 37, 38, 39, -35, -36, -37, -38, -39]], [[ 40, 41, 42, 43, 44, -40, -41, -42, -43, -44], [ 45, 46, 47, 48, 49, -45, -46, -47, -48, -49], [ 50, 51, 52, 53, 54, -50, -51, -52, -53, -54], [ 55, 56, 57, 58, 59, -55, -56, -57, -58, -59]]], [[[ 60, 61, 62, 63, 64, -60, -61, -62, -63, -64], [ 65, 66, 67, 68, 69, -65, -66, -67, -68, -69], [ 70, 71, 72, 73, 74, -70, -71, -72, -73, -74], [ 75, 76, 77, 78, 79, -75, -76, -77, -78, -79]], [[ 80, 81, 82, 83, 84, -80, -81, -82, -83, -84], [ 85, 86, 87, 88, 89, -85, -86, -87, -88, -89], [ 90, 91, 92, 93, 94, -90, -91, -92, -93, -94], [ 95, 96, 97, 98, 99, -95, -96, -97, -98, -99]], [[ 100, 101, 102, 103, 104, -100, -101, -102, -103, -104], [ 105, 106, 107, 108, 109, -105, -106, -107, -108, -109], [ 110, 111, 112, 113, 114, -110, -111, -112, -113, -114], [ 115, 116, 117, 118, 119, -115, -116, -117, -118, -119]]]], dtype=int32)&gt; . 아래와 같은 방법도 있긴하나 난 안할래 | . tf.concat([a,b],axis=-1) . &lt;tf.Tensor: shape=(2, 3, 4, 10), dtype=int32, numpy= array([[[[ 0, 1, 2, 3, 4, 0, -1, -2, -3, -4], [ 5, 6, 7, 8, 9, -5, -6, -7, -8, -9], [ 10, 11, 12, 13, 14, -10, -11, -12, -13, -14], [ 15, 16, 17, 18, 19, -15, -16, -17, -18, -19]], [[ 20, 21, 22, 23, 24, -20, -21, -22, -23, -24], [ 25, 26, 27, 28, 29, -25, -26, -27, -28, -29], [ 30, 31, 32, 33, 34, -30, -31, -32, -33, -34], [ 35, 36, 37, 38, 39, -35, -36, -37, -38, -39]], [[ 40, 41, 42, 43, 44, -40, -41, -42, -43, -44], [ 45, 46, 47, 48, 49, -45, -46, -47, -48, -49], [ 50, 51, 52, 53, 54, -50, -51, -52, -53, -54], [ 55, 56, 57, 58, 59, -55, -56, -57, -58, -59]]], [[[ 60, 61, 62, 63, 64, -60, -61, -62, -63, -64], [ 65, 66, 67, 68, 69, -65, -66, -67, -68, -69], [ 70, 71, 72, 73, 74, -70, -71, -72, -73, -74], [ 75, 76, 77, 78, 79, -75, -76, -77, -78, -79]], [[ 80, 81, 82, 83, 84, -80, -81, -82, -83, -84], [ 85, 86, 87, 88, 89, -85, -86, -87, -88, -89], [ 90, 91, 92, 93, 94, -90, -91, -92, -93, -94], [ 95, 96, 97, 98, 99, -95, -96, -97, -98, -99]], [[ 100, 101, 102, 103, 104, -100, -101, -102, -103, -104], [ 105, 106, 107, 108, 109, -105, -106, -107, -108, -109], [ 110, 111, 112, 113, 114, -110, -111, -112, -113, -114], [ 115, 116, 117, 118, 119, -115, -116, -117, -118, -119]]]], dtype=int32)&gt; . &#52264;&#50896;&#51012; &#54620;&#48264; &#51460;&#50668;&#48372;&#51088; . (4,) -&gt; (8,) | . a=tf.constant([1,2,3,4]) b=-a a,b . (&lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt;, &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([-1, -2, -3, -4], dtype=int32)&gt;) . tf.concat([a,b],axis=0) . &lt;tf.Tensor: shape=(8,), dtype=int32, numpy=array([ 1, 2, 3, 4, -1, -2, -3, -4], dtype=int32)&gt; . (4,) -&gt; (4,2) | . - 에러가 뜬다 . tf.concat([a,b],axis=1) . InvalidArgumentError Traceback (most recent call last) &lt;ipython-input-145-6f52ef50c654&gt; in &lt;module&gt;() -&gt; 1 tf.concat([a,b],axis=1) /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py in error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name) 7184 def raise_from_not_ok_status(e, name): 7185 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7186 raise core._status_to_exception(e) from None # pylint: disable=protected-access 7187 7188 InvalidArgumentError: ConcatOp : Expected concatenating dimensions in the range [-1, 1), but got 1 [Op:ConcatV2] name: concat . tf.stack . (4,) -&gt; (4,2) | . a=tf.constant([1,2,3,4]) b=-a a,b . (&lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt;, &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([-1, -2, -3, -4], dtype=int32)&gt;) . tf.stack([a,b],axis=1) . &lt;tf.Tensor: shape=(4, 2), dtype=int32, numpy= array([[ 1, -1], [ 2, -2], [ 3, -3], [ 4, -4]], dtype=int32)&gt; . tf.einsum . tnp . - 텐서만들기가 너무 힘듬 . import tensorflow.experimental.numpy as tnp tnp.experimental_enable_numpy_behavior() ## tnf 를 numpy 처럼 사용할 수 있도록 해줌 . tnp &#49324;&#50857;&#48169;&#48277; (&#48520;&#47564;&#54644;&#44208;&#48169;&#48277;) . - int 와 float 을 더할 수 있음 . tnp.array([1,2,3]) + tnp.array([1.0,2.0,3.0]) . &lt;tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 4., 6.])&gt; . 심지어 | . tnp.array(1) + tnp.array([1.0,2.0,3.0]) . &lt;tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 3., 4.])&gt; . tnp.array([1,2,3]) + tf.constant([1.0,2.0,3.0]) . &lt;tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 4., 6.])&gt; . a = tnp.diag([1,2,3]) type(a) . tensorflow.python.framework.ops.EagerTensor . a.min(),a.max() . (&lt;tf.Tensor: shape=(), dtype=int64, numpy=0&gt;, &lt;tf.Tensor: shape=(), dtype=int64, numpy=3&gt;) . a.reshape(9,1) . &lt;tf.Tensor: shape=(9, 1), dtype=int64, numpy= array([[1], [0], [0], [0], [2], [0], [0], [0], [3]])&gt; . &#49440;&#50616;, &#49440;&#50616;&#44256;&#44553; . np.random.randn(5) . array([-1.79271696, -0.17190837, 1.01536417, 0.10096996, 0.6384037 ]) . tnp.random.randn(5) . &lt;tf.Tensor: shape=(5,), dtype=float64, numpy=array([ 0.68371875, -0.77886642, -0.78283853, -1.91862598, -0.36602414])&gt; . &#53440;&#51077; . type(tnp.random.randn(5)) . tensorflow.python.framework.ops.EagerTensor . tf.contant&#47196; &#47564;&#46308;&#50612;&#46020; &#47560;&#52824; &#45336;&#54028;&#51060;&#51064;&#46319; &#50416;&#45716; &#44592;&#45733;&#46308; . - 묵시적 형변환이 가능해짐 . - 메소드를 쓸 수 있음. . &#44536;&#47111;&#51648;&#47564; np.array&#45716; &#50500;&#45784; . 여전히 값을 바꾸는 것은 허용하지 않는다. | . a = tf.constant([1,2,3]) . a[0]=11 . TypeError Traceback (most recent call last) &lt;ipython-input-181-b909b2ec59d1&gt; in &lt;module&gt;() -&gt; 1 a[0]=11 TypeError: &#39;tensorflow.python.framework.ops.EagerTensor&#39; object does not support item assignment . tf.Variable . &#49440;&#50616; . &#53440;&#51077; . &#51064;&#45937;&#49905; . tf.Variable $ to$ &#45336;&#54028;&#51060; . tf.Variable &#46020; &#48520;&#54200;&#54616;&#45796;. . &#50672;&#49328; . &#54805;&#53468;&#48320;&#54872; . &#49440;&#50616;&#44256;&#44553; . tf.concat . tf.stack . &#49900;&#51648;&#50612; tf.Variable()&#47196; &#47564;&#46308;&#50612;&#51652; &#50724;&#48652;&#51229;&#53944;&#45716; tnp&#51032; &#54952;&#44284;(&#51008;&#52509;)&#46020; &#48155;&#51648; &#47803;&#54632; .",
            "url": "https://gangcheol.github.io/big-data-analysis/python/2022/03/14/(2%EC%A3%BC%EC%B0%A8).html",
            "relUrl": "/python/2022/03/14/(2%EC%A3%BC%EC%B0%A8).html",
            "date": " • Mar 14, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "(1주차) 단순선형회귀",
            "content": "&#47196;&#46300;&#47605; . - 오늘수업할내용: 단순선형회귀 . - 단순선형회귀를 배우는 이유? . 우리가 배우고싶은것: 심층신경망(DNN) $ to$ 합성곱신경망(CNN) $ to$ 적대적생성신경망(GAN) | 심층신경망을 바로 이해하기 어려움 | 다음의 과정으로 이해해야함: 선형대수학 $ to$ 회귀분석 $ to$ 로지스틱회귀분석 $ to$ 심층신경망 | . &#49440;&#54805;&#54924;&#44480; . - 상황극 . 날이 더울수록 아이스아메리카노의 판매량이 증가함 | 이를 바탕으로 일기예보의 온도자료를 이용하여 다음과 같은 수식을 이용해 아이스아메리카노의 판매량을 예측할 수 있음 | . $$아이스아메리카노 = beta_1 times 온도 + epsilon$$ . - 가짜자료 생성 . import matplotlib.pyplot as plt import tensorflow as tf . . 온도 ${ bf x}$가 아래와 같다고 하자.(tf.constant 함수를 이용하여 상수 텐서를 생성) . x=tf.constant([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4]) # 기온 x . . &lt;tf.Tensor: shape=(10,), dtype=float32, numpy= array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4], dtype=float32)&gt; . 아이스아메리카노의 판매량 ${ bf y}$이 아래와 같다고 하자. (판매량은 정수로 나오겠지만 편의상 소수점도 가능하다고 생각하자) . $${ bf y} approx 10.2 +2.2 { bf x}$$ . 여기에서 10.2, 2.2 의 숫자는 임의로 정한 $ beta_0, , beta_1$ | 식의의미: 온도가 0일때 10.2잔정도 팔림 + 온도가 1도 증가하면 2.2잔정도 더 팔림 | . tf.random.set_seed(43052) epsilon=tf.random.normal([10]) ## 오차항 생성 y=10.2 + 2.2*x + epsilon y . . &lt;tf.Tensor: shape=(10,), dtype=float32, numpy= array([55.418365, 58.194283, 61.230827, 62.312557, 63.107002, 63.69569 , 67.247055, 71.4365 , 73.1013 , 77.84988 ], dtype=float32)&gt; . - 우리는 아래와 같은 자료를 모았다고 생각하자. . - tensorflow 문법에 관한 내용은 이후 수업에서 다루니 크게 신경쓰지 말자 . tf.transpose(tf.concat([[x],[y]],0)) . . &lt;tf.Tensor: shape=(10, 2), dtype=float32, numpy= array([[20.1 , 55.418365], [22.2 , 58.194283], [22.7 , 61.230827], [23.3 , 62.312557], [24.4 , 63.107002], [25.1 , 63.69569 ], [26.2 , 67.247055], [27.3 , 71.4365 ], [28.4 , 73.1013 ], [30.4 , 77.84988 ]], dtype=float32)&gt; . - 그려보자. . plt.figure(figsize=(12,6)) plt.plot(x,y,&#39;.&#39;) # 파란점, 관측한 데이터 plt.plot(x,10.2 + 2.2*x, &#39;--&#39;) # 주황색점선, 세상의 법칙 . . [&lt;matplotlib.lines.Line2D at 0x2e12f41df70&gt;] . - 우리의 목표: 파란색점(관측값)에 기반하여 온도에 따른 아이스크림 판매량에 대한 일반화 식을 만드는 것 . - 아이디어: 데이터를 보니까 $x$와 $y$가 선형의 관계에 있는듯 보인다. 즉 모든 $i=1,2, dots, 10$에 대하여 아래를 만족하는 적당한 $ beta_0, beta_1$가 존재할것 같다. . $$y_{i} approx beta_1 x_{i}+ beta_0$$ . - 어림짐작으로 $ beta_0, beta_1$를 알아내보자. . 데이터를 살펴보자. . tf.transpose(tf.concat([[x],[y]],0)) . . &lt;tf.Tensor: shape=(10, 2), dtype=float32, numpy= array([[20.1 , 55.418365], [22.2 , 58.194283], [22.7 , 61.230827], [23.3 , 62.312557], [24.4 , 63.107002], [25.1 , 63.69569 ], [26.2 , 67.247055], [27.3 , 71.4365 ], [28.4 , 73.1013 ], [30.4 , 77.84988 ]], dtype=float32)&gt; . 적당히 왼쪽*2+15 = 오른쪽의 관계가 성립하는것 같다. . 따라서 $ beta_0=15, beta_1=2$ 로 추론할 수 있겠다. . - 누군가가 $( beta_0, beta_1)=(14,2)$ 이라고 주장할 수 있다. (어차피 지금은 감각으로 추론하는 과정이니까) . - 새로운 주장으로 인해서 $( beta_0, beta_1)=(15,2)$ 로 볼 수도 있고 $( beta_0, beta_1)=(14,2)$ 로 볼 수도 있다. 이중에서 어떠한 추정치가 좋은지 판단할 수 있을까? . 후보1: $( beta_0, beta_1)=(15,2)$ | 후보2: $( beta_0, beta_1)=(14,2)$ | . - 가능한 $y_i approx beta_0 + beta_1 x_i$ 이 되도록 만드는 $( beta_0, beta_1)$ 이 좋을 것이다. $ to$ 후보 1,2를 비교해보자. . (관찰에 의한 비교) . 후보1에 대해서 $i=1,2$를 넣고 관찰하여 보자. . 20.1 * 2 + 15 , 55.418365 # i=1 . (55.2, 55.418365) . 22.2 * 2 + 15 , 58.194283 # i=2 . (59.4, 58.194283) . 후보2에 대하여 $i=1,2$를 넣고 관찰하여 보자. . 20.1 * 2 + 14 , 55.418365 # i=1 . (54.2, 55.418365) . 22.2 * 2 + 14 , 58.194283 # i=2 . (58.4, 58.194283) . $i=1$인 경우에는 후보1이 더 잘맞는것 같은데 $i=2$인 경우는 후보2가 더 잘맞는것 같다. . (좀 더 체계적인 비교) . $i=1,2,3, dots, 10$ 에서 후보1과 후보2중 어떤것이 더 좋은지 비교하는 체계적인 방법을 생각해보자. . 후보 1,2에 대하여 $ sum_{i=1}^{10} (y_i - beta_0 - beta_1 x_i)^2$를 계산하여 비교해보자. . sum1=0 for i in range(10): sum1=sum1+(y[i]-15-2*x[i])**2 . sum2=0 for i in range(10): sum2=sum2+(y[i]-14-2*x[i])**2 . sum1,sum2 . (&lt;tf.Tensor: shape=(), dtype=float32, numpy=14.734169&gt;, &lt;tf.Tensor: shape=(), dtype=float32, numpy=31.521088&gt;) . 후보1이 더 $ sum_{i=1}^{10} (y_i - beta_0 - beta_1 x_i)^2$의 값이 작다. . 후보1이 종합적으로 후보2에 비하여 좋다. 이 과정을 무한번 반복하면 최적의 추정치를 찾을 수 있다. . - 그런데 이 알고리즘은 현실적으로 구현이 불가능하다. (무한번 계산하기도 힘들고, 언제 멈출지도 애매함) . - 수학을 이용해서 좀 더 체계적으로 찾아보자. 결국 아래식을 가장 작게 만드는 $ beta_0, beta_1$을 찾으면 된다. . $$ sum_{i=1}^{10} (y_i - beta_0 - beta_1 x_i)^2$$ . 그런데 결국 $ beta_0, beta_1$에 대한 이차식인데 이 식을 최소화하는 $ beta_0, beta_1$을 구하기 위해서는 아래를 연립하여 풀면된다. . $$ begin{cases} frac{ partial}{ partial beta_0} sum_{i=1}^{10} (y_i - beta_0 - beta_1 x_i)^2=0 frac{ partial}{ partial beta_1} sum_{i=1}^{10} (y_i - beta_0 - beta_1 x_i)^2=0 end{cases}$$ - 풀어보자. . $$ begin{cases} sum_{i=1}^{10} -2(y_i - beta_0 - beta_1 x_i)=0 sum_{i=1}^{10} -2x_i(y_i - beta_0 - beta_1 x_i)=0 end{cases}$$ 정리하면 . $$ hat{ beta}_0= bar{y}- hat{ beta}_1 bar{x}$$ . $$ hat{ beta}_1= frac{S_{xy}}{S_{xx}}= frac{ sum_{i=1}^{n}(x_i- bar{x})(y_i- bar{y})}{ sum_{i=1}^{n}(x_i- bar{x})^2}$$ . - 따라서 최적의 추정치 $( hat{ beta}_0, hat{ beta}_1)$를 이용한 추세선을 아래와 같이 계산할 수 있음. . Sxx= sum((x-sum(x)/10)**2) Sxx . &lt;tf.Tensor: shape=(), dtype=float32, numpy=87.848976&gt; . Sxy= sum((x-sum(x)/10)*(y-sum(y)/10)) Sxy . &lt;tf.Tensor: shape=(), dtype=float32, numpy=194.64737&gt; . beta1_estimated = Sxy/Sxx beta1_estimated . &lt;tf.Tensor: shape=(), dtype=float32, numpy=2.2157044&gt; . beta0_estimated = sum(y)/10 - beta1_estimated * sum(x)/10 beta0_estimated . &lt;tf.Tensor: shape=(), dtype=float32, numpy=9.944572&gt; . plt.figure(figsize=(12,6)) plt.plot(x,y,&#39;.&#39;,label= &quot;observed value&quot;) plt.plot(x,beta0_estimated + beta1_estimated * x, &#39;--&#39;,label=&quot;hat y&quot;) # 주황색선: 세상의 법칙을 추정한선 plt.plot(x,10.2 + 2.2* x, &#39;--&#39;,label = &quot;rule of the world&quot;) # 초록색선: ture, 세상의법칙 plt.legend() . . &lt;matplotlib.legend.Legend at 0x2e12f3afd90&gt; . . Note: 샘플수가 커질수록 주황색선은 점점 초록색선으로 가까워진다. (대수의 법칙은 항상 성립하는 듯?) . - 꽤 훌륭한 도구임. 그런데 약간의 단점이 존재한다. . (1) 공식이 좀 복잡함.. . (2) $x$가 여러개일 경우 확장이 어려움 . - 단점을 극복하기 위해서 우리가 지금까지 했던논의를 매트릭스로 바꾸어서 다시 써보자. . &#47784;&#54805;&#51032; &#47588;&#53944;&#47533;&#49828;&#54868; . 모형을 행렬로 표현하면 변수가 여러개인 multiple linear regression 에서도 단순형태로 표현이 가능하다. | . 우리의 모형은 아래와 같다. . $y_i = beta_0 + beta_1 x_i + epsilon_i, quad i=1,2, dots,10$ . 풀어서 쓰면 . $ begin{cases} y_1 = beta_0 + beta_1 x_1 + epsilon_1 y_2 = beta_0 + beta_1 x_2 + epsilon_2 dots y_{10} = beta_0 + beta_1 x_{10} + epsilon_{10} end{cases}$ . 아래와 같이 쓸 수 있다. . $ begin{bmatrix} y_1 y_2 dots y_{10} end{bmatrix} = begin{bmatrix} 1 &amp; x_1 1 &amp; x_2 dots &amp; dots 1 &amp; x_{10} end{bmatrix} begin{bmatrix} beta_0 beta_1 end{bmatrix} + begin{bmatrix} epsilon_1 epsilon_2 dots epsilon_{10} end{bmatrix} $ . 벡터와 매트릭스 형태로 정리하면 . ${ bf y} = { bf X} { boldsymbol beta} + boldsymbol{ epsilon}$ . - 손실함수의 매트릭스화: 우리가 최소화 하려던 손실함수는 아래와 같다. . $loss= sum_{i=1}^{n}(y_i- beta_0- beta_1x_i)^2$ . 이것을 벡터표현으로 하면 아래와 같다. . $loss= sum_{i=1}^{n}(y_i- beta_0- beta_1x_i)^2=({ bf y}-{ bf X}{ boldsymbol beta})^ top({ bf y}-{ bf X}{ boldsymbol beta})$ . 풀어보면 . $loss=({ bf y}-{ bf X}{ boldsymbol beta})^ top({ bf y}-{ bf X}{ boldsymbol beta})={ bf y}^ top { bf y} - { bf y}^ top { bf X}{ boldsymbol beta} - { boldsymbol beta}^ top { bf X}^ top { bf y} + { boldsymbol beta}^ top { bf X}^ top { bf X} { boldsymbol beta}$ . - 미분하는 과정의 매트릭스화 . loss를 최소화하는 ${ boldsymbol beta}$를 구해야하므로 loss를 ${ boldsymbol beta}$로 미분한식을 0이라고 놓고 풀면 된다. . $ frac{ partial}{ partial boldsymbol{ beta}} loss = frac{ partial}{ partial boldsymbol{ beta}} { bf y}^ top { bf y} - frac{ partial}{ partial boldsymbol{ beta}} { bf y}^ top { bf X}{ boldsymbol beta} - frac{ partial}{ partial boldsymbol{ beta}} { boldsymbol beta}^ top { bf X}^ top { bf y} + frac{ partial}{ partial boldsymbol{ beta}} { boldsymbol beta}^ top { bf X}^ top { bf X} { boldsymbol beta}$ . $= 0 - { bf X}^ top { bf y}- { bf X}^ top { bf y} + 2{ bf X}^ top { bf X}{ boldsymbol beta} $ . 따라서 $ frac{ partial}{ partial boldsymbol{ beta}}loss=0$을 풀면 아래와 같다. . $ boldsymbol{ hat beta}= ({ bf X}^ top { bf X})^{-1}{ bf X}^ top { bf y} $ . - 공식도 매트릭스로 표현하면 : $ left( boldsymbol{ hat beta}= ({ bf X}^ top { bf X})^{-1}{ bf X}^ top { bf y} right) leftarrow$ 외우자 이건.. . - 적용을 해보자. . (X를 만드는 방법1) . X=tf.transpose(tf.concat([[[1.0]*10],[x]],0)) # X . &lt;tf.Tensor: shape=(10, 2), dtype=float32, numpy= array([[ 1. , 20.1], [ 1. , 22.2], [ 1. , 22.7], [ 1. , 23.3], [ 1. , 24.4], [ 1. , 25.1], [ 1. , 26.2], [ 1. , 27.3], [ 1. , 28.4], [ 1. , 30.4]], dtype=float32)&gt; . (X를 만드는 방법2) . tf.concat([[[1.0]*10],[x]],0) . &lt;tf.Tensor: shape=(2, 10), dtype=float32, numpy= array([[ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ], [20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4]], dtype=float32)&gt; . tf.concat([[[1.0]*10],[x]],0).T . AttributeError Traceback (most recent call last) ~ AppData Local Temp/ipykernel_45688/2773875106.py in &lt;module&gt; -&gt; 1 tf.concat([[[1.0]*10],[x]],0).T ~ Anaconda3 envs r4-base lib site-packages tensorflow python framework ops.py in __getattr__(self, name) 506 &#34;tolist&#34;, &#34;data&#34;}: 507 # TODO(wangpeng): Export the enable_numpy_behavior knob --&gt; 508 raise AttributeError(&#34;&#34;&#34; 509 &#39;{}&#39; object has no attribute &#39;{}&#39;. 510 If you are looking for numpy-related methods, please run the following: AttributeError: &#39;EagerTensor&#39; object has no attribute &#39;T&#39;. If you are looking for numpy-related methods, please run the following: from tensorflow.python.ops.numpy_ops import np_config np_config.enable_numpy_behavior() . 위 처럼 하면 error가 남 | error를 읽어보면 numpy 스타일로 구성하고 싶을 경우 np_config.enable_numpy_behavior() 을 이용하라는 문구가 나옴 | . from tensorflow.python.ops.numpy_ops import np_config np_config.enable_numpy_behavior() . X = tf.concat([[[1.0]*10],[x]],0).T . 오 이제된다. . X . &lt;tf.Tensor: shape=(10, 2), dtype=float32, numpy= array([[ 1. , 20.1], [ 1. , 22.2], [ 1. , 22.7], [ 1. , 23.3], [ 1. , 24.4], [ 1. , 25.1], [ 1. , 26.2], [ 1. , 27.3], [ 1. , 28.4], [ 1. , 30.4]], dtype=float32)&gt; . tf.linalg.inv(X.T @ X) @ X.T @ y . &lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([9.945015 , 2.2156935], dtype=float32)&gt; . 결과를 보면 $ left( beta_0, beta_1 right) = (9.94...,2.21...)$ 로 산출되었다. . - 잘 구해진다. . - 그런데.. . beta0_estimated,beta1_estimated . (&lt;tf.Tensor: shape=(), dtype=float32, numpy=9.94458&gt;, &lt;tf.Tensor: shape=(), dtype=float32, numpy=2.2157042&gt;) . 값이 좀 다르다..? . - 같은 값입니다! 신경쓰지 마세요! 텐서플로우가 좀 대충계산합니다. . - 실제로 조금 더 정확히 계산하기 위해서는 tensorflow 안에 내장된 numpy 를 사용한다. . import tensorflow.experimental.numpy as tnp . x=tnp.array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4]) y=10.2 + 2.2*x + epsilon . beta1_estimated = sum((x-sum(x)/10)*(y-sum(y)/10)) / sum((x-sum(x)/10)**2) beta0_estimated = sum(y)/10 - beta1_estimated * sum(x)/10 . beta0_estimated, beta1_estimated . (&lt;tf.Tensor: shape=(), dtype=float64, numpy=9.944573294798559&gt;, &lt;tf.Tensor: shape=(), dtype=float64, numpy=2.2157046054834106&gt;) . X=tnp.concatenate([[tnp.array([1.0]*10)],[x]],0).T tf.linalg.inv(X.T @ X) @ X.T @ y . &lt;tf.Tensor: shape=(2,), dtype=float64, numpy=array([9.94457329, 2.21570461])&gt; . do next . - 선형대수학의 미분이론.. . - tensorflow에서 매트릭스 연산을 자유롭게 다루기 . - 정규방정식을 이용하여 베타계수 추정하는 법하고 벡터 미분 정리해서 추가하자 . Extra . &#54924;&#44480;&#44228;&#49688; &#52628;&#51221; . $$Loss = sum (y- beta_0- beta_1 x)$$ . $ beta_0$ 추정 | . $$L=Loss = sum (y- beta_0- beta_1 x)$$ . $$ frac {d L}{d beta_0} = -2 sum(y- beta_0- beta_1x) = 0$$ . $$ therefore , , hat beta_0 = bar y - beta_1 bar x$$ . $ beta_1$ 추정 | . $$ begin{aligned} frac {d L}{d beta_1} &amp;= sum x left(y- beta_0- beta_1x right) nonumber &amp; = left ( sum xy - beta_1 x^2 right )- n bar x left ( bar y - beta_1 bar x right) nonumber &amp;= left ( sum xy - bar x bar y right ) - beta_1 left ( sum x^2 - ( bar x)^2 right ) nonumber end{aligned}$$ $$ begin{aligned} therefore , , hat beta_1 &amp;= , , frac{ sum xy - bar x bar y}{ sum x^2 - ( bar x)^2} nonumber &amp;= frac {S_{xy}}{S_{xx}} nonumber end {aligned}$$ &#48289;&#53552; &#48120;&#48516; / &#47588;&#53944;&#47533;&#49828; &#48120;&#48516; . $$L=loss=({ bf y}-{ bf X}{ boldsymbol beta})^ top({ bf y}-{ bf X}{ boldsymbol beta})$$ . &#48289;&#53552; &#48120;&#48516; . $$ begin{aligned} x^{ top}y &amp;= begin{bmatrix}x_1 dots x_n end{bmatrix} begin{bmatrix} y_1 dots y_n end{bmatrix} nonumber &amp;= x_1y_1 + x_2y_2+ dots x_ny_n nonumber end{aligned}$$ 위를 미분하면 . $$ begin{aligned} frac {d x^{ top}y}{d x} &amp;= , begin{bmatrix} frac {d}{d x_1} dots frac {d}{d x_n} end{bmatrix} ( x_1y_1 + x_2y_2+ dots x_ny_n) nonumber &amp;= , begin{bmatrix} y_1 dots y_n end{bmatrix}=y nonumber end{aligned}$$ &#48289;&#53552; &#48120;&#48516;&#51032; &#45796;&#47480;&#54400;&#51060; . (1) . $$ frac {d x^{ top}y}{d x} = left ( frac {d x^{ top}}{d x} right) y=y$$ . $$ begin{aligned} left( frac {d x^{ top}}{d x} right) = begin{bmatrix} frac {d}{d x_1} dots frac {d}{d x_{n}} end {bmatrix} begin{bmatrix} x_1 dots x_n end{bmatrix} = begin{bmatrix} frac {d x_1}{d x_1} &amp; dots &amp; frac {d x_n}{d x_1} dots &amp; dots &amp; dots frac {d x_1}{d x_n} &amp; dots &amp; frac {d x_n}{d x_n} end{bmatrix} = mathbf{I} nonumber end{aligned}$$ $$ therefore quad frac {d x^{ top}y}{d x} = left ( frac {d x^{ top}}{d x} right) y = mathbf{I} y = y$$ . (2) . $$ frac {d y^{ top}x}{d x} = left ( frac {d y^{ top}x}{d x} right) =y $$ . $ y^{ top} x $는 $1 times 1$ 차원이므로 인간이면 이해할 수 있을 듯? . (3) . $$ frac {d}{d boldsymbol{ beta}} left ( mathbf{y^{ top}X boldsymbol beta} right ) = mathbf{X^{ top}y}$$ . $ mathbf{y^{ top}X boldsymbol beta}$ 는 $1 times 1$ 인 스칼라 이므로 . $$ mathbf{y^{ top}X boldsymbol beta} = left ( mathbf{y^{ top}X boldsymbol beta} right)^{ top} = boldsymbol beta^{ top} mathbf{X^{ top} y}$$ . 따라서 . $$ begin{aligned} frac {d}{d boldsymbol{ beta}} left ( mathbf{y^{ top}X boldsymbol beta} right ) &amp;= frac {d}{d boldsymbol{ beta}} left ( boldsymbol beta^{ top} mathbf{X^{ top} y} right ) nonumber &amp;= left ( frac{d}{d boldsymbol{ beta}} boldsymbol{ beta}^{ top} right) mathbf{X^{ top y}} nonumber &amp;= mathbf{I ,X^{ top}y} nonumber &amp;= mathbf{X^{ top}y} nonumber end{aligned}$$ (4) . $$ frac {d }{d mathbf {y}} ( mathbf{y^{ top}y}) = 2 mathbf{y}$$ . $$ d , mathbf{y} = left [ frac {d}{y_1}, , frac {d}{y_2} dots dots frac {d}{y_n} right ]$$ . $$ mathbf{y^{ top}y} = sum{y_i}^2$$ . $$ therefore quad frac {d }{d mathbf {y}} ( mathbf{y^{ top}y}) = 2 mathbf{y}$$ . &#52280;&#44256;&#54624; &#53952;&#47536;&#54400;&#51060; . 아래와 같은 풀이는 1번의 벡터 미분의 다른 풀이처럼 풀면 안된다. | . $$ frac {d }{d mathbf {y}} ( mathbf{y^{ top}y}) = mathbf{y}$$ . $$ frac {d , mathbf{y^{ top}y}}{d , mathbf{y}} = left ( frac {d , mathbf{y^{ top}y}}{d , mathbf y} right) mathbf y= mathbf{I} y neq y$$ . because 스칼라 경우를 생각해보자 . (틀린풀이 )$ quad frac {d }{d ,y} y^2 = left ( frac {d}{d ,y } yy right) = y$ ? . (올바른 풀이) $ quad frac {d }{d ,y} y^2 = left ( frac {d}{d ,y } y_1 right)+ left ( frac {d}{d ,y } y_2 right) = 2y$ . 스칼라를 예제로 들었는데 벡터에서 이런 느낌이라고 생각하자 이게 표준적으로 사용되는 설명은 아니지만 이해적? 으로는 간편한 듯 . 다른풀이 (3)번의 경우도 원래는 안되는데 값이 스칼라 이므로 틀린풀이 처럼 안되는 경우이나 1 x 1 행렬이므로 가능한 것이다. . 다시 벡터로 돌아오면 . (올바른 풀이) $ quad left ( frac {d}{d , mathbf{y}} ( mathbf{y^{ top}y}) right ) = A + B$ . $A = f( mathbf y)$ , $B = g( mathbf y)$ 라고 생각하자 . $ frac {d}{d , mathbf y} A = left ( frac {d}{d , mathbf y} mathbf {y^{ top}y} right ) = mathbf {I ,y} = mathbf {I ,y} $ . $B$ 의 경우도 위와 동일하므로 . $$ frac {d ,(A+B)}{d , mathbf{y}} = {f( mathbf {y}) }^{ prime} + {g( mathbf {y}) }^{ prime} = 2 mathbf{y}$$ . (5) . $$ frac{ partial}{ partial boldsymbol{ beta}} { boldsymbol beta}^ top { bf X}^ top { bf X} { boldsymbol beta} = 2 mathbf{X^{ top}X} boldsymbol beta$$ . 4번의 원리를 이용하면 이지이지 . loss&#47484; &#48120;&#48516; . $$L=loss=({ bf y}-{ bf X}{ boldsymbol beta})^ top({ bf y}-{ bf X}{ boldsymbol beta})$$ . $$L = { bf y}^ top { bf y} - { bf y}^ top { bf X}{ boldsymbol beta} - { boldsymbol beta}^ top { bf X}^ top { bf y} + { boldsymbol beta}^ top { bf X}^ top { bf X} { boldsymbol beta}$$ . $L$ 을 미분하면 . $$ begin{aligned} frac{ partial}{ partial boldsymbol{ beta}} L &amp;= frac{ partial}{ partial boldsymbol{ beta}} { bf y}^ top { bf y} - frac{ partial}{ partial boldsymbol{ beta}} { bf y}^ top { bf X}{ boldsymbol beta} - frac{ partial}{ partial boldsymbol{ beta}} { boldsymbol beta}^ top { bf X}^ top { bf y} + frac{ partial}{ partial boldsymbol{ beta}} { boldsymbol beta}^ top { bf X}^ top { bf X} { boldsymbol beta} nonumber &amp;= 0 - mathbf{X^{ top}y} - mathbf{X^{ top}y} + 2 mathbf{X^{ top}X boldsymbol beta} nonumber end{aligned} $$ 따라서 아래와 같은식이 성립한다. . $$ mathbf{X^{ top}y}= mathbf{X^{ top} X} boldsymbol beta $$ . $$ hat { boldsymbol beta} = mathbf{ left (X^{ top}X right)^{-1}Xy} $$ .",
            "url": "https://gangcheol.github.io/big-data-analysis/2022/03/08/(1%EC%A3%BC%EC%B0%A8).html",
            "relUrl": "/2022/03/08/(1%EC%A3%BC%EC%B0%A8).html",
            "date": " • Mar 8, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://gangcheol.github.io/big-data-analysis/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Github . github . Soundcloud . C.I.C . NLP . NLP . Data Mining . Data Mining . Bigdata Analysis . Bigdata Analysis [^1]:a blogging platform that natively supports Jupyter notebooks in addition to other formats. .",
          "url": "https://gangcheol.github.io/big-data-analysis/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://gangcheol.github.io/big-data-analysis/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}